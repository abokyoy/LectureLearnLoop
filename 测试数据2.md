# 笔记总结

- 线性模型过于简单，只能拟合直线关系，无法表示更复杂的非线性关系，例如X1与Y之间可能存在峰值。这种限制称为模型偏差(model bias)，与参数b的偏差不同。
- 为了拟合更复杂的曲线（例如图中红色的曲线），需要更复杂的函数。
- 红色曲线可以看作一个常数加上多个“蓝方”函数的叠加。“蓝方”函数是一个分段函数，在特定阈值处具有不同的斜率，类似于阶跃函数。
- 通过调整“蓝方”函数的数量、位置和斜率，可以逼近红色曲线。每个“蓝方”函数的斜率与红色曲线对应段的斜率相同，起始和结束点与红色曲线的转折点相对应。
-  最终，红色曲线可以通过将常数项与多个“蓝方”函数相加得到。

- 2025年AI将创造97亿个新工作岗位，需要具备AI相关技能的专家。
- PG机器学习和智能研究计划由德克萨斯大学奥斯汀分校合作推出，涵盖AI和机器学习基础知识。
- 该计划包含来自世界知名学者和行业专家的课程，实践项目，以及与Microsoft、SAP、Verizon、IBM等公司专家的互动式学习。
-  学习内容包括掌握AI/ML工具和技术、解决商业问题、构建机器学习和深度学习模型、了解AI在计算机视觉和自然语言处理中的应用，最终获得一份行业认可的电子作品集和丰富的行业人脉。
-  该计划适合希望在新兴AI领域提升技能，转换职业，或寻求晋升的专业人士。
-  Wix工作室能够通过AI能力快速创建网站，并支持规模化扩展。

- 片段讨论了用分段线性曲线逼近任意连续曲线的方法。
- 分段线性曲线可以用多个“蓝色函数”（某种基函数）的组合加常数项表示。
- 曲线越复杂，需要的“蓝色函数”越多。
- 使用Sigmoid函数逼近“蓝色函数”。
- Sigmoid函数是一个S型函数，其表达式包含参数c, b, w，通过调整这些参数可以生成不同形状的Sigmoid函数，从而逼近各种“蓝色函数”。
- “蓝色函数”也称为Hard sigmoid。

- 通过改变参数w、b、c，可以改变sigmoid函数的斜率、水平位置和高度，从而生成不同的sigmoid函数。
- 将多个不同的sigmoid函数叠加，可以逼近各种不同的分段线性函数，进而近似各种连续函数。
-  一个更灵活的函数模型：y = b + Σ[cᵢ * sigmoid(bᵢ + wᵢx₁)]，其中b、wᵢ、cᵢ是未知参数。通过调整这些参数，可以生成各种不同的曲线来拟合数据。该模型克服了线性模型y = b + wx₁的局限性（模型偏差）。
-  将模型进一步扩展到多个特征：y = b + Σ[cᵢ * sigmoid(bᵢ + Σⱼ(wᵢⱼxⱼ))]，其中j代表特征编号（例如，考虑前n天的观测值）。  通过改变cᵢ、bᵢ和wᵢⱼ，可以生成各种不同的函数。
-  该模型本质上是将多个sigmoid函数（每个代表一个特征）组合起来，通过叠加逼近目标函数。  每个sigmoid函数的参数不同，代表了不同特征的权重和影响。

- 该过程包含三个 sigmoid 函数，每个函数对输入特征 (x1, x2, x3) 进行加权求和，并加上偏置项 (b1, b2, b3)，得到中间结果 (r1, r2, r3)。
-  中间结果 (r1, r2, r3) 通过 sigmoid 函数转换得到 (a1, a2, a3)。
-  (a1, a2, a3) 再进行加权求和，并加上最终偏置项，得到最终输出 Y。
-  整个过程可以用矩阵向量乘法表示：X * W + B = R，R 通过 sigmoid 函数得到 A，A * C<sup>T</sup> + B = Y。  其中，X 是输入向量，W 是权重矩阵，B 是偏置向量，R 是中间结果向量，A 是 sigmoid 函数输出向量，C 是权重向量，Y 是最终输出。

- 使用向量表示法，將一個彈性函數表示為 xw+b 經過 sigmoid function 後乘以 c 的 trend pose 再加 b，得到 y。圖示化和線性代數表示方法等價。
- 將未知參數 (W, B, C, B) 整合為一個長向量 θ。
- 尋找最佳參數需要優化過程，參數數量少時可窮舉，數量多時需使用梯度下降法等方法。
- 模型中 sigmoid 函數的數量可以調整，數量越多，可以逼近越複雜的函數，但數量也是超參數，需要自行決定。

- 课程在6:20下课，课程录像保存。
-  将所有未知参数用θ表示，Loss Function变为Loss(θ)。计算方法与之前相同，只是参数数量增多。
- Optimization步骤与之前相同，使用梯度下降法。
-  θ是一个长向量(θ1, θ2, θ3…)，目标是找到使Loss最小的θ (θ* )。
-  初始值θ0随机选择，之后会介绍更好的选择方法。
- 计算每个未知参数对L的偏导数，构成梯度向量G (长度为参数个数)。
- 更新参数：θ1 = θ0 - learning rate * G  ，重复此过程迭代更新参数，直至Loss最小。

- 梯度下降法迭代更新参数θ，直到梯度为零向量或停止迭代。
- 实作中，梯度几乎不可能为零向量，通常以人为设定条件停止迭代。
-  为了提高效率，将大规模数据分成多个Batch进行处理，每个Batch包含B个数据点。
- 每次迭代使用一个Batch的数据计算损失函数L1 (与使用全部数据计算的损失函数L不同)，并根据L1计算梯度更新参数。
-  一个Epoch (APOC) 指的是所有Batch都被遍历一次。
-  一次参数更新称为一次Update。一个Epoch包含多个Update。
-  例1：10000个数据点，Batch size为10，一个Epoch包含1000次Update。
-  例2：1000个数据点，Batch size为1，一个Epoch包含100次Update。
-  Batch Size是超参数 (Hyperparameter)。
-  Hard Sigmoid可以由两个Rectifier Linear Unit (ReLU) 的加权求和表示。
-  ReLU是一个分段线性函数，表达式为C * max(0, b + wx)。 通过调整C, b, w可以改变ReLU函数的形状和位置。

- 比較了ReLU和Sigmoid激活函數，指出使用ReLU需要兩倍的ReLU單元才能達到與一個Sigmoid單元相同的效能。
- 實驗使用了線性模型和不同數量ReLU單元的模型(10, 100, 1000)。結果顯示10個ReLU單元提升有限，100個ReLU單元顯著降低訓練資料的損失(從0.32K降到0.28K)，並在測試資料上有所改善。1000個ReLU單元在訓練資料上的提升有限。
- 探討了增加模型層數(將ReLU應用多次)的影響。實驗結果顯示，將ReLU應用三次，訓練資料的損失從0.28K降至0.14K，測試資料的損失從0.43K降至0.38K。
- 展示了三次ReLU應用後的模型預測結果圖表，指出模型在低谷時期的預測相對準確，但在某些峰谷點的預測存在偏差，例如某一天的低谷被預測到下一天才出現。

- 模型名称演变：从Neural Network到Deep Learning，因早期夸大宣传导致Neural Network名声受损，故改名。
- Deep Learning的结构：由多层神经元（Neuron）组成，每层称为Layer，多层则称为Deep。
- Deep Learning的发展：从AlexNet（16.4%错误率）到VGG（7.3%错误率），再到GoogleNet（6.7%错误率），以及更深的ResNet。
- Deep Learning的疑问：  堆叠多层神经元（Deep）的必要性存疑，与直接增加神经元数量（Fat）效果对比不明确，可能只是“Deep”听起来更高级。
- 过拟合问题（Overfitting）：在训练数据上表现优秀，但在未见过的数据上表现较差，四层模型示例中即出现此问题。
- 未来方向：利用已有的数据（截止2021年2月14日）预测未知数据，选择三层模型可能更有效。

---

# 原始语音转文字

但他們都有一件事在共同啊 太過簡單了linear的model也許太過簡單了怎麼說它太過簡單呢怎麼說它太過簡單呢我們可以想像說X1跟Y也許它中間有比較複雜的關係但是對linear的model而言對linear的model來說X1跟Y的關係就是一條直線隨著X1越來越高Y就應該越來越大你可以設定不同的W改變這條線的斜率你可以設定不同的B改變這一條藍色的直線 Y軸的交叉點但是無論你怎麼改A6跟B它永遠都是一條直線永遠都是X1越大 Y就越大前一天觀看的人數越多隔天的觀看人數就越多但也許現實並不是這個樣子啊也許在X1小於某一個數值的時候前一天的觀看人數跟隔天的觀看人數是成正比但也許當X1大於一個數值的時候這個物極必反過了一個峰值以後過了一個假設X1太大前天觀看的人數太高那隔天觀看人數就會變少也說不定啊 x1跟y中間有一個比較複雜的像這個紅色線一樣的關係但你不管怎麼擺弄你的w跟b你永遠製造不出紅色那一條線你永遠無法用linear的model製造紅色這一條線所以怎麼辦呢顯然linear的model有很大的限制這一種來自於model的限制叫做model bias那其實我們剛才在課堂一開始的時候也叫做也說b叫做bias那這個地方有一點在用詞上有一點ambiguous那所以這邊特別強調說呢這個東西 叫做model的bias它跟b的這個bias不太一樣它指的意思是說我們今天的所以它沒有辦法模擬真實的狀況所以怎麼辦呢我們需要寫一個更複雜的更有彈性的有未知參數的functionlinear的model顯然是不夠的那怎麼辦呢怎麼寫出一個更複雜的有未知參數的function呢我們可以觀察一下紅色的這一條曲線紅色的這一條曲線它可以看作是一個常數 加上一群藍色的這樣子的function那這個藍色的function它的特性是這個樣子當輸入的值當excel的值小於某一個這個threshold的時候它是某一個定值大於另外一個threshold的時候又是另外一個定值那中間呢有一個斜坡所以它是先水平的然後再斜坡然後再水平的那它其實有名字它的名字我們等一下再講這邊我們因為它是藍色的function我們就先叫它藍方吧這樣子好那所以呢這個紅色的線啊它可以看作是一個長數項 大堆的藍方那這個常數項它的值應該要有多大呢你就看這一條紅色的線它跟X軸的交點在哪裡那這個常數項呢就設跟X軸的交點一樣大那怎麼加上這個藍色的function以後變成紅色的這一條線呢你看就這樣子加這個藍色function它的這個坡度這個斜坡的起點設在紅色function的起始的地方然後第二個斜坡的終點設在第一個轉角處所以這邊紅色function有一個轉角 那你就有一個藍色的 function它的斜坡的終點設在紅色 function 的第一個轉角然後呢 你刻意讓這邊這個藍色 function 的斜坡跟這個紅色 function 的斜坡它們的斜率是一樣的這個時候如果你把 0 加上 1你就可以得到紅色曲線紅色這個線段的第一個到這個第一個轉折點之前的數值所以 0 加上 1可以得到紅色線段第一個轉折點之前的部分然後接下來再加第二個藍色的 function怎麼加呢你就看紅色這個線 第二個轉折點出現在哪裡所以第二個藍色function它的斜坡就在紅色function的第一個轉折點到第二個轉折點之間第一個轉折點到第二個轉折點之間那你可以讓這邊的斜率跟這邊的斜率一樣這個時候你把0加1加2你就可以得到兩個轉折點這邊的線段你就可以得到紅色的這條線這邊的部分然後接下來第三個部分第二個轉折點之後的部分怎麼產生呢你就加第三個藍色的function第三個藍色的function它這個坡度其實也不一樣 物一色的跟這個轉折點一樣這邊的斜率 物一色的跟這邊的斜率一樣接下來你把0加1加2加3全部加起來你就得到紅色的這個線就得到紅色這個線所以紅色這個線可以看作是一個常數再加上一堆藍色的function 你知道AI将在2025年制造97亿新工作吗这些新年工作需要有才华的专家来驾驭AI-基于新創新并帮助生产生发展你准备成为一位吗在过去的几年里AI从简单的规则基于系统到进步的机械学学计算计算进行过程中的巨大数量数据我们在这些发展中见证了 从医疗保险到财政 公共交通 娱乐等想参与这高增长的行业吗让我介绍给你PG的机器学和智能研究计划是由德州大学在奥斯丁的合作与伟大的学习在这个计划的过程中你将掌握AI和机器学的基础由世界著名的学生和业界专家你将学会 从教育中学习出来的专业通过现实世界的设计获得手工体验并参与实际教育课程在这段旅程结束后你将能够获得在广泛使用的AI ML工具和技术中的知识辨识和解决商业问题建立可供使用的机器学习和深入学习模式了解AI的应用在像是电脑视觉和自然语言处理中 展示一份业界准备的 e-portfolio 与一份业界丰富的工作体系获得一份资格完成证 从德州大学的 博士生涯以及更多的我们相信 最好的学习经验 建立于人类互动的基础上这就是为什么 这个计划设计 与业界专家进行直播教育课程这些互动会在小组中 进行并协助您 获得个人指导 例如 计算机 案例 专案专业学习 由主流公司的 AI 专业学习者包括 MicrosoftSAPVerizonIBM 和其他多个公司这些导师会带你通过专业的专业项目来增强你的 e- 行业专业的计算机管理员来确保你能够顺利完成所有学习目标一人一人的工作支持和专业评测课程为了确保你能展示出最好的一面 这一项计划是您想要学习的新年工具成长在您的现任职位想要转换成新的工作职位在AI或机器学习是一个有经验的专业人士想要在国家领导职位是一个早期职业专业人士在AI ML上寻找获益的工作更多的是这项计划是设计和制作与您的工作业务相关的更多的是这项计划是设计和制作 而是由德州大學的敬業教授教導的他們的焦點是應付每個學習者的能力並幫助他們達成自己的潛力他們在業界的廣泛經驗會幫助你成為在市場上最受歡迎的專業人士準備負責你的事業嗎?加入PG的智能智能學習以解鎖世界的機會 MING PAO CANADA MING PAO TORONTO 踏入 Wix 工作室,從一個平台開始處理每個部分的建築將你的項目圖示出,以 AI 能力的視覺地圖將地圖構造、電線圖形,在秒數,而不是日子中當你的客戶登機時,進入設計自由地創造,甚至將最小的元素調整使用地圖方式,以積極延伸品牌到每個頁面並觀察它適應地跨越磨擦點Now, SCALE 你能夠在CMS中不斷地建造數量的數據從重複的數據到數百個動態的專頁商業解決方案已經建立在這裡所以當客戶想要更多的功能你就可以更快地去做計劃快一點設計自由一點規格更聰明這就是Wix工作室那你仔細想一下就會發現說不管我畫什麼樣的平面線條什麼叫做平面線條呢 就是你現在這個curve它是由很多線段所組成的它是由很多鋸齒狀的線段所組成的這個叫做piecewise linear的curve那你會發現說這些piecewise linear的curve你有辦法用常數項加一大堆的藍色function組合出來只是它們用的藍色function不見得一樣你要有很多不一樣的藍色function加上一個常數以後你就可以組出這些piecewise linear的curve那如果你今天piecewise linear的curve越複雜也就是這個轉折的點越多那你需要的這個藍色的function就越多所以呢那講到這邊有人可能會 我們會說那也許我們今天要考慮的x 跟 y 的關係不是 piecewise linear 的 curve也許它是這樣子的曲線那就算是這樣的曲線也無所謂我們可以在這樣的曲線上面先取一些點再把這些點點起來變成一個 piecewise linear 的 curve而這個 piecewise linear 的 curve跟原來的曲線它會非常接近如果你今天點取的夠多或你點取的位置適當的話你點取的夠多這個 piecewise linear 的 curve就可以逼近這個連續的這個曲線就可以逼近這個不是 piecewise linear 它是有角度的有弧度的這條曲線所以我們今天知道一件事情你可以用piecewise linear的curve去逼近任何的連續的曲線而每一個piecewise linear的curve又都可以用一大堆藍色的function組合起來也就是說我只要有足夠的藍色function把它加起來我也許就可以變成任何連續的曲線所以今天假設我們的x跟y的關係它也許非常的複雜那也沒關係我們就想辦法寫一個帶有未知數的functionB��i few 表示的就是一堆藍色的function加上一個constant那我們接下來問的問題就是這個藍色function它的式子應該要怎麼把它寫出來呢怎麼把這個藍色function的式子寫出來呢也許你要直接寫出它沒有那麼容易但是你可以用一條曲線來逼近它用什麼樣的曲線來逼近它呢用一個sigmoid的function來逼近這個藍色的function那sigmoid function它的式子長的是這個樣子的它的橫軸輸入是x1 輸出是y所有的x1我們先乘上一個w 再加上一個 b 再取一個負號再取 exponential 再加 1這一算被放在分子 放在分母的地方把 1 除以 1 加上 exponential 負 b 加 w x1前面可以乘上一個 constant 叫做 c那如果你今天輸入的這個 x1 的值趨近於無窮大的時候 會發生什麼事呢如果這一項趨近於無窮大那 exponential 這一項就會消失那當 x1 非常大的時候這一條這邊就會收斂在這個高度是 c 的地方那如果今天 x1 負的非常大的時候會發生什麼事呢如果 x1 負的非常大的時候會發生什麼事呢 角度非常大的時候分母的地方就會非常大那 y 的時就會趨近於0所以你可以用這樣子的一個 function來試著畫出這條曲線用這條曲線來逼近這一個藍色的 function那這個東西它的名字叫做 sigmoidsigmoid 是什麼意思呢sigmoid 如果你要硬要翻成中文的話可以翻成 s 型的所以 sigmoid function 就是 s 型的 function它長得是有點像是 s 型的所以叫它 sigmoid function那這邊我們之後都懶得把 exponential 寫出來我們就直接寫成這個樣子 等於 c 倍的 sigmoid然後這個括號裡面放 b 加 w 乘 x1然後這個 b 加 w x1實際上做的事情就是把它放在 exponential 的指數項前面加一個負號然後一加 exponential 的負b 加 w x1 放在分母的地方而且會乘上 c 就等於 -感覺有些東西不一樣-我不知道發生了什麼-這不是...只是雨雨歪好 所以我們可以用這個 sigmoid function去逼近一個藍色的 function那其實這個藍色的 function比較常見的名字就叫做hard的 sigmoid只是我本來想說一開始我們是先介紹藍色的 function來介紹 sigmoid所以一開始說它叫做 Hard sigmoid 有點奇怪所以我們先告訴你說有一個 sigmoid function它可以逼近這個藍色的 function那這個藍色的 function其實通常就叫做 Hard sigmoid那我們今天我們需要各式各樣不同的藍色的 function還記得嗎 我們要組出各種不同的曲線那我們就需要各式各樣合適的藍色的 function而這個合適的藍色的 function 怎麼製造出來呢我們就需要調整這裡的 b 跟 w 跟 c你可以調整 b 跟 w 跟 c你就可以製造各種不同形狀的 sigmoid function用各種不同形狀的 sigmoid function去逼近這個藍色的 function 舉例來說 如果你今天改w會發生什麼事呢你就會改變斜率就會改變斜坡的坡度如果你動了b會發生什麼事呢你就可以把這個sigmoid function左右移動如果你改c會發生什麼事呢你就會改變它的高度所以你只要有不同的w 不同的b 不同的c你就可以製造出不同的sigmoid function把不同的sigmoid function疊起來以後你就可以疊出各種不同的你就可以去逼近各種不同的piecewise linear function而piecewise linear function可以拿來近似各種不同的continuous function所以今天假設我們要把紅色的這條線它的函數寫出來的話那可能長什麼樣子呢我們知道說紅色這條線就是0加1加2加3而這個1 2 3他們都是藍色的function所以他們的函數就是有一個固定的樣子他們都寫做x1乘上w再加上b做sigmoid再乘上c1只是1跟2跟3他們的w不一樣他們的b不一樣他們的c不一樣 如果是第一個綠色第一個藍色function他就是W1 B1 C1第二個藍色function我們就說他用的是W2 B2 C2第三個藍色function我們就說他用的是W3 B3 C3那我們接下來就是把0跟1 2 3全部加起來以後我們得到的函式就長這個樣子我們把1加2加3加起來這邊就是summation over i我們的i等於1或2或3然後summation裡面就是Ci 乘上sigma由bi加wi乘上x1所以這邊每一個式子都代表了一個不同藍色的functionsummation的意思就是把不同的藍色的function給它加起來就是這邊summation的意思然後別忘了加一個constant這邊用b來表示這個constant所以今天我們有一個如果我們今天就寫出了一個這樣子的function如果我們假設裡面的b跟w跟c它是未知的它是我們未知的參數那我們就可以設定不同的b跟w跟c設定不同的b跟w跟c我們就可以製造不同的藍色的function 制造不同的藍色方向疊起來以後就可以制造出不同的紅色的curve制造出不同的紅色的curve就可以制造出不同的piecewise linear的curve就可以去逼近各式各樣不同的continuous的function所以我們其實有辦法寫出一個這個非常有彈性的有未知參數的function它長這樣子就是summation一堆sumoi但它們有不同的c不同的b不同的r好 那所以本來我們是linear的modely等於b加w乘上x1它有非常大的限制這個限制叫做model的bias那我們要如何減少嗎我們要如何減少嗎 我們可以寫一個更有彈性的有位置參數的功能它叫做 y 等於 b 加 summation c i sigmoid b i 加 w i x 1本來這邊是 b 加 w x 1 這邊變成 b i 加 w i x 1然後我們有很多不同的 b i 有很多不同的 w i他們都通過 sigmoid 都乘上 c i 把它都加起來再加 b 等於 y我們只要代入不同的 c 不同的 b 不同的 w我們就可以變出各式各樣就可以組合出各式各樣不同的功能那我們剛才其實已經進化到不是只用一個功能是一個feature 啊我們可以用多個feature 我們這邊用j來代表feature的編號舉例來說剛才如果要考慮前28天的話j就是1到28考慮前56天的話j就是1到56那如果要把這個function再擴展成我們剛才講的上面這個比較有彈性的function的話那也很簡單我們就把sigmoid裡面的東西換掉本來這邊是b加summation over j wj xj那這邊就把這一下放到這個括號裡面改成bi加summation over j wij xj我們把本來放在這邊的東西放到sigmoid裡面 然後每一個Symoy的function裡面都有不同的bi不同的wij然後取Symoy以後再加上ci就全部加起來再加上b就得到y我們只要這邊ci bi跟wij放不同的值就可以變成不同的function好 那如果講到這邊你還是覺得有點抽象的話因為你看這個式子覺得有點頭痛的話那我們用比較直觀的方式把這個式子實際上做的是把它畫出來它畫出來看起來像是這個樣子好 我們先考慮一下j就是1 2 3的狀況就是我們只考慮三個feature舉例來說我們只考慮前一天 前兩天跟前三天的case所以j等於1 2 3那所以輸入就是x1代表前一天的觀看人數x2兩天前觀看人數x3三天前的觀看人數i是什麼i是每一個i就代表了一個藍色的function只是我們現在每一個藍色的function都用一個sigmoid function來禁制它所以每一個i就代表了一個sigmoid function或者是代表了一個藍色的function好 那這邊呢這個1 2 3就代表我們有三個sigmoid function那我們先來看一下這個括號裡面做的事情是什麼每一個sigmoid都有一個括號 這個括號裡面做的事情是什麼呢好第一個 sigmoid i 等於 1 的 case就是把 x1 乘上個 weight 叫 w1 1x2 乘上另外一個 weight 叫 w1 2x3 再乘上一個 weight 叫做 w1 3全部把它加起來那不要忘了再加一個 b把 b 加起來然後呢這個得到的式子就是這個樣子所以這邊我們用 w i j 呢來代表在第 i 個 sigmoid 裡面乘給第 j 個 feature 的 weight第一個 feature 它就是 w1 1第二個 feature 就是乘 w1 2第三個 feature 就是W13所以三個feature 123這個W的第二個下標就是123W的第一個下標呢代表是現在在考慮的是第一個Sigmoid function那我們有三個Sigmoid function好 那第二個Sigmoid function呢我們就不把它的W寫出來了我們就不把它的W放在這個箭頭旁邊不然會太擠那第二個Sigmoid function它的在括號裡面做的事情是什麼呢它在括號裡面做的事情就是把X1X1乘上W21把X2乘上W22把X3X3乘上W23通通加起來再加B2第三個Sigmoid呢 Sigmoid在括號裡面做的事情就是把123 123 x1 x2 x3分別乘上W31 W32跟W33再加上比3那我們現在為了要簡化起見我們把括弧裡面的數字用比較簡單的符號來表示所以這一串東西我們當作R1這一串東西我們當作R2這一串東西我們當作我們叫它R3那這個x1 x2跟x3和R1 R2 R3中間的關係是什麼呢你可以用矩陣跟向量相乘的方法寫一個比較簡單的簡潔的寫法 我們剛才已經知道說 R1 R2 R3也就是跨弧裡面算完的結果三個Symbol跨弧裡面算完的結果R1 R2 R3跟輸入的三個Feature X1 X2 X3他們中間的關係就是這樣把X1 X2 X3乘上不同的位加上不同的Bias也就是不同的B會得到不同的R那這三個字這一連串的運算其實我們可以把它簡化就如果你熟悉線性代數的話簡化成矩陣跟向量的強乘把X1 X2 X3拼在一起變成一個向量這邊所有的W通通放在一起變成一個矩陣把B1 B2 B3 拼起來變成一個項量吧R1 R2 R3拼起來變成一個項量那這三個式子你就可以簡寫成有一個項量叫做X這個X乘3個矩陣叫做W這個W裡面有9個數值就是這邊的9個R6就是這邊的9個位X先乘上W以後再加上B就得到R這個項量那這邊做的事情跟這邊做的事情是一模一樣的沒有半毛錢的不同只是表示的方式不一樣而已只是本來寫三個數字裡面有一堆++-有一堆還有什麼上標結果還有什麼兩個下標什麼看起來就讓人佛大 改成漸進代數比較常用的表示方式X乘上矩陣大略再加上向量B會得到一個向量叫做R 好 那所以這邊這件事情在這個括號裡面做的事情就是這麼一回事把X乘上大倫加上B等於RR就是這邊的R1 R2 R3我這電腦有點卡 微卡沒辦法控制這個滑鼠沒關係 我可以控制了控制了就是R1 R2 R3好 那接下來這個R1 R2 R3就要分別通過Sigmoid Function分別通過Sigmoid Function我們實際上做的事就是R1 R2 R3R2 R3 R2 R3R1 R2 R3我們實際上做的事就是做的事情就是 把 r1 取一個負號再做 exponential 再加 1然後把它放到分母的地方1 除以 1 加 exponential 負 r1 等於 a1然後同樣的方法由 r2 取得到 a2把 r3 透過 sigmoid function 得到 a3所以這邊這個藍色的虛線框框裡面做的事情就是從 x1 x2 x3 得到了 a1 a2 a3接下來我們這邊有一個簡潔的表示方法是我們用 r 通過一個叫做 sigmoid function我們用這個東西我們這邊用這個符號來代表通過 sigmoid function然後呢 說我們得到了A這個向量就把R1 R2 R3分別通過Symoy function那我們直接用這個符號來表示它然後得到A1 A2 A3然後接下來呢接下來我們這個Symoy的輸出還要乘上C1 I然後還要再加上B那我們這邊做的事情就是把A1乘C1A2乘C2 A3乘C3通通加起來再加上B最終就得到了Y那這邊呢如果你要用向量來表示的話A1 A2 A3拼起來叫這個向量AC1 C2 C3拼起來叫一個向量C那我們這邊把這個C 做 transpose那 A 呢乘上 C 的 transpose再加上 B我們就得到了 Y所以這一連串的運算剛才寫的那一個我們說比較有彈性的式子它整體而言做的事情就是X 輸入是 X我們的 feature 是 X 這個向量X 乘上矩陣 W 加上向量 B得到向量 R再把向量 R 通過 sigmoid function得到向量 A再把向量 A 跟乘上 C 的 transpose加上 B 就得到 Y所以這上面這件事情如果你想要用線性代數的方法來表示它 用向量體制的相乘方法來表示它就長得一副這個樣子那這邊的這個R就是這邊的R這邊的A就是這邊的A所以我們可以把這一串東西放到這個括號裡面再把這個A放到這裡來所以把相同的東西併起來以後整體而言就是長這個樣子上面這一串東西我們覺得比較有彈性的這個function如果你要線性代數來表示它的話就是下面這個式子x乘上w再加上b通過sigmoid function乘上c的trend pose加b就得到y上面這一串就是下面這一串就是我剛才寫的那個比較有彈性的function講來講去都是一樣的東西 只是不同的表示方式而已上面這個是圖示化的表示方式下面這個是線性代數的表示方式其實都在講同一件事情好 那接下來在我們繼續講說要怎麼把這些未知的參數找出來之前我們先再稍微重新定義一下我們的符號這邊的這個X是feature這邊的WBC跟B這邊有兩個B但是這兩個B是不一樣的這邊這個是一個向量這邊是一個數值你看他們的這個底色是不一樣的這個是綠色這個是灰色顯示他們是不一樣的東西 我们把这个黄色的这个这个W把这个B把这个C把这个B统统拿出来集合在这边他们就是我们的unknown的parameter就是我们的未知的参数那我们把这些东西统统拉直拼成一个很长的向量我们把W的每一个row或者是每一个column拿出来这边不管你是拿row或拿column都可以啦意思是一样啦你就把W的每一个column或每一个row拿出来拼成一个长的向量把B拼上来把C拼上来把B拼上来这个长的向量我们直接用一个符号这样做 由θ來表示它θ是一個很長的向量裡面的第一個數值我們叫θ1第二個叫θ2 第五個叫θ3那θ裡面 這個向量裡面有一些數值是來自於這個矩陣有一些數值是來自於b有一些數值來自於c有一些數值來自於這邊這個b但我們就不分了反正θ它統稱我們所有的未知的參數我們就一律統稱θ 在瑞麗兒童醫院的心臟病患心臟病患中心動作是我們所有的一切我們全球級的心臟專家團隊的無懈可擊的動作我們現代藝術技術的迅速的動作最後是我們患者的自由自由的動作患者從世界各地來到我們這裡因為在瑞麗兒童醫院我們知道動作有多強大以及它能帶領我們所有人 那这边我们就是换了一个新的我们就重新改写了机器学习的第一步我们重新定了一个有未知参数的function那接下来我们就要进入第二步跟第三步那在我们进入之前我们来看大家有没有问题想要问的 好 那你看線上人員有要問問題嗎那個沒有問題他是說load上數據是說算很複雜 時間也有所以需要optimization的過程去找到一個組那他這樣寫什麼好 我試著回答看看我猜他的問題是說我們其實要做optimization這件事找一個可以讓load最小的參數有一個最暴力的方法就是報收所有可能的未知參數的值對不對像我們剛才在只有W跟B兩個參數的前提之下我根本就可以報收所有可能的W跟B的組合所以在參數很少的情況下甚至你有可能不用 combination 的技巧但是我們今天參數很快就會變得非常多像在這個例子裡面參數有一大把有WB有C跟B串起來變成一個很強的向量叫SAN那這個時候你就不能夠用報收的方法了你需要 gradient descent 這樣的方法來找出可以讓 loss 最低的參數好希望這樣回答到他的問題在座還有同學有問題嗎來請說 没说错吧 可以 非常 這是一個這個同學的問題是說剛才的例子裡面有三個 sigmoid那為什麼是三個呢能不能夠四個五個六個呢可以 sigmoid 的數目是你自己決定的而且 sigmoid 的數目越多你可以產生出來的 piecewise linear的 function 就越複雜就是假設你只有三個 sigmoid意味著你只能產生三個線段但是假設你有越多 sigmoid你就可以產生有越多線段的piecewise linear 的 function你就可以逼近越複雜的 function但是至於要幾個 sigmoid這個又是另外一個 hyperparameter這個你要自己決定我們在剛才的例子裡面取三個 一個例子也許我以後不應該取三個因為這樣會讓你誤以為說input feature是三個 sigmoid 是三個不是就是說 sigmoid 幾個可以自己決定好 這樣回答大家還有問題想問嗎請說跟什麼 sigmoidhard 的 sigmoid 啊首先那個 function你寫出來可能會比較複雜你一下子寫不出他的 function但如果你要你可以寫得出他的 function 的話你其實也可以用 hard 的 sigmoid你想要用也可以所以不是一定只能夠用剛才那個 sigmoid去逼近那個 hard sigmoid你完全有別的做法等一下我們就會講別的做法好 答案我問你 有问题想要问吗 如果目前暫時沒有的話就請容我繼續講下去那你知道這門課是6點20才下課啦所以只要講到6點20前都是可以的那如果你有事想要早點離開也沒有問題我們課程都是有錄影好 那接下來進入第二步啦我們要訂Loss有了新的這個Model以後我們Loss會不會有什麼不同啊沒有什麼不同定義的方法是一樣的只是我們的符號改了一下之前是L of W跟B因為W跟B是未知的那我們現在接下來的未知的參數很多啦你再把它一個一個列出來 所以我們直接用θ來統設所有的參數用θ來代表所有位值的參數所以我們現在的Loss Function就變成Lossθ這個Loss Function要問的就是這個θ如果它是某一組數值的話會有多不好 會有多好那計算的方法跟剛才只有兩個參數的時候其實是一模一樣的就你先給定某一組WBCT跟B的值你先給定某一組θ的值假設你知道W的值是多少把W的值寫進去 B的值寫進去C的值寫進去 B的值寫進去然後呢 你把一組feature x 然後看看你估測出來的Y是多少再計算一下跟真實的Label之間的差距你得到一個T把所有的誤差統統加起來你就得到你的Loss那接下來下一步就是OptimizationOptimization的Problem跟前面講的有沒有什麼不同呢沒有什麼不同它是一樣的所以就算我們換了一個新的模型這個Optimization的步驟Optimization的演算法還是歸零Descent看起來其實沒有真的太多的差別我們現在的Seda它是一個很長的向量我們把它表示成Seda1 Seda2 Seda3 我們現在就是要找一組SETA這個SETA可以讓我們的Loss越小越好可以讓Loss最小的那一組SETA我們叫做SETA的SUM好 那怎麼找出這個SETA的SUM呢我們一開始要隨機選一個初始的數值那這邊叫做SETA0你可以隨機選那之後也可能會講也會講到更好的找初始值的方法我們現在先隨機選就好那接下來呢你要計算為分你要對每一個未知的參數這邊用SETA1 SETA2 SETA3來表示你要為每一個未知的參數 都去計算它對L的微分那把每一個參數都拿去計算對L的微分以後集合起來它就是一個向量這個向量我們用G來表示它這邊假設有1000個參數那這個向量的長度就是1000這個向量裡面就有1000個數字這個東西有一個名字就我們把每一個參數對L的微分集合起來以後它有一個名字這個向量的名字叫做gradient那很多時候你會看到gradient的表示方法是這個樣子的你把L前面放了一個倒三角形 那這個就代表了 gradient這是一個 gradient 的簡寫的方法那其實我要表示的就是這個向量我前面放一個倒三角形的意思就是把所有的參數θ1,θ2,θ3 通通拿去對我做微分就是這個倒三角形的意思那後面放θ0的意思是說我們這個算微分的位置是在θ等於θ0的地方我們算出這個 gradient算出這個 g 以後接下來我們就要 update 我們的參數了我們就要更新我們的參數了更新的方法跟剛才只有兩個參數的狀況是一模一樣的只是從更新兩個參數可能換成更新成一千個參數 但更新的方法是一樣的本來有一個參數叫SETA1那上標0代表它是一個起始的值它是一個隨機選的起始的值那這個SETA10減掉learning rate乘上微分的值得到SETA11代表SETA1更新過一次的結果SETA20減掉微分乘以減掉learning rate乘上微分的值得到SETA21那以此類推你就可以把那一千個參數通通都更新了那這邊有一個簡寫就是你會把這邊所有的SETA合起來當這個項量我們用SETA0來表示這邊你可以把learning rate提出來那剩下的部分微分的部分 每一個參數對L為分的部分叫做歸點叫做G所以SETA0減掉任意RAY乘上G就得到SETA1把這邊的所有的SETA統統集合起來把這邊所有的SETA統統集合起來就叫做SETA1SETA0減掉SETA0是個向量減掉任意RAY乘上GG也是個向量會得到SETA1那假設你這邊的參數有1000個那SETA0就是有1000個數值1000維的向量G是1000維的向量SETA1也是1000維的向量好那整個操作就是這樣啦就是有SETA0算歸點根據歸點去把SETA0更新成SETA1然後呢 再算一次Gradient然後根據Gradient把θ1再更新成θ2再算一次Gradient把θ2更新成θ3一直再推直到你不想做或者是你算出來的這個Gradient是0項量是Zero vector導致你沒有辦法再更新參數為止不過在實作上你幾乎不太可能做出Gradient是0項量的結果通常你會停下來就是你不想做了好 那但是實作上那這邊是一個實作的Detail Issue之所以在這邊重點其他是因為助教的程式裡面有這一段所以我們必須要講一下別人注意看助教程式的時候覺得有點固惑實際上我們在 做Gradient Descent的時候我們會這麼做我們這邊有大N比資料我們會把這大N比資料分成一個一個的Batch就是一包一包的東西 一組一組的怎麼分 隨機分就好隨機分就好好 所以每一個Batch裡面有大B比資料所以本來全部有大N比資料現在大B比資料一組 大B比資料一組每一組叫做Batch怎麼分組 隨便分就好隨便分就好那本來我們是把所有的Data拿出來算一個Loss那現在我們不這麼做我們只拿一個Batch裡面的Data 只拿B比θ出來算一個Loss我們這邊把它叫L1那跟這個L是區別因為你把全部的資料拿出來算Loss跟只拿一個Batch拿出來的資料拿出來算Loss它不會一樣嘛所以這邊用L1來表示它但是你可以想像說假設這個B夠大也許L跟L1會很接近也說不定好 所以實作上的時候每次我們會先選一個Batch用這個Batch來算L根據這個L1來算Gradient用這個Gradient來更新參數接下來再選下一個Batch算出L2根據L2算出Gradient然後再更新參數再取下一個Batch算出L3根據L3 算出Gradient再用L3算出來的Gradient來更新參數所以我們並不是拿大L來算Gradient實際上我們是拿一個Batch算出來的L1 L2 L3來計算Gradient那把所有的Batch都看過一次叫做一個APOC每一次更新參數叫做一次Update所以在文獻上常常會有人聽到Update這個詞彙常常有人聽到APOC這個詞彙那Update跟APOC是不一樣的東西每次更新一次參數叫做一次Update把所有的Batch都看過一遍叫做一個APOC為了要 那至於為什麼要分一個一個Batch那這個我們下週再講好但是為了讓大家更清楚認識Update跟Apple的差別這邊就舉個例子假設我們用1萬筆Data也就是大N等於1萬假設我們的Batch的大小是這10也就是大B等於10接下來問你我們在一個Apple中總共Update了幾次參數呢那你就算一下這個大N個Example1萬筆Example總共形成了幾個Batch總共形成了1萬除以10也就是1000個Batch所以在一個Apple裡面你其實已經更新了參數1000次所以一個App並不容易 不是更新參數一次在這個例子裡面一個APP已經更新了參數一千次了第二個例子那就是假設有一千個資料Batch Size是一檔那其實Batch Size的大小也是你自己決定的所以這邊我們又多了一個Hyperparameter所謂Hyperparameter剛才講過就是你自己決定的東西人手設的東西不是機器自己找出來的叫做Hyperparameter我們今天已經聽到了London Rail是個Hyperparameter幾個Symbol也是一個HyperparameterBatch Size也是一個Hyperparameter好 一千個exampleBatch Size 那一個AirPods總共更新幾次參數呢?是10次所以有人跟你說我做了一個AirPods的訓練那你其實不知道他更新了幾次參數有可能1000次有可能10次取決於他的Batch Size有多大好 那我們其實還可以對模型做更多的變形剛才我同學問到說咦 這個Hard的Sigmoid不好嗎?為什麼我們一定要把它換成Soft的Sigmoid你確實可以不一定要換成Soft的Sigmoid有其他的做法舉例來說這個Hard的Sigmoid 我剛才說它的函數有點難寫出來其實也沒有那麼難寫出來它可以看作是兩個rectifier linear unit的加總所謂rectifier linear unit它就是長這個樣子就是它有一個水平的線走到某一個地方有一個轉折的點然後變成一個斜坡那這種function它的式子寫成C乘上max0b加wx1這個max0b加wx1的意思就是看0跟b加wx1誰比較大比較大的那個就會被當作輸出所以如果b加wx1小於0那輸出就是0如果b加 Wx1大於0 輸出就是B加Wx1那總之這一條線可以寫成Cmax0B加Wx1那你調不同的W不同的B不同的C你就可以挪動它的位置你就可以改變這條線的斜率那這種線在機器學習裡面我們叫做Rectifier Linear Unit它的縮寫叫做ReLU它名字唸起來蠻有趣的它真的就是唸ReLU那你把兩個ReLU疊起來就可以變成Hard Simulator對不對我們把這樣子的一個ReLU疊這樣子的一個ReLU把它們加起來它就變成Hard Simulator 所以我們能不能用ReLU呢?可以所以如果我們不要用Sigmoid你想用ReLU的話你就把Sigmoid的地方換成max括號0EI加summation over jWIJXI那本來這邊只有I個Sigmoid但我想說你要兩個ReLU才能夠合成一個hard Sigmoid所以這邊有I個Sigmoid那如果ReLU要做到一樣的事情你可能需要兩倍的ReLU因為兩個ReLU合起來才是一個hard Sigmoid所以這邊要兩倍的ReLU好 所以我們把Sigmoid換成ReLU這邊就是把一個式子換了因為要表示一個這個hard的Sigmoid表示那個藍色的方式不是只有一種做法 也完全可以用其他的做法好 那這個Sigmoid或是ReLU他們在機器學習裡面我們就叫它Activation Function他們是有名字的他們統稱為Activation Function當然還有其他常見的還有其他的Activation Function但Sigmoid跟ReLU應該是今天最常見的Activation Function那哪一種比較好呢這個我們下次再見哪一種比較好呢我接下來的實驗都選擇用了ReLU簡單ReLU比較好至於它為什麼比較好那就是下週的事情了好 接下來就真的做了這個實驗這個都是真實的數據你知道嗎真的做了這個實驗 如果是linear的model我們現在考慮56天訓練資料上面的Loss是0.32K沒看過的資料2021年的資料是0.46K如果用10個relu好像沒有進步太多這邊跟用linear是差不多的所以看起來10個relu不太夠100個relu就有顯著的差別了100個relu在訓練資料上的Loss就可以從0.32K降到0.28K100個relu我們就可以製造比較複雜的曲線本來linear就是一直線但是100個relu我們就可以產生100個有100個折線的piecewise linear 在測試資料上也好了一些接下來換1000個review1000個review在訓練資料上Loss更低了一些但是在沒看過的資料上看起來也沒有太大的進步好 接下來還可以做什麼呢我們還可以繼續改我們的模型舉例來說剛才我們說從X到A做的事情是什麼是把X乘上W加B再通過Sigmoid Function不過我們現在知道說不一定要通過Sigmoid Function通過review也可以然後得到A我們可以把這個同樣的事情再反覆的多做幾次剛才我們把W X A x 乘上 w 加 b 通過 sigmoid function 得到 a我們可以把 a 再乘上另外一個 w'再加上另外一個 b'再通過 sigmoid 或 reduce function 得到 a'所以我們可以把 x 做這一連串的運算產生 a接下來把 a 做這一連串的運算產生 a'那我們可以反覆的多做幾次那要做幾次欸 這個又是另外一個 hyperparameter這是另外一個你要自己決定的事情你要做兩次嗎 三次嗎 四次嗎 一百次嗎這個你自己決定不過這邊的 w 跟這邊的 w'他們不是同一個參數喔這個 b 跟這邊的 b' 他們不是同一個參數 是同一個參數是增加了更多的未知的參數好 那就是接下來就真的做了實驗了我們就是每次都加100個review那我們就是input的feature就是56天前的資料如果是只做一次只做一次就那個乘上W再加B再通過review或signal這件事只做一次的話這是我們剛才看到的結果兩次 哇 這個Loss降低很多啊0.28K降到0.18K沒看過的資料上也好了一些三成 哇 又有進步從0.18K降到0.14K 所以從一層到就是乘一次W到通過一次REDUCE到通過三次REDUCE我們可以從0.28K到0.14K在訓練資料上在沒看過資料上從0.43K降到了0.38K看起來也是有一點進步的好 那這個是真實的實驗結果了就我們來看一下今天有做通過三次REDUCE的時候做出來的結果怎麼樣橫軸剛才已經看過了就是時間就是日子縱軸是觀看的人次是千人紅色的線代表的是真實的數據藍色的線 是預測出來的數據那你會發現說欸 在這種低點的地方啊你看紅色的數據是每隔一段時間就會有兩天的低點就會有兩天的低點在低點的地方機器的預測還算是蠻準確的它都準確抓到說這兩天就是低的這兩天都是低的這兩天就是低的這兩天就是低的那這邊有一個神奇的事情這個機器高估了真實的觀看人次尤其是在這一天這一天有一個很明顯的低谷但是機器沒有預測到這一天有明顯的低谷它是晚一天才預測出低谷那你知道是怎麼回事嗎蛤 欸 論年 不是 因為還沒有到2月28號嘛 大家有什麼想法嗎 各位 過年啦這一天最低點是什麼今天最低點就是除夕啦誰除夕還學機器學習對不對所以對機器來說你不能怪他他根本不知道除夕是什麼他只知道看前56天的值來預測下一天會發生什麼事他不知道那一天是除夕所以你不能怪他預測的不準這一天就是除夕好 那到目前為止我們講了很多各式各樣的模型那我們現在還缺了一個東西你知道缺什麼東西嗎缺一個好名字你知道這個外表啊是很重要的一個死破酸宅穿上西裝以後就潮了起來或者是芝奇範呂的 說他是漢佐將軍夷城停留中山晉王之後也就潮了起來 對不對所以我們的模型也需要一個好名字其實它叫做什麼名字呢這些Sigmund 或 Relu他們叫做 Neuron我們這邊有很多的 Neuron很多的 Neuron 叫什麼很多的 Neuron 就叫做 Neuron NeuronNeuron 是什麼Neuron 就是神經元人腦中就是有很多神經元很多神經元串起來就是一個神經網路跟你的腦是一樣的接下來你就可以到處騙麻瓜說看到沒有這個模型就是在模擬人腦 知道嗎這就是在模擬人腦 這個就是人工詞彙然後麻瓜就會下午把錢掏出來但是這個把戲在80、90年代的時候已經玩過了Neural Network不是什麼新的技術80、90年代就已經用過了當時已經把這個技術的名字搞到臭掉了Neural Network因為之前吹捧的太過浮誇所以後來大家對Neural Network這個名字都非常感冒它就像是個髒話一樣寫在paper上面都會註定害你的paper被拒絕所以後來為了要重振Neural Network的雄風所以怎麼辦呢 需要新的名字怎麼樣新的名字呢這邊有很多的 Neural每一排 Neural我們就叫它一個 Layer他們叫 Hidden Layer有很多的 Hidden Layer就叫做 Deep這整套技術就叫做 Deep Learning好 我們就把 Deep Learning 講完了就是這個 就是這麼回事就是這樣來好 所以人們就開始把內神經網路越疊越多越疊越深12年的時候有一個 ANS NET他有八成他的錯誤率是16.4%兩年之後 VGG 19成錯誤率在影像辨識上 進步到7.3%這個都是在影像辨識上一個這個基準的資料庫上面的結果後來Google內有錯誤率降到6.7%有22成但這些都不算是什麼residual內有152成啊他比101還要高啊但是這個residual內啊其實要訓練這麼深的Navigate是有訣竅的這個我們之後再講但是講到這邊如果你仔細思考一下我們一路的講法的話你有沒有發現一個奇妙的違和的地方不知道大家有沒有發現 什麼樣違和的地方呢我們一開始說我們想要用relu或者是sigmoid去逼近一個複雜的function實際上只要夠多的relu夠多的sigmoid就可以逼近任何的連續的function對不對我們只要夠多的sigmoid就可以製造夠複雜的線段就可以逼近任何的continuous function所以我們只要一排relu一排sigmoid夠多就足夠了那生的意義到底何在呢把relu sigmoid反復用到底有什麼好處呢為什麼不把它們直接排一排呢直接排一排也可以表示任何function所以把它反復用沒什麼道理啊 所以有人就說把deep learning把reducing memory反復用不過是個噱頭你之所以喜歡deep learning只是因為deep這個名字好聽reducing memory排成一排你只可以製造一個肥胖的networkfat neural network跟deep neural network聽起來量級就不太一樣deep聽起來就比較厲害fat neural network還以為是死肥宅network就不厲害這樣子那到底deep的理由為什麼我們不把network變胖只把network變深呢這個是我們日後要再講的話題好 那有人就說那怎麼不變得更深呢剛才只做到三成應該要做得更深嘛現在network都是別級擺深的 每幾百層都不好意思說你在做deep learning對不對所以要做更深所以確實做了更深做四層四層在訓練資料上他的loss是0.1K在沒有看過2021年的資料上是如何呢是0.44K慘掉了欸怎麼會這樣子呢在訓練資料上三成比四成差四成比三成好但是在沒看過的資料上四成比較差三成比較好在有看過資料上在訓練資料上跟沒看過的資料上他的結果是不一致的這種訓練資料跟測試這種訓練資料跟沒看過的資料 它的結果是不一致的狀況這個狀況叫做overfeeding那你常常聽到有人說機器學習會發生overfeeding的問題指的就是在訓練資料上有變好但是在沒看過的資料上沒有變好這件事情但是做到目前為止我們都還沒有真的發揮這個模型的力量你知道我們要發揮這個模型的力量2021年的資料到2月14號之前的資料我們也都已經手上有了所以我們要真正做的事情是什麼我們要做的事情就是預測未知的資料但是如果我們要預測未知的資料我們應該選三層的內容 還是四層的內臥呢舉例來說今天是2月26號今天的觀看人數我們還不知道如果我們要用一個Neural Network用我們已經訓練出來的Neural Network去預測今天的觀看人數你覺得應該要選三層的還是選四層的呢好這個我們來問一下大家的意見吧你覺得應該選三層的同學舉手一下好手放下好應該選四層的同學舉手一下好比較少好至於怎麼選模型這個是夏多會講問題但是大家都非常有sense知道我們要選三層的多數人都決定要選三層的你怎麼會說我總會選三層的 我們選四成呢四成在訓練資料上的結果比較好啊可是我們並不在意訓練資料的結果啊我們在意的是沒有看過的資料而2月26號是沒有看過的資料我們應該選一個在訓練的時候沒有看過的資料上表現會好的模型所以我們應該選三成的內網那你可能以為這門課就到這邊結束了其實不是我們真的來預測一下2月26號應該要我的觀看次數是多少好 但是因為其實YouTube的統計它沒有那麼及時啦所以它現在只統計到2月24號沒關係 我們先計算一下2月25號的觀看次數 人數是多少這個三成的內幕告訴我說2月25號這個頻道的總觀看人次應該是5250人那我們先假設2月25號是對的但實際上我們還不知道2月25號對不對因為YouTube後台統計的數據還沒有出來但我們先假設這一天就是對的然後再給我們的模型去預測2月26號的數字得到的結果是3.96K有3960次那你會發現為什麼這邊特別低因為模型知道說這個禮拜五觀看的人數就是比較少所以他預測特別低聽起來也是很合理但是你覺得這個預測個人這邊 0.38K比起來哪一個會比較準確呢你覺得你覺得我們下週來看看2月26號實際的值是多少但是你覺得這個值他跟真實值的誤差會小於0.38K的同學舉手一下絕對大於0.38K的同學舉手一下哇好可怕大家都對我這麼沒有信心這樣好我們就來看看這個下一週誤差會有多少但是我想應該是不會準啦因為你看這麼多人都覺得誤差會大你們回去每個人都去點那個影片的話哇誤差就大了今天講這麼久其實就是騙大家 去點影片而已啦好那今天其實就講了深度學習那今天講的不是一般的介紹方式如果想要聽一般的介紹方式過去的課程影片也是有的我就把連結附在這邊然後深度學習的訓練會用到一個東西叫Fabrication其實它就是比較有效率算規範的方法跟我們今天講的東西沒有什麼不同但如果你真的很想知道Fabrication是什麼的話影片連結也附在這邊好今天上課就上到這邊 謝謝大家謝謝