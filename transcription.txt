# 01. MCP 的核心概念与基础功能

- **MCP 的全称**：Model Context Protocol（简称 MCP），由Anthropic 开发
- **核心概念**：
  - **数据流转**：MCP 是一个分布式协议，允许不同硬件设备之间进行信息传递和处理
  - **基本用法**：
    - 提供跨设备交互的基础框架
    - 支持多用户环境下的协作与协同工作

## 进阶挑战（挑战 MCP 的进阶能力）

### 从零构建 MCP Server

- **基本功能**：通过配置 Python 库，实现对本地或异构网络的 MCP 环境搭建
- **挑战点**：
  - 创建完整的 MCP Server 需要熟悉文件结构和配置方式
  - 实现异构环境下的数据交互需要理解不同设备的通信协议

### 掌握进阶功能

- **从零构建 MCP Server 的具体实现**：
  - 使用 VS Code 编程插件“MCP Host”
  - 加载 OpenRouter 广室（OpenRouter）作为 MCP 齿轮
  - 运行模型配置，选择最优质的 MCP 模型进行训练

## 尝试解决实际应用问题

- **问题：如何配置天气 MCP？**
  - 新增 MCP 函数，将 MAPI 调用到 Google 天气 API 中
  - 确保 API 设置符合 MCP 标准
  - 实现异构环境的天气交互，确保数据一致性

## 小结与展望

通过本次整理，我们对 MCP 的核心概念有了清晰的理解，并认识到其在跨设备协作中的重要性。未来，随着技术的不断进步，MCP 可能会扩展到更多场景的应用中，例如智能硬件、物联网设备等。或者是Python启动的只不过在使用的过程中可能会连网当然它也可能不连网纯本地使用也是可以的不管是连不连网它都可以叫做MCP Server所以我觉得MCP Server的这个名字里面带Server这个词是有一定的误导性的你不要觉得这个玩意儿很高端 很玄妙它其实本质上就是一个程序跟你手机上面的应用没有什么太大的区别不管是MCP Server还是手机应用它们都内置了一些功能模块来解决你的问题我用iPhone上面的时钟进行举例它内置了四个功能模块分别是世界时钟 闹钟 钟表 计时器分别可以给我们解决四个场景的诉求而刚才Client想要给我们安装的这个Open Weather Map


略有不同的问题时它又会给出一个错的离谱的答案让你觉得它根本什么都不懂只是一个更高级大家好 这里是最佳拍档我是大飞经常接触大模型的朋友一定会有过这样的经历当你向AI提出一个复杂的问题时它不仅给出了答案还给出了详细的一步一步的看起来逻辑严密的解题过程那一瞬间你是不是感觉屏幕对面的不再是一堆冰冷的代码而是一个真正能够推理的智能体但是有的时候你换一个同样复杂但是略有不同的问题时它又会给出一个错的离谱的答案让你觉得它根本什么都不懂只是一个更高级的复读机这种体验上的巨大反差正是当前AI领域最核心的谜题之一大圆模型展现出的推理能力究竟是一种真正的智能涌现还是一种基于海量数据训练出来的更高级的模式匹配他是在進行邏輯推導還是在模仿他在網上看過的無數解題步驟關於這個問題學術界和工業界爭論不休但是爭論的意義遠不如我們去搞清楚這種所謂的推理能力到底是怎麼來的以及我們如何才能穩定的可靠的駕馭它踏入 Wix 工作室,從一個平台開始處理每個部份的建築。將你的項目圖畫出來,用 AI 能力的視覺地圖圖畫,能夠在秒數,而不是日子內製造視覺結構和線條。當你的客戶登機後,進入設計,自由地創作,甚至可以自行調整最小的元素。使用視覺方式,以積極地延伸品牌到每個頁面,並觀察它適應地穿越磨擦點。Now, Scale.而要解答這個問題有一個人的名字我們就繞不開他就是來自於谷歌DingMine的丹妮周他和他的團隊可以說是奠定了我們今天理解和使用大圓模型推理能力的基石他們開創性地提出了像思維鏈提示和自洽性這類的關鍵技術並且深度參與了谷歌靜奶模型推理能力的構建最近丹妮周在斯坦福大學做了一場演講系統性地梳理了從他創立谷歌大腦的推理團隊開始到今天我們所看到的強大的AI這條技術路線是如何一步一步演進的這場講座的信息量巨大它不僅揭示了AI推理能力的本質更是對過去幾年中所有相關技術的一次趣味所以今天這期視頻我們就將以丹妮周的這場講座為藍本帶著大家從最基礎的概念出發層層遞進徹底搞懂大圓模型思考秘密我們會看到那些看似神奇的技術並且會看到這些看似神奇的技術是如何一步一步的發展我們會看到這些看似神奇的技術是如何一步一步的發展我們會看到這些看似神奇的技術是如何一步一步的發展往往遵循著一些極其簡單而深刻的原理相信看完這些視頻你再看待大圓模型的時候會有一個全新的更加清晰的視角在深入探討之前我們必須先明確一件事情當我們討論大圓模型的推理時我們到底在談論什麼單機周一上來就拋出了一個非常清晰而且可操作的定義這個定義也成為了整個領域的共識他說關於模型到底會不會推理的哲學辯論他從不參加因為沒有一個明確的定義大家都是在自說自話而在他的團隊裡推理是有一個非常具體的含義的那就是在模型的輸入也就是你的問題和最終輸出也就是答案之間生成的所有中間步驟這個定義非常關鍵他把一個模糊的哲學層面的思考概念轉化成了一個具體的工程上可以實現和優化的目標為了讓我們更好的理解這個問題丹妮周设计了一个非常巧妙的任务叫做末尾字母拼接这个任务听起来很简单比如说我问模型请拼接artificial intelligence这两个单词的末尾字母如果我们直接让模型输出答案它可能会凭借着语言的惯性直接猜一个答案比如说LE这时候它只是在预测下一个最可能的字符而不是在执行一个多步骤的逻辑操作但是如果我们引导模型先生成中间步骤它的输出就会变成这样artificial的最后一个字母是Lintelligence的最后一个字母是E将L和E拼接起来得到LE这就是丹妮周所定义的推理她把一个复杂的任务分解成了一系列简单的可执行的子任务最终导出了正确的答案你可能会觉得这不就是我们人类解决问题的方式吗先思考再作答但是丹妮周提醒我们作为研究者你应该先想清楚你应该先想清楚你应该先想清楚你应该先想清楚你应该先想清楚你应该先想清楚你应该先想清楚你应该先想清楚你应该先想清楚你应该先想清楚你应该先想清楚必须时刻记住大猿模型不是人类它们只是概率模型把它们拟人化虽然有助于我们理解但是也很容易让我们走入歧途一个有趣的故事是丹琪周最初尝试的是首字母拼接但是他发现当时所有的模型都能够做得很好为什么呢因为互联网上有大量的缩写词模型在预训练阶段已经背会了如何拼接首字母于是他换成了末尾字母拼接结果当时所有的模型都失败了这恰恰说明模型并没有真正理解拼接这个动作而只是记住了某种常见的模式那么为什么要如此执着于生成这些中间步骤呢仅仅是为了模仿人类吗当然不是这背后有着非常坚实的理论依据丹琪周提到了他们和斯坦福大学教授滕上华团队合作的一项理论研究这个研究得出了一个非常强大的结论对于任何一个可以被大小都可以被拼接的模型来说是一个非常强大的证据这就是为什么丹琪周在2014年在中国的一个大学研究院他发现了一个模型它是一个非常强大的模型为T的布尔电路解决的问题一个常数大小的transformer模型可以通过生成OT长度的中间步骤来解决它这句话听起来有点过于技术我们来把它翻译一下布尔电路可以被看作是执行逻辑运算的基本单元任何复杂的计算任务比如说运行一个大型的软件本质上都可以被分解成一个巨大规模的布尔电路这里的大小T就代表了问题的计算复杂度这个理论告诉我们哪怕是一个相对简单的transformer模型只要你允许它生成也就是中间步骤它就有潜力解决几乎任何可计算的问题反过来说如果我们强迫模型直接蹦出最终答案就相当于要求这个模型的网络深度本身要能够模拟整个复杂的计算过程这要么需要一个巨大到不切实际的深度要么就根本无法解决问题所以让模型的计算方式是一种很简单的计算方式生成中間步驟不是一個可有可無的選項而是在計算原理上解鎖模型解決複雜問題能力的一把金鑰匙這徹底改變了我們訓練和使用大圓模型的範式從單純的追求答案轉向追求過程好 既然我們知道了推理過程如此的重要那麼下一個問題自然就是如何讓模型來生成這個過程呢這裡丹妮周提出了一個顛覆了當時很多人認知的觀點當時普遍認為一個普通的只經過預訓練的大圓模型是不會推理的你必須通過像思維鏈提示這樣的高級技巧或者進行專門的微調才能夠教會他們推理但是丹妮周說這個觀點是錯的而且大錯特錯她認為預訓練模型早就已經準備好進行推理了我們所需要做的僅僅是改變解碼的過程這又是一個非常深刻的洞察為了證明這一點她舉了一個經典的例子的数学硬题我有三个苹果我的爸爸比我多两个苹果我们总共有多少个苹果如果你把这个问题直接输入给一个原始的预训的模型比如说早期的GPT-3或者是拉马然后使用默认的贪婪解码方式会发生什么呢贪婪解码的意思是模型在生成每一个词的时候总是会选择当前概率最高的那一个在这种模式下模型很可能会直接输出一个看似合理但却是错误的答案比如说5个苹果因为他看到了3个和多两个就直接联想到了5这是模型的一种直觉反应接著丹妮周向我們展示了這些隱藏的候選答案比如說候選二可能以我字開頭模型會聲稱我有三個蘋果我爸爸比我多兩個所以他有五個蘋果三加五等於八所以我們總共有八個蘋果這是一個完美的推理鏈答案也正確候選三可能以我們開頭模型會聲稱我們總共有八個蘋果雖然沒有過程但是答案也對了候選四可能以你字開頭模型會聲稱你有三個蘋果你爸爸有三加二等於五個蘋果你們總共有三加五等於八個蘋果這同樣是一個清晰的推理鏈看到了嗎其實一直都存在於模型的輸出空間裏他們就像是隱藏在主幹道旁邊的小路默認的貪婪解碼因為只看到了眼前最寬的路所以錯過了他們這個發現被稱為思維鏈解碼它告訴我們推理能力不是被注入到模型裏面的而是模型在選擇的時間裏面海量文本中蘊含的邏輯關係之後自然湧現出來的於是我們的任務從交會模型推理變成了如何引導模型把他已經知道的東西以正確的形式表達出來那麼問題就變成了在這麼多候選的輸出裏我們怎麽知道哪一個是最好的呢一個簡單的想法是看長度在有思考過程的回答通常會更長但是丹妮周的團隊發現一個更可靠的指標那就是答案自信度他們觀察到一個驚人的現象對於那些包含了正確思維鏈的回答模型在生成最終答案的那個詞比如說數字8的時候其內部的自信度也就是概率會異常的高在蘋果的例子裏模型預測8這個詞的概率可能會高達98%這是一個非常強的型號因為對於一個擁有巨大詞彙表的模型來說通常每個詞的概率都接近於0這就像是一個人在經過深思熟慮之後他想要的答案对自己得出的结论会非常笃定一样所以说思维链解码的核心就两步一超越贪婪解码生成并且检查更多的候选输出二选择那个对最终答案自信度最高的候选这个方法虽然简单有效但还是需要写一些代码对于普通用户来说不够友好于是研究者们开始思考我们能不能用更自然的方式比如说自然语言来重塑模型的输出概率分布让那些带有思考过程的优秀答案能够自动排到第一名这样我们用最简单的贪婪解码就能够直接得到它这就引出了我们后来耳熟能详的一系列提示工程技术首先最著名的就是思维链提示它的做法非常直观在你提出你的问题之前先给模型看一两个类似的从问题到思考过程再到答案的例子比如说你想让模型解决前面苹果的问题你可以先给它一个例子问题是一个农村農民有5個香蕉他又買了6個後來吃了2個還剩幾個答案是農民開始有5個香蕉買了6個之後他有5加6等於11個然後他吃了2個所以他剩下11減2等於9個答案是9然後你再提出你的問題我有3個蘋果我爸爸比我多2個我們總共有多少個蘋果神奇的事情發生了模型會模仿你給出的例子的風格自動的開始一步一步分析生成詳細的解題步驟最後給出正確答案從概率分佈的角度來看你給出的例子極大的提升了模型生成類似思考過程的巨式的概率反而把原本隱藏在後面的正確推理路徑推到了最前面但是這種方法有一個問題你需要為不同類型的任務手動編寫高質量的實例這很麻煩而且如果你自己都知道怎麼解決一個類似的問題那你為什麼還要問AI呢於是一個更加神奇的提示就出現了它就是讓我們以後一步一步思考Let's think step by step但尼州坦言這篇論文剛出來的時候他以為這是個玩笑怎麼可能在問題後面加上這麼一句簡單的話模型就會自動開始思考了呢他當時就在谷歌內部的PALM模型上做了測試他非常清楚PALM的訓練數據裏絕對沒有針對這個咒語做過任何的優化結果他震驚地發現它真的有效模型真的開始輸出一步一步的解題過程了這個發現極大的啟發了他儘管Let's think step by step這種零樣本提示效果通常比不過提供具體事例的少樣本思維鏈提示但是它證明了我們可以用非常通用的方式來激發模型的推理潛能然而無論是哪種提示方法都感覺有點奇怪想象一下你問一個聰明人問題還必須得在後面加上一句請一步一步思考否則他就不會思考了這顯然不符合我們對於一個真正智能體系的看法所以我们需要一种更稳定更内化的方式让推理能力成为模型固有的一部分而不是需要外部咒语来触发这就把我们带到了下一个阶段微调 我们先说尖度微调SFT它的思路非常的直接我们不就是希望模型能够生成从问题到思考过程再到答案这样的数据吗那我们就雇佣一批人针对大量的问题写出高质量的一步一步的解题方案然后我们再把这些标准答案喂给模型 让模型去学习这个方法在机器学习里边叫做最大自然估计简单来说就是让模型生成的序列跟人类专家写的序列尽可能的一模一样这个想法其实很早就有了单极周提到早在2017年丁麦的一篇论文就在做类似的事情他们收集了一批数学应用题和人类手写的解题步骤来训练一个序列模型后来在2021年OPI更进一步构建了一个更稳定的模型更著名的數據集也就是GSM8K包含了8000多個小學水平的數學題和詳細解法用來微調GPT-3模型這種方法訓練出來的模型在你給它一個新問題的時候確實能夠生成不錯的解析步驟看起來問題似乎解決了一旦模型訓練好就可以隨時部署不再需要複雜的提示了然而在2021年夏天丹尼州的團隊發現了一個嚴重的問題那就是SFT訓練出來的模型算話能力很差它在那些和訓練數據很相似的問題上表現很好但是一旦遇到一個新的類型稍微不同的問題就很容易失敗他們嘗試了大力出奇技的方法擴大了數據的規模找更多的人標註更多的數據可惜結果卻是無論如何擴大規模這個問題始終存在丹尼州在這裡給出了一個重要的教訓不要盲目的擴大規模當你的範式本身是錯誤的時候再多的數據也不代表你那麼SFT的範式錯在哪裡了呢問題又出在流程裏的哪一步呢丹妮周給出的答案可能會讓你大吃一驚她說錯誤出在人身上這個轉折點來自於自我提升後來也被稱為self-improve或者是start方法當她第一次聽到機器生成的訓練數據可能比人類專家寫得還好這個想法的時候她自己也感到非常驚訝這個新範式的流程是這樣的首先我們仍然從一批問題開始但是我們不再找人類去寫解題步驟我們讓一個已經比較強大的大圓模型自己去針對這些問題生成大量的多樣的解題步驟最關鍵的一步是我們用一個驗證器去檢查模型生成的這些解題步驟看哪個最終得出了正確的答案比如說對於數學題我們知道標準答案就可以直接的判斷於是我們只保留下來那些過程多樣但是結果正確的生成結果把解題步驟寫出來把它们当作新的高质量的训练数据然后用这些由模型自己生成的并且经过验证的好数据再去微调模型自己这个过程可以不断地迭代一个微调后变得更强的模型又可以去生成质量更高更复杂的解题步骤用来进一步的训练自己这就形成了一个自我进化的闭环赖铁周提到一篇在2024年1月发表的来自字节跳动的论文Reasoning with reinforced for ITUny是他在学术界看到的最早公开阐述类似思想的出版物之一他相信在OpenAI等多个机构内部大家可能都独立地发现了这个简单而又极其有效的思想现在我们必须回答那个核心的问题为什么模型自己生成的数据会比人类专家手写的数据在训练效果上更好呢这背后其实蕴含着技艺学习的一个第一性原理那就是直接优化你想要的东西在SFT的范式里我们优化的目标是让模型更有效我是大飞经常接触大模型的朋友一定会有过这样的经历当你向AI提出一个复杂的问题时它不仅给出了答案还给出了详细的一步一步的看起来逻辑严密的解题过程那一瞬间你是不是感觉屏幕对面的不再是一堆冰冷的代码而是一个真正能够推理的智能体但是有的时候你换一个同样复杂但是略有不同的问题时它又会给出一个错的离谱的答案让你觉得它根本什么都不懂只是一个更高级的复读机这种体验上的巨大反差正是当前AI领域最核心的谜题之一大元模型展现出的推理能力究竟是一种真正的智能涌现还是一种基于海量数据训练出来的更高级的模式匹配它是在进行逻辑推导还是在模仿它在网上看过的无数解题步骤关于这个问题学术界和工业界争论不休但是我们还是要看但是争论的意义远不如我们去搞清楚这种所谓的推理能力到底是怎么来的以及我们如何才能稳定的可靠的驾驭它而要解答这个问题有一个人的名字我们就绕不开他就是来自于谷歌丁曼的丹尼周他和他的团队可以说是奠定了我们今天理解和使用大元模型推理能力的基石他们开创性的提出了像思维链提示和自下行这类的关键技术并且深度参与了谷歌静态模型推理能力的构建最近丹尼周在斯坦福大学做了一场演讲他系统性的梳理了从他创立谷歌大脑的推理团队开始到今天我们所看到的强大的AI这条技术路线是如何一步一步演进的这场讲座的信息量巨大它不仅揭示了AI推理能力的本质更是对过去几年中所有相关技术的一次趣味所以今天这期视频我们就将以丹尼周的这场讲座为了帶著大家從最基礎的概念出發層層遞進徹底搞懂大約模型思考秘密我們會看到那些看似神奇的技術背後往往遵循著一些極其簡單而深刻的原理相信看完這期視頻你再看待大約模型的時候會有一個全新的更加清晰的視角在深入探討之前我們必須先明確一件事情當我們討論大約模型的推理時我們到底在談論什麼單機中一上來就拋出了一個非常清晰而且可操作的定義這個定義也成為了整個領域的共識他說關於模型到底會不會推理的哲學辯論因為沒有一個明確的定義大家都是在自說自話而在他的團隊裡推理是有一個非常具體的含義的那就是在模型的輸入也就是你的問題和最終輸出也就是答案之間生成的所有中間步驟這個定義非常關鍵他把一個很簡單的模糊的哲学层面的思考概念转化成了一个具体的工程上可以实现和优化的目标为了让我们更好的理解这个目标丹尼周设计了一个非常巧妙的任务叫做末尾字母拼接这个任务听起来很简单比如说我问模型请拼接artificial intelligence这两个单词的末尾字母如果我们直接让模型输出答案它可能会凭借着语言的惯性直接猜一个答案比如说LE这时候它只是在预测下一个最可能的字符而不是在执行一个多步骤的逻辑操作但是如果我们引导模型先生成中间步骤它的输出就会变成这样artificial的最后一个字母是Lintelligence的最后一个字母是E将L和E拼接起来得到LE这就是丹尼周所定义的推理他把一个复杂的任务分解成了一系列简单的可执行的子任务最终导出了一个正確答案现在我们可以更好地加入功能符合你的工作模式我会加入掃描收入 数据化和支付费用现在我可以在照片中立刻刷出一张图片没有任何的手册费用最后我们加入费用预测档预测月和年费现在你有一个应用软件专注于你的生意需要建立在你工作的方式里开始你的工作你可能会觉得这不就是我们人类解决问题的方式吗先思考再作答但是赶紧周提醒我们作为研究者必须时刻记住大猿模型无视人类他們只是概率模型把他們擬人化雖然有助於我們理解但是也很容易讓我們走入歧途一個有趣的故事是丹琪周最初嘗試的是手字母拼接但是他發現當時所有的模型都能夠做得很好為什麼呢因為互聯網上有大量的縮寫詞模型在預訓練階段已經背會了如何拼接手字母於是他換成了末尾字母拼接結果當時所有的模型都失敗了這恰恰說明模型並沒有真正理解拼接這個動作而只是記住了某種常見的模式那麼為什麼要如此執著於生成這些中間步驟呢僅僅是為了模仿人類嗎當然不是這背後有著非常堅實的理論依據丹琪周提到了他們和斯坦福大學教授滕上華團隊合作的一項理論研究這個研究得出了一個非常強大的結論對於任何一個可以被大小為T的物理電路解決的問題一個常識的問題可以通过生成OT长度的中间步骤来解决它这句话听起来有点过于技术我们来把它翻译一下布尔电路可以被看作是执行逻辑运算的基本单元任何复杂的计算任务比如说运行一个大型的软件本质上都可以被分解成一个巨大规模的布尔电路这里的大小T就代表了问题的计算复杂度这个理论告诉我们哪怕是一个相对简单的Transformer模型只要你允许它生成足够长的思考过程也就是中间步骤它就有潜力解决几乎任何可计算的问题反过来说如果我们强迫模型直接蹦出最终答案就相当于要求这个模型的网络深度本身要能够模拟整个复杂的计算过程这要么需要一个巨大到不切实际的深度要么就根本无法解决问题所以让模型思考生成中间步骤不是一个可有可无的计算过程而是在计算原理上解锁模型解决复杂问题能力的一把金钥匙这彻底改变了我们训练和使用大学模型的方式从单纯的追求答案转向追求过程好既然我们知道了推理过程如此的重要那么下一个问题自然就是如何让模型来生成这个过程呢这里呢丹妮周提出了一个颠覆了当时很多人认知的观点当时普遍认为一个普通的只经过预训练的大学模型是不会推理的你必须通过像思维链提示这样的高级技巧或者进行专门的微调才能够交配但是丹妮周说这个观点是错的而且大错特错她认为预训的模型早就已经准备好进行推理了我们所需要做的仅仅是改变解码的过程这又是一个非常深刻的洞察为了证明这一点她举了一个经典的数学硬题我們總共有多少個蘋果如果你把這個問題直接輸入給一個原始的預訓練模型比如說早期的GDP3或者是拉馬然後使用默認的貪婪解碼方式會發生什麼呢貪婪解碼的意思是模型在生成每一個詞的時候總是會選擇當前概率最高的那一個在這種模式下模型很可能會直接輸出一個看似合理但卻是錯誤的答案比如說5個蘋果因為它看到了3個和多2個就直接聯想到了5這是模型的一種直覺反應或者說是一種系統思維但是模型的強大之處在於它的輸出概率分佈中並不僅僅只有這一個選項在生成第一個詞的時候5個可能是概率最高的但是還有第二第三第四高的選項如果我們不那麼貪婪而是去探索一下那些概率稍低一些的岔路奇蹟就會發生了接著丹妮周向我們展示了這些隱藏的候選答案比如說就說候選二可能以我字開頭模型會聲稱我有三個蘋果我爸爸比我多兩個所以他有五個蘋果三加五等於八所以我們總共有八個蘋果這是一個完美的推理鏈答案也正確候選三可能以我們開頭模型會聲稱我們總共有八個蘋果雖然沒有過程但是答案也對了候選四可能以你字開頭模型會聲稱你有三個蘋果你爸爸有三加二等於五個蘋果你們總共有三加五等於八個蘋果這同樣是一個清晰的推理鏈看到了嗎正確的推理路徑其實一直都存在於模型的輸出空間裏它們就像是隱藏在主幹道旁邊的小路默認的貪婪解碼因為只看到了眼前最寬的路所以錯過了它們這個發現被稱為思維鏈解碼它告訴我們推理能力不是被注入到模型裏面的而是模型在學習海量文本中蘊含的邏輯關系之後自然有了這個能力於是我們的任務從教會模型推理變成了如何引導模型把他已經知道的東西以正確的形式表達出來那麼問題就變成了在這麼多候選的輸出裏我們怎麽知道哪一個是最好的呢一個簡單的想法是看長度在有思考過程的回答通常會更長但是丹尼州的團隊發現一個更可靠的指標那就是答案自信度他們觀察到一個驚人的現象對於那些包含了正確思維鏈的回答模型在生成最終答案的那個詞比如說數字8的時候其內部的自信度也就是概率會異常的高在這個蘋果的例子裏模型預測8這個詞的概率可能會高達98%這是一個非常強的信號因為對於一個擁有巨大詞彙表的模型來說通常每個詞的概率都接近於0這就像是一個人在經過深思熟慮之後對自己得出的結論會非常篤定一樣所以說思维链解码的核心就两步一超越贪婪解码生成并且检查更多的候选输出二选择那个对最终答案自信度最高的候选这个方法虽然简单有效但还是需要写一些代码对于普通用户来说不够友好于是研究者们开始思考我们能不能用更自然的方式比如说自然语言来重塑模型的输出概率分布让那些带有思考过程的优秀答案能够自动排到第一名这样我们用最简单的贪婪解码就能够直接得到它这就引出了我们后来耳熟能详的一系列提示工程技术首先最著名的就是思维链提示它的做法非常直观在你提出你的问题之前先给模型看一两个类似的从问题到思考过程再到答案的例子比如说你想让模型解决前面那个苹果的问题你可以先给它一个例子问题是一个农民有五个香蕉他又买了六个后来吃了两个还吃了三个这就叫做贪婪解码这就是我们的思考过程我们的思考过程我们的思考过程我们的思考过程我们的思考过程答案是農民開始有五個香蕉買了六個之後他有5加6等於11個然後他吃了兩個所以他剩下11減2等於9個答案是9然後你再提出你的問題我有三個蘋果我爸爸比我多兩個我們總共有多少個蘋果神奇的事情發生了模型會模仿你給出的例子的風格自動的開始一步一步分析生成詳細的解題步驟最後給出正確答案從概率分佈的角度來看你給出的例子極大的提升了模型生成類似思考過程的巨式的概率從而把原本隱藏在後面的正確推理路徑推到了最前面但是這種方法有一個問題你需要為不同類型的任務手動編寫高質量的實例這很麻煩而且如果你自己都知道怎麼解決一個類似的問題那你為什麼還要問AI呢於是一個更加神奇的提示就出現了它就是讓我們一步一步思考Let's think step by step單機周談一下这篇论文刚出来的时候他以为这是个玩笑怎么可能在问题后面加上这么一句简单的话模型就会自动开始思考了呢他当时就在谷歌内部的PALM模型上做了测试他非常清楚PALM的训练数据里绝对没有针对这个咒语做过任何的优化结果他震惊地发现它真的有效模型真的开始输出一步一步的解题过程了这个发现极大的启发了他尽管Let's think step by step这种零样本提示效果通常比不过提供具体示例的但是他证明了我们可以用非常通用的方式来激发模型的推理潜能然而无论是哪种提示方法都感觉有点奇怪想象一下你问一个聪明人问题还必须得在后面加上一句请一步一步思考否则他就不会思考了这显然不符合我们对于一个真正智能体的期望所以我们需要一种更稳定更内化的方式讓推理能力成為模型固有的一部分而不是需要外部咒語來觸發這就把我們帶到了下一個階段微調我們先說監督微調SFT它的思路非常的直接我們不就是希望模型能夠生成從問題到思考過程再到答案這樣的數據嗎那我們就僱用一批人針對大量的問題寫出高質量的一步一步的解題方案然後我們再把這些標準答案背給模型讓模型去學習這個方法在機器學習裏面叫做最大三人估計簡單來說就是讓模型生成的序列跟人類專家寫的序列盡可能的一模一樣這個想法其實很早就有了單機周提到早在2017年DMITE的一篇論文就在做類似的事情他們收集了一批數學應用題和人類手寫的解題步驟來訓練一個序列模型後來在2021年OPI更進一步構建了一個更大更著名的數據集也就是GSM8K包含了百分之一百的模型大圆模型展现出的推理能力究竟是一种真正的智能涌现还是一种基于海量数据训练出来的更高级的模式匹配呢它是在进行逻辑推导还是在模仿它在网上看过的无数解题步骤呢关于这个问题学术界和工业界争论不休但是争论的意义远不如我们去搞清楚这种所谓的推理能力到底是怎么来的以及我们如何才能稳定的可靠的驾驭它而要解答这个问题有一个人的名字我们就绕不开他就是来自于谷歌丁曼的丹尼周他和他的团队可以说是奠定了我们今天理解和使用大圆模型推理能力的基石他们开创性的提出了像思维链提示和自下行这类的关键技术并且深度参与了谷歌金本奶模型推理能力的构建最近丹尼周在斯坦福大学做了一场演讲系统性的梳理了从他创立谷歌丁曼的基础大腦的推理團隊開始到今天我們所看到的強大的AI這條技術路線是如何一步一步演進的這場講座的信息量巨大它不僅揭示了AI推理能力的本質更是對過去幾年中所有相關技術的一次去媚所以今天這期視頻我們就將以丹尼周的這場講座為藍本帶著大家從最基礎的概念出發層層遞進徹底搞懂大約模型思考秘密我們會看到那些看似神奇的技術背後往往遵循著一些極其簡單而深刻的原理相信看完這期視頻你再看待大約模型的時候會有一個全新的更加清晰的視角在深入探討之前我們必須先明確一件事情當我們討論大約模型的推理時我們到底在談論什麼丹尼周一上來就拋出了一個非常清晰而且可操作的定義這個定義也成為了整個領域的共識他說關於模型到底会不会推理的哲学辩论他从不参加因为没有一个明确的定义大家都是在自说自话而在他的团队里推理是有一个非常具体的含义的那就是在模型的输入也就是你的问题和最终输出也就是答案之间生成的所有中间步骤这个定义非常关键他把一个模糊的哲学层面的思考概念转化成了一个具体的工程上可以实现和优化的目标为了让我们更好的理解这个目标丹尼周设计了一个非常巧妙的任务叫做末尾字母拼接这个任务听起来很简单比如说我问模型请拼接artificial intelligence这两个单词的末尾字母如果我们直接让模型输出答案它可能会凭借着语言的惯性直接猜一个答案比如说LE这时候它只是在预测下一个最可能的字符而不是在执行一个多步骤的逻辑操作但是如果我们拼接这个最可能的字符它可能会直接猜一个答案比如说LE这时候它只是在预测下一个最可能的字符而不是在执行一个多步骤的逻辑操作但是如果我们拼接artificial intelligence如果我们引导模型先生成中间步骤它的输出就会变成这样artificial的最后一个字母是Lintelligence的最后一个字母是E将L和E拼接起来得到LE这就是丹妮周所定义的推理她把一个复杂的任务分解成了一系列简单的可执行的子任务最终导出了正确的答案你可能会觉得这不就是我们人类解决问题的方式吗先思考再作答但是丹妮周提醒我们作为研究者必须时刻记住大猿模型不是人类它们只是概率模型把它们拟人化虽然有助于我们理解但是也很容易让我们走入歧途一个有趣的故事是丹妮周最初尝试的是首字母拼接但是她发现当时所有的模型都能够做得很好为什么呢因为互联网上有大量的缩写词模型在预训那阶段已经背会了如何拼接首字母于是她换成了末尾字母拼接结果当时所有的模型都失败了这恰恰说明模型并没有真正理解拼接这个动作而只是记住了某种常见的模式那么为什么要如此执着于生成这些中间步骤呢仅仅是为了模仿人类吗当然不是这背后有着非常坚实的理论依据丹琪州提到了他们和斯坦福大学教授腾讯华团队合作的一项理论研究这个研究得出了一个非常强大的结论对于任何一个可以被大小为T的布尔电路解决的问题一个常数大小的Transformer模型可以通过生成OT长度的中间步骤来解决它这句话听起来有点过于技术我们来把它翻译一下布尔电路可以被看作是执行逻辑运算的基本单元任何复杂的计算任务比如说运行一个大型的软件本质上都可以被分解成一个巨大规模的布尔电路这里的大小T就代表了问题的计算复杂度这个理论告诉我们哪怕是一个相对简单的transformer模型只要你允许它生成足够长的思考过程也就是中间步骤它就有潜力解决几乎任何可计算的问题反过来说如果我们强迫模型直接蹦出最终答案就相当于要求这个模型的网络深度本身要能够模拟整个复杂的计算过程这要么需要一个巨大到不切实际的深度要么就根本无法解决问题所以让模型思考生成中间步骤不是一个可有可无的选项而是在计算原理上解锁模型解决复杂问题能力的一把金钥匙这彻底改变了我们训练和使用大学模型的范式从单纯的追求答案转向追求过程好 既然我们知道了推理过程如此的重要那么下一个问题自然就是如何让模型来生成这个过程呢这里丹妮周提出了一个颠覆了当时很多人认知的最简单的模型思考方法就是用模型思考的方法来解决问题这种方法是非常简单的就是用模型思考的方法来解决问题然后用模型思考的方法来解决问题当时普遍认为一个普通的只经过预训练的大圆模型是不会推理的你必须通过像思维链提示这样的高级技巧或者进行专门的微调才能够教会他们推理但是丹尼周说这个观点是错的而且大错特错他认为预训模型早就已经准备好进行推理了我们所需要做的仅仅是改变解码过程全面的设计和设置都包括了全面的设计 没有微管使用图形设计来连接多个桌子并且设置不同的团队同样的位置没有项目费用没有需要的加装没有无聊的日常站台搅拌一下我们的Shuffle模式并且给每个团队员提供AI的建议没有什么像不可预料的公开说话的好玩MondayDev的GitHub的融合使用了强大的自动化以给你全面的设计进行日常进步并且让你的发展者能够在GitHub和关注CodeGeek不需要手册更新Monday Dev计划 追踪 和 运行程序 迅速订阅免费试试不需要信用卡Atera是IT的痛苦凶手遥控和管理遥控和接收助援设备和订单聪明的自动化编程 软件解决编程最神奇的是它能做到这些却又轻易使用真是神奇只是一个清晰 优雅的UI所有东西都在它应该的位置它就做了它应该做的事情对不起我感到激动不过以伪想性警告你解决了真正的IT问题才會成為IT的問題對不起我聽不懂你在修理電腦的聲音有多酷以後以後在科學上的報告管理層終於明白我為什麼是真正的MVP另外以設備價格來看這就省了50%的IT費用讓我成為新公司的英雄你的腦袋在發瘋真正的英雄是戴著耳機你會不會也許不會你不需要買任何東西你可以現在立即試試免費的 設置成風景只要點擊連結 試試看很棒Atera這是我需要的接受餅乾餅乾接受了这又是一个非常深刻的洞察为了证明这一点他举了一个经典的数学硬题我有三个苹果我的爸爸比我多两个苹果我们总共有多少个苹果如果你把这个问题直接输入给一个原始的预训量模型比如说早期的GPT-3或者是拉玛然后使用默认的贪婪解码方式会发生什么呢贪婪解码的意思是模型在生成每一个词的时候总是会选择当前概率最高的那一个在这种模式下模型很可能会直接输出一个看似合理但却是错误的答案比如说五个苹果那些概率稍低一些的岔路奇迹就会发生了接着单机周向我们展示了这些隐藏的候选答案比如说候选二可能以我字开头模型会生成我有三个苹果我爸爸比我多两个所以他有五个苹果3加5等于8所以我们总共有八个苹果这是一个完美的推理链答案也正确候选三可能以我们开头模型会生成我们总共有八个苹果虽然没有过程但是答案也对了候选四可能以你字开头模型会生成你有三个苹果你爸爸有3加2等于五个苹果你们总共有3加5等于八个苹果这同样是一个清晰的推理链看到了吗正确的推理路径其实一直都存在于模型的输出空间里它们就像是隐藏在主干道旁边的小路默认的贪婪解码因为只看到了眼前最宽的路所以错过了它们这个发现被称为思维链解码它告诉我们推理能力不是被注入到模型里面的而是模型在学习海量文本中蕴含的逻辑关系之后自然涌现出来的于是我们的任务从交汇模型推理变成了如何引导模型把它已经知道的东西以正确的形式表达出来那么问题就变成了在这么多候选的输出里我们怎么知道哪一个是最好的呢一个简单的想法是看长度带有思考过程的回答通常会更长但是丹基州的团队发现一个更可靠的指标那就是答案自信度他们观察到一个惊人的现象对于那些包含了正确思维链的回答模型在生成最终答案的那个词比如说数字8的时候其内部的自信度也就是概率会异常的高在这个苹果的例子里模型预测8这个词的概率可能会高达98%这是一个非常强的信号因为对于一个拥有巨大词汇表的模型来说通常每个词的概率都会比较高都接近于0这就像是一个人在经过深思熟虑之后对自己得出的结论会非常笃定一样所以说思维链解码的核心就两步一超越贪婪解码生成并且检查更多的候选输出二选择那个对最终答案自信度最高的候选这个方法虽然简单有效但还是需要写一些代码对于普通用户来说不够友好于是研究者们开始思考我们能不能用更自然的方式比如说自然语言来重塑模型的输出概率分布让那些带有思考过程的优秀答案能够自动排到第一名这样我们用最简单的贪婪解码就能够直接得到它这就引出了我们后来耳熟能详的一系列提示工程技术首先最著名的就是思维链提示它的做法非常直观在你提出你的问题之前先给模型看一两个类似的从问题到思考过程再到答案的例子比如说你想让模型解决前面那个苹果的问题你可以先给他一个例子问题是一个农民有5个香蕉他又买了6个后来吃了两个还剩几个答案是农民开始有5个香蕉买了6个之后他有5加6等于11个然后他吃了两个所以他剩下11-2等于9个答案是9然后你再提出你的问题我有三个苹果我爸爸比我多两个我们总共有多少个苹果神奇的事情发生了模型会模仿你给出的例子的风格自动的开始一步一步分析生成详细的解题步骤最后给出正确答案从概率分布的角度来看你给出的例子极大的提升了模型生成类似思考过程的巨式的概率反而把原本隐藏在后面的正确推理路径推到了最前面但是这种方法有一个问题你需要为不同类型的任务手动编写高质量的实例这很麻烦而且如果你自己都知道怎么解决一个类似的问题那你为什么还要问AI呢因为你自己都知道於是一個更加神奇的提示就出現了它就是讓我們一步一步思考Let's think step by step但尼周坦言這篇論文剛出來的時候他以為這是個玩笑怎麼可能在問題後面加上這麼一句簡單的話模型就會自動開始思考了呢他當時就在谷歌內部的PALM模型上做了測試他非常清楚PALM的訓練數據裏絕對沒有針對這個咒語做過任何的優化結果他震驚地發現它真的有效模型真的開始輸出一步一步的解題過程了這個發現極大的啟發了他儘管Let's think step by step這種零樣本提示效果通常比不過提供具體事例的少樣本思維鏈提示但是他證明了我們可以用非常通用的方式來激發模型的推理潛能然而無論是哪種提示方法都感覺有點奇怪想象一下你問一個聰明人問題還必須得在後面加上一句請一步一步思考否則他就不會思考了這顯然不符合我們對於一個真正智能體的期望所以我們需要一種更穩定更內化的方式讓推理能力成為模型固有的一部分而不是需要外部咒語來觸發這就把我們帶到了下一個階段微調我們先說監督微調SFT它的思路非常的直接我們不就是希望模型能夠生成從問題到思考過程再到答案這樣的數據嗎那我們就僱用一批人針對大量的問題寫出高質量的一步一步的解題方案然後我們再把這些標準答案餵給模型讓模型去學習這個方法在機器學習裏面叫做最大自然估計簡單來說就是讓模型生成的序列跟人類專家寫的序列盡可能的一模一樣這個想法其實很早就有了賴清周提到早在2017年DMI的一篇論文就在做類似的事情他們收集了一批數學硬體和人類手寫的解題步驟來訓練一個人類的智能體他們開創性地提出了像思維鏈提示和自洽性這類的關鍵技術並且深度參與了谷歌靜態模型推理能力的構建最近丹尼州在斯坦福大學做了一場演講系統性地梳理了從它創立谷歌大腦的推理團隊開始到今天我們所看到的強大的AI這條技術路線是如何一步一步演進的這場講座的信息量巨大它不僅揭示了AI推理能力的本質更是對過去幾年中所有相關技術的一次趣味所以今天這期視頻我們就將以丹尼州的這場講座為藍本帶著大家從最基礎的概念出發層層遞進徹底搞懂大約模型思考秘密我們會看到那些看似神奇的技術背後往往遵循著一些極其簡單而深刻的原理相信看完這期視頻你再看待大約模型的時候會有一個全新的更加清晰的視角在深入探討之前我們必須先來個探討當我們討論大圓模型的推理時我們到底在談論什麼丹妮周一上來就拋出了一個非常清晰而且可操作的定義這個定義也成為了整個領域的共識她說關於模型到底會不會推理的哲學辯論她從不參加因為沒有一個明確的定義大家都是在自說自話而在她的團隊裡推理是有一個非常具體的含義的那就是在模型的輸入也就是你的問題和最終輸出也就是答案之間生成的所有中間步驟這個定義非常關鍵她把一個模糊的哲學層面的思考概念轉化成了一個具體的工程上可以實現和優化的目標為了讓我們更好地理解這個目標丹妮周設計了一個非常巧妙的任務叫做末尾字母拼接這個任務聽起來很簡單比如說我問模型請拼接artificial intelligence這兩個單詞的末尾字母請拼接如果我们直接让模型输出答案它可能会凭借着语言的惯性直接猜一个答案比如说LE这时候它只是在预测下一个最可能的字符而不是在执行一个多步骤的逻辑操作但是如果我们引导模型先生成中间步骤它的输出就会变成这样artificial的最后一个字母是Lintelligence的最后一个字母是E将L和E拼接起来得到LE这就是丹妮周所定义的推理她把一个复杂的任务分解成了一系列简单的可执行的子任务最终导出了正确的答案你可能会觉得这不就是我们人类解决问题的方式吗先思考再作答但是丹妮周提醒我们作为研究者必须时刻记住大猿模型不是人类它们只是概率模型把它们擬人化虽然有助于我们理解但是也很容易让我们走入歧途一个有趣的故事是丹妮周最初尝试的是手动模型但是他发现当时所有的模型都能够做得很好为什么呢因为互联网上有大量的缩写词模型在预训练阶段已经背会了如何拼接首字母于是他换成了末尾字母拼接结果当时所有的模型都失败了这恰恰说明模型并没有真正理解拼接这个动作而只是记住了某种常见的模式那么为什么要如此执着于生成这些中间步骤呢仅仅是为了模仿人类吗当然不是这背后有着非常坚实的理论依据单极州提到了他们和斯坦福大学教授腾上华团队合作的一项理论研究这个研究得出了一个非常强大的结论对于任何一个可以被大小为T的布尔电路解决的问题一个常数大小的Transformer模型可以通过生成OT长度的中间步骤来解决它这句话听起来有点过于技术我们来把它翻译一下布尔电路可以被固定为T看作是执行逻辑运算的基本单元任何复杂的计算任务比如说运行一个大型的软件本质上都可以被分解成一个巨大规模的布尔电路这里的大小t就代表了问题的计算复杂度这个理论告诉我们哪怕是一个相对简单的transformer模型只要你允许它生成足够长的思考过程也就是中间步骤它就有潜力解决几乎任何可计算的问题反过来说如果我们强迫模型直接蹦出最终答案就相当于要求这个模型的网络深度本身要能够模拟整个复杂的计算过程这要么需要一个巨大到不切实际的深度要么就根本无法解决问题所以让模型思考生成中间步骤不是一个可有可无的选项而是在计算原理上解锁模型解决复杂问题能力的一把金钥匙这彻底改变了我们训练和使用大约模型的范式單純的追求答案轉向追求過程好 既然我們知道了推理過程如此的重要那麼下一個問題自然就是如何讓模型來生成這個過程呢這裡丹妮周提出了一個顛覆了當時很多人認知的觀點當時普遍認為一個普通的只經過預訓練的大圓模型是不會推理的你必須通過像思維鏈提示這樣的高級技巧或者進行專門的微調才能夠教會他們推理但是丹妮周說這個觀點是錯的而且大錯特錯他認為預訓練模型早就已經準備好進行推理了我們所需要做的僅僅是改變解碼的過程這又是一個非常深刻的洞察為了證明這一點他舉了一個經典的數學硬題我有三個蘋果我的爸爸比我多兩個蘋果我們總共有多少個蘋果如果你把這個問題直接輸入給一個原始的預訓練模型比如說早期的GDP-3或者是拉馬然後使用默認的推理模式貪婪解碼方式會發生什麼呢貪婪解碼的意思是模型在生成每一個詞的時候總是會選擇當前概率最高的那一個在這種模式下模型很可能會直接輸出一個看似合理但卻是錯誤的答案比如說五個蘋果因為它看到了三個和多兩個就直接聯想到了五這是模型的一種直覺反應或者說是一種系統思維但是模型的強大之處在於它的輸出概率分布中並不僅僅只有這一個選項在生成第一個詞的時候五個可能是概率最高的但是還有第二第三第四高的選項如果我們不那麼貪婪而是去探索一下那些概率稍低一些的岔路奇蹟就會發生了接著丹尼周向我們展示了這些隱藏的候選答案比如說候選二可能以我字開頭模型會生成我有三個蘋果我爸爸比我多兩個所以他有五個蘋果三加五等於八所以我們總共有八個蘋果這是一個完美的推理鏈答案也正確候選三可能以我們開頭模型會聲稱我們總共有八個蘋果雖然沒有過程但是答案也對了候選四可能以你自開頭模型會聲稱你有三個蘋果你爸爸有3加2等於五個蘋果你們總共有3加5等於八個蘋果這同樣是一個清晰的推理鏈看到了嗎正確的推理路徑其實一直都存在於模型的輸出空間裏它們就像是隱藏在主幹道旁邊的小路默認的貪婪解碼因為只看到了眼前最寬的路所以錯過了它們這個發現被稱為思維鏈解碼它告訴我們推理能力不是被注入到模型裏面的而是模型在學習海量文本中蘊含的邏輯關系之後自然湧現出來的於是我們的任務從教會模型推理變成了如何引導模型把它已經知道的東西以正確的形式表達出來那麽問題就變成了在正確的推理模型裏面模型裏面的東西是否存在著一個正確的推理模型的存在這就是我們要學習的模型的正確性模型的正確性模型的正確性模型的正確性模型的正確性后选的输出里我们怎么知道哪一个是最好的呢一个简单的想法是看长度带有思考过程的回答通常会更长但是丹妮周的团队发现一个更可靠的指标那就是答案自信度他们观察到一个惊人的现象对于那些包含了正确思维链的回答模型在生成最终答案的那个词比如说数字8的时候其内部的自信度也就是概率会异常的高在这个苹果的例子里模型预测8这个词的概率可能会高达98%这是一个非常强的信号因为对于一个拥有巨大词汇表的模型来说通常每个词的概率都接近于0这就像是一个人在经过深思熟虑之后对自己得出的结论会非常笃定一样所以说思维链解码的核心就两路一 超越贪婪解码生成并且检查更多的候选输出二 选择那个对最终答案自信度最高的候选这个方法呢是可以用来解决很多问题的雖然簡單有效踏入 Wix Studio 公司和企業的平台在你設計的工作環境中 踏出新項目穩定地工作 自由地實驗以精準和精準的意識 去設計複雜的設計到那最後的一張畫面穩定地結構和集中 動態內容以優雅和有效率的 管理豐富內容以 AI 為主體,以每個螢幕為主體,以 AI 為主體,以每個螢幕為主體,創造和自訂互動,從細節動作到進步行為,並通過視覺,從策劃到客戶手機。這是網路創作的規模,複雜,快速,無限,這是你的工作室。邏輯推導還是在模仿他在網上看過的無數解題步驟呢關於這個問題學術界和工業界爭論不休但是爭論的意義遠不如我們去搞清楚這種所謂的推理能力到底是怎麼來的以及我們如何才能穩定的可靠的駕馭它而要解答這個問題有一個人的名字我們就繞不開他就是來自於谷歌DeepMind的丹尼周他和他的團隊可以說是奠定了我們今天理解和使用大圓模型推理能力的基石他們開創性的提出了像思維鏈提示和自下行這類的關鍵技術並且深度參與了谷歌GNI模型推理能力的構建最近丹尼周在斯坦福大學做了一場演講系統性的梳理了從他創立谷歌大腦的推理團隊開始到今天我們所看到的強大的AI這條技術路線是如何一步一步演進的這場講座的信息量巨大它不僅揭示了AI推理能力的真實性更是对过去几年中所有相关技术的一次去媚所以今天这期视频我们就将以丹尼周的这场讲座为蓝本带着大家从最基础的概念出发层层递进彻底搞懂大约模型思考秘密我们会看到那些看似神奇的技术背后往往遵循着一些极其简单而深刻的原理相信看完这期视频你再看待大约模型的时候会有一个全新的更加清晰的视角在深入探讨之前我们必须先明确一件事情当我们讨论大约模型的推理时我们到底在谈论什么丹尼周一上来就抛出了一个非常清晰而且可操作的定义这个定义也成为了整个领域的共识他说关于模型到底会不会推理的哲学辩论他从不参加因为没有一个明确的定义大家都是在自说自话而在他的团队里推理是有一个非常具体的含义的那就是在模型的推理中我们可以看到模型的推理是一个非常具体的语言它是一个非常具体的语言它是一个非常具体的语言它是一个非常具体的语言它是一个非常具体的语言它是一个非常具体的语言也就是你的問題和最終輸出也就是答案之間生成的所有中間步驟這個定義非常關鍵它把一個模糊的哲學層面的思考概念轉化成了一個具體的工程上可以實現和優化的目標為了讓我們更好地理解這個目標丹尼周設計了一個非常巧妙的任務叫做末尾字母拼接這個任務聽起來很簡單比如說我問模型請拼接artificial intelligence這兩個單詞的末尾字母如果我們直接讓模型輸出答案它可能會憑藉著語言的慣性直接猜一個答案比如說LE這時候它只是在預測下一個最可能的字符而不是在執行一個多步驟的邏輯操作但是如果我們引導模型先生成中間步驟它的輸出就會變成這樣artificial的最後一個字母是Lintelligence的最後一個字母是E將L和E拼接起來得到這個結果这就是丹妮周所定义的推理她把一个复杂的任务分解成了一系列简单的可执行的子任务最终导出了正确的答案你可能会觉得这不就是我们人类解决问题的方式吗先思考再作答但是丹妮周提醒我们作为研究者必须时刻记住大猿模型不是人类它们只是概率模型把它们拟人化虽然有助于我们理解但是也很容易让我们走入歧途一个有趣的故事是丹妮周最初尝试的是首字母拼接但是她发现当时所有的模型都能够做得很好遠不如我們去搞清楚這種所謂的推理能力到底是怎麼來的以及我們如何才能穩定的可靠的駕馭它而要解答這個問題有一個人的名字我們就繞不開他就是來自於谷歌DeepMind的丹尼周他和他的團隊可以說是奠定了我們今天理解和使用大圓模型推理能力的基石他們開創性的提出了像思維鏈提示和自洽性這類的關鍵技術並且深度參與了谷歌近代模型推理能力的構建最近丹尼周在斯坦福大學做了一場演講系統性的梳理了從他創立谷歌大腦的推理團隊開始到今天我們所看到的強大的AI這條技術路線是如何一步一步演進的這場講座的信息量巨大它不僅揭示了AI推理能力的本質更是對過去幾年中所有相關技術的一次趣味所以今天這期視頻我們就將以丹尼周的這場講座為藍本帶著大家從谷歌大腦開始最基础的概念出发层层递进彻底搞懂大学模型思考秘密我们会看到那些看似神奇的技术背后往往遵循着一些极其简单而深刻的原理相信看完这期视频你再看待大学模型的时候会有一个全新的更加清晰的视角在深入探讨之前我们必须先明确一件事情当我们讨论大学模型的推理时我们到底在谈论什么单机中一上来就抛出了一个非常清晰而且可操作的定义这个定义也成为了整个领域的共识他说关于模型到底会不会推理的他从不参加因为没有一个明确的定义大家都是在自说自话而在他的团队里推理是有一个非常具体的含义的那就是在模型的输入也就是你的问题和最终输出也就是答案之间生成的所有中间步骤这个定义非常关键他把一个模糊的推理当成了一个中间步骤哲学层面的思考概念转化成了一个具体的工程上可以实现和优化的目标为了让我们更好的理解这个目标丹妮周设计了一个非常巧妙的任务叫做末尾字母拼接这个任务听起来很简单比如说我问模型请拼接artificial intelligence这两个单词的末尾字母如果我们直接让模型输出答案它可能会凭借着语言的惯性直接猜一个答案比如说LE这时候它只是在预测下一个最可能的字符而不是在执行一个多步骤的逻辑操作但是如果我们引导模型先生成中间步骤它的输出就会变成这样artificial的最后一个字母是Lintelligence的最后一个字母是E将L和E拼接起来得到LE这就是丹妮周所定义的推理她把一个复杂的任务分解成了一系列简单的可执行的子任务最终导出了正确的答案那我们就来看看你可能会觉得这不就是我们人类解决问题的方式吗先思考再作答但是丹妮周提醒我们作为研究者必须时刻记住大猿模型不是人类它们只是概率模型把它们拟人化虽然有助于我们理解但是也很容易让我们走入歧途一个有趣的故事是丹妮周最初尝试的是手字母拼接但是她发现当时所有的模型都能够做得很好为什么呢因为互联网上有大量的缩写词模型在预训练阶段已经背会了如何拼接手字母于是她换成了末尾字母拼接结果当时所有的模型都失败了这恰恰说明模型并没有真正理解拼接这个动作而只是记住了某种常见的模式那么为什么要如此执着于生成这些中间步骤呢仅仅是为了模仿人类吗当然不是这背后有着非常坚实的理论依据丹妮周提到了她们和斯坦福大学教授腾尚合作的一項理論研究這個研究得出了一個非常強大的結論對於任何一個可以被大小為T的布爾電路解決的問題一個常數大小的Transformer模型可以通過生成OT長度的中間步驟來解決它這句話聽起來有點過於技術我們來把它翻譯一下布爾電路可以被看作是執行邏輯運算的基本單元任何復雜的計算任務比如說運行一個大型的軟件本質上都可以被分解成一個巨大規模的布爾電路這裡的大小T就代表了問題的計算復雜度這個理論告訴我們哪怕是一個相對簡單的Transformer模型只要你允許它生成足夠長的思考過程也就是中間步驟它就有潛力解決幾乎任何可計算的問題反過來說如果我們強迫模型直接蹦出最終答案就相當於要求這個模型的網絡深度本身要能夠模擬整個復雜度这要么需要一个巨大到不切实际的深度要么就根本无法解决问题所以让模型思考生成中间步骤不是一个可有可无的选项而是在计算原理上解锁模型解决复杂问题能力的一把金钥匙这彻底改变了我们训练和使用大圆模型的范式从单纯的追求答案转向追求过程好 既然我们知道了推理过程如此的重要那么下一个问题自然就是如何让模型来生成这个过程呢这里丹基周提出了一个颠覆了当时很多人认知的观点当时普遍认为一个普通的只经过预训练的大圆模型是不会推理的你必须通过像思维链提示这样的高级技巧或者进行专门的微调才能够教会他们推理但是丹基周说这个观点是错的而且大错特错他认为预训模型早就已经准备好进行推理了我们所需要做的就是僅僅是改變解碼的過程這又是一個非常深刻的洞察為了證明這一點他舉了一個經典的數學硬題我有三個蘋果我的爸爸比我多兩個蘋果我們總共有多少個蘋果如果你把這個問題直接輸入給一個原始的預訓練模型比如說早期的GDP3或者是拉馬然後使用默認的貪婪解碼方式會發生什麼呢貪婪解碼的意思是模型在生成每一個詞的時候總是會選擇當前概率最高的那一個在這種模式下模型很可能會直接輸出一個看似合理但卻是錯誤的答案比如說五個蘋果因為它看到了三個和多兩個就直接聯想到了五這是模型的一種直覺反應或者說是一種系統思維但是模型的強大之處在於它的輸出概率分佈中並不僅僅只有這一個選項在生成第一個詞的時候五個可能是概率最高的但是還有第二第三第四高的選項如果我們把這個模型直接輸入給一個原始的預訓練模型我們總共會有多少個蘋果我們總共會有多少個蘋果我們總共會有多少個蘋果我們總共會有多少個蘋果我們總共會有多少個蘋果我們總共會有多少個蘋果我們不那麼貪婪而是去探索一下那些概率稍低一些的岔路奇蹟就會發生了接著單機周向我們展示了這些隱藏的候選答案比如說候選二可能以我字開頭模型會生成我有三個蘋果我爸爸比我多兩個所以他有五個蘋果3加5等於8所以我們總共有八個蘋果這是一個完美的推理鏈答案也正確候選三可能以我們開頭模型會生成我們總共有八個蘋果雖然沒有過程但是答案也對了候選四可能以你字開頭模型會生成你有三個蘋果你爸爸有3加2等於五個蘋果你們總共有3加5等於八個蘋果這同樣是一個清晰的推理鏈看到了嗎正確的推理路徑其實一直都存在於模型的輸出空間裏它們就像是隱藏在主幹道旁邊的小路默認的貪婪解碼因為只看到了眼前最寬的路所以錯過了它們這個發現被探索了他告诉我们推理能力不是被注入到模型里面的而是模型在学习海量稳稳中蕴含的逻辑关系之后自然涌现出来的于是我们的任务从交汇模型推理变成了如何引导模型把它已经知道的东西以正确的形式表达出来那么问题就变成了在这么多候选的输出力我们怎么知道哪一个是最好的呢一个简单的想法是看长度带有思考过程的回答通常会更长但是丹妮周的团队发现一个更可靠的指标那就是答案自信度他们观察到一个惊人的现象对于那些包含了正确思维链的回答模型在生成最终答案的那个词比如说数字8的时候其内部的自信度也就是概率会异常的高在这个苹果的例子里模型预测8这个词的概率可能会高达98%这是一个非常强的信号因为对于一个拥有巨大词汇表的模型来说它是一个非常强的信号所以我们就要更加的注意模型的自信度它是一个非常强的信号因为对于一个拥有巨大词汇表的模型来说它是一个非常强的信号所以我们就要更加的注意模型的自信度通常每個詞的概率都接近於零這就像是一個人在經過深思熟慮之後對自己得出的結論會非常篤定一樣所以說思維鏈解碼的核心就兩步一超越貪婪解碼生成並且檢查更多的候選輸出二選擇那個對最終答案自信度最高的候選這個方法雖然簡單有效但還是需要寫一些代碼對於普通用戶來說不夠友好於是研究者們開始思考我們能不能用更自然的方式比如說自然語言來重塑模型的輸出概率分布讓那些帶有思考過程的優秀答案能夠自動排到第一名這樣我們用最簡單的貪婪解碼就能夠直接得到它這就引出了我們後來耳熟能詳的一系列提示工程技術首先最著名的就是思維鏈提示它的做法非常直觀在你提出你的問題之前先給模型看一兩個類似的從問題到思考過程再到答案经常接触大模型的朋友一定会有过这样的经历当你向AI提出一个复杂的问题时它不仅给出了答案还给出了详细的一步一步的看起来逻辑严密的解题过程那一瞬间你是不是感觉屏幕对面的不再是一堆冰冷的代码而是一个真正能够推理的智能体但是有的时候你换一个同样复杂但是略有不同的问题时它又会给出一个错的离谱的答案让你觉得它根本什么都不懂只是一个更高级的复读机这种体验上的巨大反差正是当前AI领域最核心的谜题之一大圆模型展现出的推理能力究竟是一种真正的智能涌现还是一种基于海量数据训练出来的更高级的模式匹配它是在进行逻辑推导还是在模仿它在网上看过的无数解题步骤关于这个问题学术界和工业界争论不休但是争论的意义远不如理解不如我们去搞清楚这种所谓的推理能力到底是怎么来的以及我们如何才能稳定的可靠的驾驭它而要解答这个问题有一个人的名字我们就绕不开他就是来自于谷歌丁曼的丹尼周他和他的团队可以说是奠定了我们今天理解和使用大元模型推理能力的基石他们开创性的提出了像思维链提示和自下行这类的关键技术并且深度参与了谷歌静态模型推理能力的构建最近丹尼周在斯坦福大学做了一场演讲系统性的梳理了最基础的概念出发层层递进彻底搞懂大学模型思考秘密我们会看到那些看似神奇的技术背后往往遵循着一些极其简单而深刻的原理相信看完这期视频你再看待大学模型的时候会有一个全新的更加清晰的视角在深入探讨之前我们必须先明确一件事情当我们讨论大学模型的推理时我们到底在谈论什么单机中一上来就抛出了一个非常清晰而且可操作的定义这个定义也成为了整个领域的共识他说关于模型到底会不会推理的哲学辩论他从不参加因为没有一个明确的定义大家都是在自说自话而在他的团队里推理是有一个非常具体的含义的那就是在模型的输入也就是你的问题和最终输出也就是答案之间生成的所有中间步骤这个定义非常关键他把一个模糊的哲学辩论完全放在了模型里面这就是模型的输入也就是你的问题和最终输出也就是答案之间生成的所有中间步骤这个定义非常关键他把一个模糊的哲学辩论从科学层面的思考概念转化成了一个具体的工程上可以实现和优化的目标为了让我们更好地理解这个目标丹尼周设计了一个非常巧妙的任务叫做末尾字母拼接这个任务听起来很简单比如说我问模型请拼接Artificial Intelligence这两个单词的末尾字母如果我们直接让模型输出答案它可能会凭借着语言的惯性直接猜一个答案比如说LE这时候它只是在预测下一个最可能的字符而不是在执行一个多步骤的逻辑操作如果我们引导模型先生成中间步骤它的输出就会变成这样Artificial的最后一个字母是LIntelligence的最后一个字母是E将L和E拼接起来得到LE这就是丹尼周所定义的推理他把一个复杂的任务分解成了一系列简单的可执行的子任务最终导出了正确的答案你可能问這不就是我們人類解決問題的方式嗎先思考再作答但是丹妮周提醒我們作為研究者必須時刻記住大圓模型無視人類它們只是概率模型把它們擬人化雖然有助於我們理解但是也很容易讓我們走入歧途一個有趣的故事是丹妮周最初嘗試的是手字母拼接但是她發現當時所有的模型都能夠做得很好為什麼呢因為互聯網上有大量的縮寫詞模型在預訓練階段已經背會了如何拼接手字母於是她換成了末尾字母拼接結果當時所有的模型都失敗了這恰恰說明模型並沒有真正理解拼接這個動作而只是記住了某種常見的模式那麼為什麼要如此執著於生成這些中間步驟呢僅僅是為了模仿人類嗎當然不是這背後有著非常堅實的理論依據丹妮周提到了她們和斯坦福大學教授藤上華曾經說過合作的一項理論研究這個研究得出了一個非常強大的結論對於任何一個可以被大小為T的布爾電路解決的問題一個常數大小的Transformer模型可以通過生成OT長度的中間步驟來解決它這句話聽起來有點過於技術我們來把它翻譯一下布爾電路可以被看作是執行邏輯運算的基本單元任何復雜的計算任務比如說運行一個大型的軟件本質上都可以被分解成一個巨大規模的布爾電路這裡的大小T就代表了問題的計算復雜度這個理論告訴我們哪怕是一個相對簡單的Transformer模型只要你允許它生成足夠長的思考過程也就是中間步驟它就有潛力解決幾乎任何可計算的問題反過來說如果我們強迫模型直接蹦出最終答案就相當於要求這個模型的網絡深度本身要能夠模擬整個復雜度计算过程这要么需要一个巨大到不切实际的深度要么就根本无法解决问题所以让模型思考生成中间步骤不是一个可有可无的选项而是在计算原理上解锁模型解决复杂问题能力的一把金钥匙这彻底改变了我们训练和使用大学模型的方式从单纯的追求答案转向追求过程好 既然我们知道了推理过程如此的重要那么下一个问题自然就是如何让模型来生成这个过程这里丹妮周提出了一个颠覆了当时很多人认知的观点当时普遍认为一个普通的只经过预训练的大学模型是不会推理的你必须通过像思维链提示这样的高级技巧或者进行专门的微调才能够教会他们推理但是丹妮周说这个观点是错的而且大错特错她认为预训模型早就已经准备好进行推理了我们所需要的就是只要改變解碼的過程這又是一個非常深刻的洞察為了證明這一點他舉了一個經典的數學硬題我有三個蘋果我的爸爸比我多兩個蘋果我們總共有多少個蘋果如果你把這個問題直接輸入給一個原始的預訓練模型比如說早期的GDP-3或者是拉馬然後使用默認的貪婪解碼方式會發生什麼呢貪婪解碼的意思是模型在生成每一個詞的時候總是會選擇當前概率最高的那一個在這種模式下模型很可能會直接輸出一個看似合理但卻是錯誤的答案比如說五個蘋果因為他看到了三個和多兩個就直接聯想到了五這是模型的一種直覺反應或者說是一種系統思維但是模型的強大之處在於它的輸出概率分佈中並不僅僅只有這一個選項在生成第一個詞的時候五個可能是概率最高的但是還有第二第三第四高的選項任何一個都可以后来在2021年OPI更进一步构建了一个更大更著名的数据集也就是GSM给出了详细的一步一步的看起来逻辑严密的解题过程大家好这里是最佳拍档我是大飞经常接触大模型的朋友一定会有过这样的经历当你向AI提出一个复杂的问题时它不仅给出了答案还给出了详细的一步一步的看起来逻辑严密的解题过程那一瞬间你是不是感觉屏幕对面的不再是一堆冰冷的代码而是一个真正能够推理的智能体但是有的时候你换一个同样复杂但是略有不同的问题时它又会给出一个错的离谱的答案让你觉得它根本什么都不懂只是一个更高级的复读机这种体验上的巨大反差正是当前AI领域最核心的谜题之一大圆模型展现出的推理能力究竟是一种真正的智能涌现还是一种基于海量的推理能力训练出来的更高级的模式匹配呢它是在进行逻辑推导还是在模仿它在网上看过的无数解题步骤呢关于这个问题学术界和工业界争论不休但是争论的意义远不如我们去搞清楚这种所谓的推理能力到底是怎么来的以及我们如何才能稳定的可靠的驾驭它而要解答这个问题有一个人的名字我们就绕不开他就是来自于谷歌DeepMind的丹尼周他和他的团队可以说是奠定了我们今天理解和使用大元模型推理能力的基石他们开创性的提出了像思维链提示和自下行这类的关键技术并且深度参与了谷歌Gemini模型推理能力的构建最近丹尼周在斯坦福大学做了一场演讲系统性的梳理了从他创立谷歌大脑的推理团队开始到今天我们所看到的强大的AI这条技术路线是如何一步一步演进的這場講座的信息量巨大它不僅揭示了AI推理能力的本質更是對過去幾年中所有相關技術的一次去媚所以今天這期視頻我們就將以丹妮周的這場講座為藍本帶著大家從最基礎的概念出發層層遞進徹底搞懂大約模型思考秘密我們會看到那些看似神奇的技術背後往往遵循著一些極其簡單而深刻的原理相信看完這期視頻你再看待大約模型的時候會有一個全新的更加清晰的視角在深入探討之前我們必須先明確一件事情當我們討論大約模型的推理時我們到底在談論什麼丹妮周一上來就拋出了一個非常清晰而且可操作的定義這個定義也成為了整個領域的共識她說關於模型到底會不會推理的哲學辯論她從不參加因為沒有一個明確的定義大家都是在自說自話而在她的團體中推理是有一個非常具體的含義的那就是在模型的輸入也就是你的問題和最終輸出也就是答案之間生成的所有中間步驟這個定義非常關鍵它把一個模糊的哲學層面的思考概念轉化成了一個具體的工程上可以實現和優化的目標為了讓我們更好地理解這個目標單機周設計了一個非常巧妙的任務叫做末尾字母拼接這個任務聽起來很簡單比如說我問模型請拼接artificial intelligence這兩個單詞的末尾字母如果我們直接讓模型輸出答案它可能會憑藉著語言的慣性直接猜一個答案比如說LE這時候它只是在預測下一個最可能的字符而不是在執行一個多步驟的邏輯操作但是如果我們引導模型先生成中間步驟它的輸出就會變成這樣artificial的最後一個字母是Lintelligence的中間步驟最後一個字母是E將L和E拼接起來得到LE這就是丹妮周所定義的推理他把一個復雜的任務分解成了一系列簡單的可執行的子任務最終導出了正確的答案你可能會覺得這不就是我們人類解決問題的方式嗎先思考再作答但是丹妮周提醒我們作為研究者必須時刻記住大猿模型不是人類它們只是概率模型把它們擬人化雖然有助於我們理解但是也很容易讓我們走入歧途一個有趣的故事是丹妮周最初嘗試的是手字母拼接但是他發現當時所有的模型為什麼呢因為互聯網上有大量的縮寫詞模型在預訓練階段已經背會了如何拼接手字母於是它換成了落尾字母拼接結果當時所有的模型都失敗了這恰恰說明模型並沒有真正理解拼接這個動作而只是記住了某種動作那么为什么要如此执着于生成这些中间步骤呢仅仅是为了模仿人类吗当然不是这背后有着非常坚实的理论依据丹妮周提到了他们和斯坦福大学教授腾尚华团队合作的一项理论研究这个研究得出了一个非常强大的结论对于任何一个可以被大小为T的布尔电路解决的问题一个常数大小的Transformer模型可以通过生成OT长度的中间步骤来解决它这句话听起来有点过于技术我们来把它翻译一下布尔电路可以被看作是执行逻辑运算的基本单元任何复杂的计算任务比如说运行一个大型的软件本质上都可以被分解成一个巨大规模的布尔电路这里的大小T就代表了问题的计算复杂度这个理论告诉我们哪怕是一个相对简单的Transformer模型只要你允许它生成足够长的思考过程也就是一个中間步驟它就有潛力解決幾乎任何可計算的問題反過來說如果我們強迫模型直接蹦出最終答案就相當於要求這個模型的網絡深度本身要能夠模擬整個復雜的計算過程這要麼需要一個巨大到不切實際的深度要麼就根本無法解決問題所以讓模型思考生成中間步驟不是一個可有可無的選項而是在計算原理上解鎖模型解決復雜問題能力的一把金鑰匙這徹底改變了我們訓練和使用大圓模型的範式從單純的追求答案轉向追求過程好 既然我們知道了推理過程如此的重要那麼下一個問題自然就是如何讓模型來生成這個過程呢這裡丹妮周提出了一個顛覆了當時很多人認知的觀點當時普遍認為一個普通的只經過預訓練的大圓模型是不會推理的你必須通過向模型進行推理思维链提示这样的高级技巧或者进行专门的微调才能够交费他们推理但是丹基周说这个观点是错的而且大错特错他认为预训类模型早就已经准备好进行推理了我们所需要做的仅仅是改变解码的过程这又是一个非常深刻的洞察为了证明这一点他举了一个经典的数学硬题我有三个苹果我的爸爸比我多两个苹果我们总共有多少个苹果如果你把这个问题直接输入给一个原始的预训类模型比如说早期的GPT-3或者是拉玛然后使用默认的贪婪解码方式会发生什么呢贪婪解码的意思是模型在生成每一个词的时候总是会选择当前概率最高的那一个在这种模式下模型很可能会直接输出一个看似合理但却是错误的答案比如说五个苹果因为他看到了三个和多两个就直接联想到了五这是模型的一种直觉反应或者说是真正能夠推理的智能體但是有的時候你換一個同樣複雜但是略有不同的問題時它又會給出一個錯的離譜的答案讓你覺得它根本什麼都不懂只是一個更高級的復讀機這種體驗上的巨大反差正是當前AI領域最核心的謎題之一大圓模型展現出的推理能力究竟是一種真正的智能湧現還是一種基於海量數據訓練出來的更高級的模式匹配呢它是在進行邏輯推導還是在模仿它在網上看過的無數解題步驟呢關於這個問題學術界和工業界爭論不休但是爭論的意義遠不如我們去搞清楚這種所謂的推理能力到底是怎麼來的以及我們如何才能穩定的可靠的駕馭它而要解答這個問題有一個人的名字我們就繞不開他就是來自於谷歌DeepMind的丹尼周他和他的團隊可以說是奠定了我們今天理解的科學思想和使用大圆模型推理能力的基石他们开创性的提出了像思维链提示和自下性这类的关键技术并且深度参与了谷歌金奶模型推理能力的构建最近丹尼州在斯坦福大学做了一场演讲系统性的梳理了从它创立谷歌大脑的推理团队开始到今天我们所看到的强大的AI这条技术路线是如何一步一步演进的这场讲座的信息量巨大它不仅揭示了AI推理能力的本质更是对过去几年中所有相关技术的一次去媚所以今天这期视频我们就将以丹尼州的这场讲座为蓝本带着大家从最基础的概念出发层层递进彻底搞懂大圆模型思考秘密我们会看到那些看似神奇的技术背后往往遵循着一些极其简单而深刻的原理相信看完这期视频你再看待大圆模型的时候会有一个全新的更加清晰的理解在深入探讨之前我们必须先明确一件事情当我们讨论大约模型的推理时我们到底在谈论什么丹尼周一上来就抛出了一个非常清晰而且可操作的定义这个定义也成为了整个领域的共识他说关于模型到底会不会推理的哲学辩论他从不探加因为没有一个明确的定义大家都是在自说自话而在他的团队里推理是有一个非常具体的含义的那就是在模型的输入也就是你的问题和最终输出也就是答案之间生成的所有中间步骤这个定义非常关键他把一个模糊的哲学层面的思考概念转化成了一个具体的工程上可以实现和优化的目标为了让我们更好地理解这个目标丹尼周设计了一个非常巧妙的任务叫做末尾字母拼接这个任务听起来很简单比如说我问模型请拼接Artificial IntelligenceIntelligence這兩個單詞的末尾字母如果我們直接讓模型輸出答案它可能會憑藉著語言的慣性直接猜一個答案比如說LE這時候它只是在預測下一個最可能的字符而不是在執行一個多步驟的邏輯操作但是如果我們引導模型先生成中間步驟它的輸出就會變成這樣artificial的最後一個字母是Lintelligence的最後一個字母是E將L和E拼接起來得到LE這就是丹妮周所定義的推理她把一個復雜的任務分解成了一系列簡單的可執行的子任務最終導出了正確的答案你可能會覺得這不就是我們人類解決問題的方式嗎先思考再作答但是丹妮周提醒我們作為研究者必須時刻記住大約模型不是人類它們只是概率模型把它們擬人化雖然有助於我們理解但是也很容易讓我們走入歧途一個有趣的故事是丹妮周最初尝试的是手字母拼接但是她发现当时所有的模型都能够做得很好为什么呢因为互联网上有大量的缩写词模型在预训练阶段已经背会了如何拼接手字母于是她换成了末尾字母拼接结果当时所有的模型都失败了这恰恰说明模型并没有真正理解拼接这个动作而只是记住了某种常见的模式那么为什么要如此执着于生成这些中间步骤呢仅仅是为了模仿人类吗当然不是这背后有着非常坚实的理论依据丹妮周提到了他们和斯坦福大学教授腾尚华团队合作的一项理论研究这个研究得出了一个非常强大的结论对于任何一个可以被大小为T的布尔电路解决的问题一个常数大小的Transformer模型可以通过生成OT长度的中间步骤来解决它这句话听起来有点过于技术化但是实际上是一个很好的解决方案所以这就是为什么我们要学习的这种模型是要学习的我们要学习的我们要学习的我们来把它翻译一下布尔电路可以被看作是执行逻辑运算的基本单元任何复杂的计算任务比如说运行一个大型的软件本质上都可以被分解成一个巨大规模的布尔电路这里的大小T就代表了问题的计算复杂度这个理论告诉我们哪怕是一个相对简单的Transformer模型只要你允许它生成足够长的思考过程也就是中间步骤它就有潜力解决几乎任何可计算的问题反过来说如果我们强迫模型直接蹦出最终答案就相当于要求这个模型的网络深度本身要能够模拟整个复杂的计算过程这要么需要一个巨大到不切实际的深度要么就根本无法解决问题所以让模型思考生成中间步骤不是一个可有可无的选项而是在计算原理上解锁模型解决复杂问题能力的一把金钥匙这彻底改变了我们学习的方式和使用大猿猿模型的方式從單純的追求答案轉向追求過程好 既然我們知道了推理過程如此的重要那麼下一個問題自然就是如何讓模型來生成這個過程呢這裡丹妮周提出了一個顛覆了當時很多人認知的觀點當時普遍認為一個普通的只經過預訓練的大猿猿模型是不會推理的你必須通過向思維鏈提示這樣的高級技巧或者進行專門的微調才能夠教會他們推理但是丹妮周說這個觀點是錯的而且大錯特錯他認為預訓練模型早就已經準備好進行推理了GDP3或者是拉玛然后使用默认的贪婪解码方式会发生什么呢贪婪解码的意思是模型在生成每一个词的时候总是会选择当前概率最高的那一个在这种模式下模型很可能会直接输出一个看似合理但却是错误的答案比如说5个苹果因为它看到了3个和多两个就直接联想到了5这是模型的一种直觉反应或者说是一种系统思维但是模型的强大之处在于它的输出概率分布中并不仅仅只有这一个选项在生成第一个词的时候5个可能是概率最高的但是还有第二第三第四高的选项如果我们不那么贪婪而是去探索一下那些概率稍低一些的岔路奇迹就会发生了接着单机周向我们展示了这些隐藏的候选答案比如说候选二可能以我字开头模型会生成我有3个苹果我爸爸比我多两个所以还有5个苹果3加5等于8所以我们总共有8个苹果这是一个完美的推理链答案也正确候选三可能以我们开头模型会生成我们总共有8个苹果虽然没有过程但是答案也对了候选四可能以你自开头模型会生成你有3个苹果你爸爸有3加2等于5个苹果你们总共有3加5等于8个苹果这同样是一个清晰的推理链看到了吗正确的推理路径其实一直都存在于模型的输出空间里它们就像是隐藏在主干道旁边的小路默认的贪婪解码因为只看到了眼前最宽的路所以错过了他们这个发现被称为思维链解码它告诉我们推理能力不是被注入到模型里面的而是模型在学习海量文本中蕴含的逻辑关系之后自然涌现出来的于是我们的任务从交汇模型推理变成了如何引导模型把它已经知道的东西以正常的方式推理正確的形式表達出來那麼問題就變成了在這麼多候選的輸出裏我們怎麽知道哪一個是最好的呢一個簡單的想法是看長度帶有思考過程的回答通常會更長但是丹妮周的團隊發現一個更可靠的指標那就是答案自信度他們觀察到一個驚人的現象對於那些包含了正確思維鏈的回答模型在生成最終答案的那個詞比如說數字8的時候其內部的自信度也就是概率會異常的高在這個蘋果的例子裏模型預測8這個詞的概率可能會高達98%這是一個非常強的型號因為對於一個擁有巨大詞彙表的模型來說通常每個詞的概率都接近於0這就像是一個人在經過深思熟慮之後對自己得出的結論會非常篤定一樣所以說思維鏈解碼的核心就兩步一超越貪婪解碼生成並且檢查更多的候選輸出二選擇那個最合理的答案對最終答案信證度最高的候選這個方法雖然簡單有效但還是需要寫一些代碼對於普通用戶來說不夠友好於是研究者們開始思考我們能不能用更自然的方式比如說自然語言來重塑模型的輸出概率分布讓那些帶有思考過程的優秀答案能夠自動排到第一名這樣我們用最簡單的貪婪解碼就能夠直接得到它這就引出了我們後來耳熟能詳的一系列提示工程技術首先最著名的就是思維鏈提示它的做法非常直觀在你提出你的問題之前先給模型看一兩個類似的從問題到思考過程再到答案的例子比如說你想讓模型解決前面蘋果的問題你可以先給它一個例子問題是一個農民有5個香蕉他又買了6個後來吃了2個還剩幾個答案是農民開始有5個香蕉買了6個之後他有5加6等於11個然後他吃了2個所以他剩下了5個11-2等于9个 答案是9然后你再提出你的问题我有三个苹果 我爸爸比我多两个我们总共有多少个苹果呢神奇的事情发生了模型会模仿你给出的例子的风格自动的开始一步一步分析声称详细的解析步骤最后给出正确答案从概率分布的角度来看你给出的例子极大的提升了模型声称类似思考过程的巨式的概率从而把原本隐藏在后面的正确推理路径推到了最前面但是这种方法有一个问题你需要为不同类型的任务手动编写高质量的实例 这很麻烦而且如果你自己都知道怎么解决一个类似的问题那你为什么还要问AI呢于是一个更加神奇的提示就出现了它就是让我们一步一步思考Let's think step by step但泥洲坦言这篇论文刚出来的时候他以为这是个玩笑怎么可能在问题后面加上这么一句简单的话模型就会自动模仿出来他当时就在谷歌内部的PALM模型上做了测试他非常清楚PALM的训练数据里绝对没有针对这个咒语做过任何的优化结果他震惊地发现它真的有效模型真的开始输出一步一步的解题过程了这个发现极大的启发了他尽管Let's think step by step这种零样本提示效果通常比不过提供具体事例的少样本思维链提示但是它证明了我们可以用非常通用的方式来激发模型的推理潜能然而无论是哪种提示方法都感觉有点奇怪想象一下你问一个聪明人问题还必须得在后面加上一句请一步一步思考否则他就不会思考了这显然不符合我们对于一个真正智能体的期望所以我们需要一种更稳定更内化的方式让推理能力成为模型固有的一部分而不是需要外部咒语来触发这就把我们带到了下一个阶段如果你想建立一个应用软件,但不知道该怎么制作或开始,请去看看Base44我建立了一个日常的习惯追踪器,帮助使用者计算小赢利,追踪自己的心情,反省过去的进度,并看他们走到哪里它是完全功能的,我没有写一条线你开始是要描述你的想法,你想做什么,功能,看法和感觉现在看Base做它的魔术设计互联网,建立数据库,连接逻辑工程师 设计师和产品经纪人来创造你的视野立刻当你第一版本完成了你就继续进行我加入黑色模式处理过渡确保它能顺利运行最好的部分我可以保持流畅没有阻挡没有切换工具只是建造所以开始你的想法BASE44做其他微调 我们先说监督微调SFT它的思路非常的直接我们不就是希望模型能够生成从问题到思考过程再到谈论这样的数据吗那我们就雇佣一批人针对大量的问题写出高质量的一步一步的解题方案然后我们再把这些标准答案备给模型让模型去学习这个方法在机器学习里面叫做最大三人估计简单来说就是让模型生成的序列跟人类专家写的序列尽可能的一模一样这个想法其实很早就有了单一周提到早在2017年DMI的一篇论文就在做类似的事情他们收集了一批数学硬体和人类手写的解题步骤来训练一个序列模型后来在2021年OPI更进一步构建了一个更大更著名的数据集也就是GSM8K包含了8000多个小学水平的数学题和详细解法用来微调GPT-3模型这种方法训练出来的模型在你给它一个新问题的时候确实能够生存无错的解题步骤看起来问题似乎解决了一旦模型训练好就可以随时部署不再需要分析然而在2021年夏天丹妮周的团队发现了一个严重的问题那就是SFT训练出来的模型算话能力很差它在那些和训练数据很相似的问题上表现很好但是一旦遇到一个新的类型稍微不同的问题就很容易失败他们尝试了大力出奇迹的方法扩大了数据的规模找更多的人标注更多的数据可惜结果却是无论如何扩大规模这个问题始终存在丹妮周在这里给出了一个重要的教训不要盲目地扩大规模当你的犯事本身是错误的时候再多的数据也无济于事那么SFT的犯事错在哪里了呢问题又出在流程里的哪一步呢丹妮周给出的答案可能会让你大吃一惊她说错误出在人身上这个转折点来自于自我提升后来也被称为Self-improve或者是START方法按照第一次听到机器生成的训练数据可能比较容易理解人類專家寫的還好這個想法的時候他自己也感到非常驚訝這個新範式的流程是這樣的首先我們仍然從一批問題開始但是我們不再找人類去寫解題步驟我們讓一個已經比較強大的大約模型自己去針對這些問題生成大量的多樣的解題步驟最關鍵的一步是我們用一個驗證器去檢查模型生成的這些解題步驟看哪個最終得出了正確的答案比如說對於數學題我們知道標準答案就可以直接的判斷於是我們只保留下來那些過程多樣但是結果正確的生成結果把它們當作新的高質量的訓練數據一篇在2024年1月发表的来自字节跳动的论文Rhythm with Reinforced by Tune是他在学术界看到的最早公开阐述类似思想的出版物之一他相信在OpenAI等多个机构内部大家可能都独立地发现了这个简单而又极其有效的思想现在我们必须回答那个核心的问题为什么模型自己生成的数据会比人类专家手写的数据在训练效果上更好呢这背后其实蕴含着既学习的一个第一性原理那就是直接优化你想要的东西在SFT的范式里我们优化的目标是让模型的输出模仿人类的解题步骤我们假设人类的思维过程就是最优的但是实际上人类的思维方式千差万别充满了跳跃和不一致而且人类专家写的标准答案对于模型来说可能并不是最容易学习和范化的路径而在新的范式里我们的目标变了我们不再关心模型的发展解題過程是否和人類一模一樣我們只關心一件事情它最終的答案是否正確我們用最終答案的正確性這個指標相當於強化學習裏的獎勵信號來指導模型的學習這在數學上就等同於我們要求解一個策略梯度問題模型需要調整自己的參數使得生成能夠獲得高獎勵的序列的概率最大化但金周強調我們不需要用激勵模型去思考這種擬人化的神秘的語言來描述這個過程本質上就是三件事情定義你的目標 計算梯度然後反向傳播這就是記憶學習的全部通過這種方式模型會自己去探索什麼樣的思考過程能夠最穩定最泛化的導向正確的答案這些過程可能看起來跟人類的思維不完全一樣但是它們更符合模型自身內部結構的學習路徑這個方式的轉變威力是巨大的它也讓我們明白在整個自我進化的循環中最終的答案是否正確最最关键的环节不是什么花哨的强化学习算法而是那个验证器一个可靠的能够自动判断答案好坏的验证器也是整个新范式的基石这样他想起了加拿大计算机科学家强化学习之父Richard Sutton在2001年写的一篇文章标题验证是通往人工智能的关键20多年前的冻结在今天的大猿模型时代得到了完美的印证通过这种自我进化的方式训练出来的模型推理能力达到了一个前所未有的高度他所展现出来的智慧与经典的人工智能有着本质的不同戴季周在这里引用了一句名言来自于国际象棋大师加里·卡斯·帕罗夫在1997年输给IBM的深蓝之后说的话他说深蓝的智能就像你给闹钟编程让它准时响起一样是程序化的智能卡斯帕罗夫说的没错深蓝的强大来自于穷举式的搜索它会暴力计算未来几个月的数据甚至幾十步棋的所有可能性然後選擇最優繼這是經典AI的核心思想但是大圓模型的推理完全不同它是一種類人的啟發式的推理過程是從海量的語言數據中湧現出來的而不是依賴於任何顯示的暴力的搜索為了展示這一點單機周分享了一個令人攀岸較絕的例子這個例子來自於谷歌內部的一個模型問題是這樣的請使用數字1到10每個數字只能夠用一次通過加法和乘法運算得到結果2025這是一個非常難的組合優化問題如果用傳統的方法你需要寫一個程序去進行暴力搜索嘗試各種組合但是讓我們看看這個GNDI模型是怎麼思考的單機周展示了模型在生成最終答案之前內部的思考過程模型首先判斷2025是一個相對較大的數字這表明乘法將在其中扮演重要角色這是一個非常像人類的思考方法然後模型突然冒出了一個驚人的洞察值得注意的是2025是45的平方單線周坦言他自己出這道題的時候都完全沒有意識到這一點這給解決問題提供了一個巨大的線索接下來模型的思考繼續深入目標很大我們應該考慮如何得到較大的中間成績我們的目標是構建一些成績讓它接近2025的平方也就是45在經過一長串類似這樣的自我對話和推理之後模型最終給出了答案並且它的答案完美地遵循了自己的思考路徑它將1到10的數字分成了兩組每一組都通過運算得到了45最後模型將兩個45相乘得到了2025整個過程沒有任何窮舉搜索模型就像是一個頂尖的數學家通過洞察啟發著思考和目標才能夠達到最大的成績一步一步逼近了答案這個例子有力的回應了Richard Sutton在他著名的文章《苦澀的教訓》中提出的觀點Sutton在看到AlphaGo的成功之後總結到人工智能領域幾十年的研究表明最終能夠規模化並且取得成功的只有兩種方法 學習和搜索但是丹尼周在這裡對這個苦澀的教訓提出了一個更進一步的看法也許我們只需要學習就夠了一個通過大規模學習訓練出來的模型它的內部湧現出來的推理能力本身就可以完成過去需要依賴搜索當然這並不是說搜索完全沒有用搜索可以作為一種外部工具被模型調用就像是我們使用計算器一樣但是在構建模型的核心推理能力時重點應該放在學習上通過強化學習微調訓練出來的模型已經非常強大但這還不是終點丹尼周接著介紹了兩種在推理時進一步壓縮模型性能的方法意大利亞阿德萊德的一個普通社區誕生了這樣一個男孩他三歲半開始自學算術七歲完成微積分課程八歲參加SAT數學考試拿到了760分這個被稱為數學神童的孩子就是後來的費爾茲獎得主陶哲軒當他在24歲成為加州大學洛杉磯分校歷史上最年輕的正教授時人們驚嘆於他對於數學領域的全域掌控能力從調和分析到數論從片位分方程到組合數學6月15日知名播客主持人Lex Friedman對陶哲軒進行了一場在對話中陶哲軒深入探討了三個核心的命題包括那些困擾人類百年的數學難題究竟難在何處當AI開始輔助定理證明傳統的數學研究範式正在發生怎樣的變革以及在算法日益精進的時代人類數學家的獨特價值又該如何定義呢今天大飛就來試著給大家解答大家梳理一下这场对话的核心内容看看在这位号称数学界莫扎特的天才眼中人类数学和AI的未来将会如何发展我打算用 Temporal 来建立一个 AI 代理人的概念證明我代理人的好處是他的運作程序本身是完全典型的我只是將這些作為我的運作的輸入也就是幾百個 Python 文字的輸入從那裡來的都是 LLM從這裡,我可以看到我的代理人運作正在運作你可以看到活動歷史正在開始被填滿我正在提醒它,LLM 正在回應代理人已經決定它有足夠的資訊來運行它的第一種工具我們來確認一下,它會運行另外,我會要求它在兩邊加上幾天我希望能在这次的活动中能够更加享受到澳大利亚它在两边都增加了几天它知道我要去澳大利亚从LA来到这里搜索航班正在运行现在它在找到真正的航班用真正的API这个工具是使用真正的Stripe API如果我点击这个邮件你会看到Stripe我的邮件是正确的钱量点击这个邮件你会看到这些航班的细节您urer table各位,這就是它的運作只要把任何YouTube連結放進系統裡觀看影片將被轉化成一個四階的AI過程成為你能用的清潔、整理的筆記首先,它不僅能打破音量多元化的AI也能理解影片的聲音、文字和視覺它能創造出非常精準的翻譯清潔語言、加上正確的音準甚至能在需要時分離音源結果,比自動翻譯更好,而且更有用拜拜多謝您收睇時局新聞,再會!这场对话的起点还要从那个让陶哲轩在博士阶段辗转反侧的针尖转向问题说起1918年日本数学家挂骨宗一提出了一个看似很简单的几何问题那就是在平面上一根长度为1的针做180度转向时所扫过的最小面积是多少传统的几何学家们设想了一个三点转向法针先绕一端旋转60度再绕另一端旋转60度从而形成一个等边三角形而实际的面积为8分之π但是这个答案在1928年被苏联数学家阿布拉姆别西科维奇所颠覆他证明了通过构造无限多端的锯齿形路径转向面积可以趋近于0而陶哲轩在1990年代研究这个问题时并没有局限于二维平面而是将目光投向了三维空间如果针具有厚度delta比如说好像现实中的一台望远镜设备那么最小的转向面积是多少它發現當Delta趨近於0的時候體積衰減速率大約會與Delta的對數相關這個發現看似局限在幾何領域卻意外地與流體力學中的起點形成產生了聯繫在流體力學中有一個著名的納維·斯托克斯方程這是一個描述流體運動的偏微分方程圖包含了速度場壓力場和粘性力等多個物理量之間的複雜交互它的核心問題是對於給定的光滑初始條件是否會在有限的時間內產生起點也就是速度或者壓力趨近於無窮大的點從而引發爆破Blow-up現象陶喆軒指出這個問題之所以極端困難源自於它的超臨界特性在流體系統中存在著兩種相互競爭的力量一種是讓系統趨於平穩的粘性耗散另一種是驅動能量運輸的非線性效應在超臨界的狀態下當尺度越來越小的時候它就會變成一個超臨界的特性非线性效应会压倒性的战胜粘性效应它将这个过程比作是麦克斯韦腰的杰作麦克斯韦腰指的是理论上有一个微观的腰井可以巧妙地操纵能量将它不断地推向更小的尺度从而形成一个自相似的能量汇聚链条最终在有限的时间内形成齐电因此这个问题也被列为千禧年七大数学难题之一悬赏高达100万美元2016年陶哲轩在《数学物理通讯》发表的论文中通过构造了一个平均化的三维纳维斯托克斯方程首次严格证明了在特定的初始条件下能量会在有限的时间内集中到非常小的尺度上引发爆破它的敏感来自于七子的电路设计图那些由电阻电容构成的分层能量耗散结构让他联想到流体能量在不同尺度之间的传递机制就像工程师设计电路的时候需要考虑信号延迟一样陶哲轩在《方程》中引入了能量传输的时间制后证明了当粘性耗散无法及时平衡能量的集中时就会形成类似于流体计算机的自相似坍缩在他的设想中驱动计算的单元不再是电子而是一股股以特定速率流动的水流脉冲水流的不同形态可以代表二进制的零和一通过设计出特定的流体交互完全可以实现雨门和货门等逻辑运算再将这些逻辑门串联理论上可以构建出一个將自行啟動並且以指數級的速度重復這個過程從理論上來講這個無限迭代的自我複製過程就可以在真實的納維斯托克斯方程中製造出一個爆破陶哲軒關於流體計算機的構想核心在於複雜的計算能力可以從簡單的底層規則中引線這個思想在數字世界中其實有著一個強大而且經典的印證那就是約翰·康威的《生命遊戲》陶哲軒明確地表示他對於元胞自動機的了解深刻地影響了他對複雜系統行為的思考並且為理解人工智能系統的內在邏輯提供了關鍵的類比《生命遊戲》是一個由極簡規則驅動的二維數字宇宙它的演化模式有時候看起來極為混亂顧似流體湍流然而 陶哲軒指出研究者們在這個極簡系統中發現了令人驚嘆的具有高度組織性的結構比方說他們發現了滑翔者一種能夠穩定向特定方向發射的物質通常是人類給一個明確的指令你問AI說AI agent的翻譯是什麼那AI呢按照你的口令一個口令一個動作把你要求的翻譯翻譯出來他也不會再做更多的事情了那AI agent的意思是說人類不提供明確的行為或步驟的指示人類只給AI目標那至於怎麼達成目標呢AI要自己想辦法去達成目標比如說你給AI某一個研究的議題那你期待說一個AI agent就應該有能力自己提出假設自己設計實驗自己進行實驗自己分析結果如果分析出來的結果跟假設不符合要回頭去修正假設那通常你期待AI agent要解決的目標要達成的目標是需要透過多個步驟跟環境做很複雜的互動才能夠完成而環境會有一些不可預測的地方所以AI agent還要能夠做到靈活的根據現在的狀況來調整他的計劃那AI agent是怎麼做到人類給予一個目標用多個步驟來完成目標的呢那我們可以把AI agent背後運作的過程簡化成以下這張投影片那AI agent的第一個輸入是一個目標這個目標是人給定的那接下來呢AI agent會觀察目前的狀況那AI agent可以看到看到的目前的狀況我們叫做observation那AI agent會看目前的狀況分析目前的狀況決定他要採取什麼樣的行動那今天這個AI agent做的事情叫做action那他執行個action以後會影響環境的狀態會看到不一樣的observation看到不一樣的observation就會執行不同的action那這個步驟會一直循環直到AI agent達成我們要他達成的目標為止那那我只要講到這邊你可能還覺得非常的抽象那我們可以用夏維琪來舉例那AlphaGo是大家非常熟悉的東西AlphaGo其實也可以看作是一個AI Agent這個AI Agent的目標就是下棋要贏他的observation是什麼他的observation是現在棋盤上黑子跟白子的位置現在棋盤上的盤勢那他可以採取的action是什麼他可以採取的action就是在棋盤上的19X19路的範圍中選擇贏一個動作選擇一個可以落子的位置那選擇完可以落子的位置他落下一隻以後他會改變他對手的輸出你落下一隻以後你的對手會落下另外一隻那會改變你觀察到的observation那你就要採取下一個action所以AlphaGo是一個AI agent那他背後運作的原理我想大家其實或多或少也都已經聽過那像這樣的講法我相信你一定覺得非常的熟悉好像在哪裡聽過一樣的段落沒錯如果你有上過任何Basic的Reinforcement Learning RL的課程往往都是用這樣的方式來開場的為什麼呢因為過去要打造AI Agent的時候往往覺得就是要透過RL的演算法來打造AI Agent那怎麼透過RL的演算法來打造AI Agent呢IL這個演算法就是他可以去learn一個agent那這個agent可以maximize reward所以你要把你的目標呢轉換成一個叫做reward的東西那這個reward呢是人定義的越接近你的目標reward就越大那如果在下圍棋裡面你通常就會定說贏棋reward就是贈一輸棋reward就是負一然後你要訓練的那個AI agent就會學習去maximize reward透過IL的演算法所以透過IL的演算法其實你也有可能學一個AI Agent但是透過RL演算法的局限是你需要為每一個任務都用RL的演算法訓練一個模型AlphaGo在經過了大量的訓練以後他可以下圍棋但並不代表他可以下其他的棋類西洋棋或將棋我知道你可能看了一篇文章AlphaGo ZERO他說圍棋外也可以下將棋跟西洋棋那是另外訓練後的結果能夠下將棋的那個模型並不是原來可以下圍棋的那個AlphaGo他們是不同的模型模型有不同的參數而今天AI Agent又再次被討論是因為人們有了新的想法我們能不能夠直接把Large Language Model把LLM直接當成一個AI Agent來使用呢也就是說我們的Agent背後就是一個Language Model你要告訴他你的目標是什麼的時候直接用文字輸入要告訴他要下圍棋就先給他圍棋的規則然後跟他說你的目標就是贏得勝利那接下來歡迎環境 因為一般語言模型是用文字作為輸入所以你可能需要把環境轉化成文字的敘述不過我這邊寫了一個option今天有很多語言模型都是可以直接看圖片的所以把環境轉成文字的敘述今天也不一定是必要的接下來語言模型要產生action那產生action的方式可能就是用一段文字來決定它的action是什麼它的action用一段文字來描述我們需要把那段文字轉譯成真正可以直接看的文字執行的指令真正可以執行的行動然後就會改變環境看到不同的observation然後AI agent的運作就可以持續下去直到達成目標今天啊AI agent再次爆紅並不是真的有了什麼跟AI agent本身相關的新的技術而是在LLM變強之後人們開始想我們能不能直接用Large Language Model來實踐人類擁有一個Agent的渴望好那我們這邊呢是拿下棋做例子也許你就會很好奇說現在的語言模型能不能夠下棋呢其實早就有人嘗試過了有一個在語言模型領域很多人使用的benchmark叫做BigBench它是什麼時候做的呢它是2022年上古時代做的以後有Chad GVT之前我們都叫上古時代然後在2022年上古時代的時候就有人嘗試過用那個時候的語言模型看看能不能下西洋棋那時候語言模型沒有辦法真的看圖所以你需要把棋盤上黑紙跟白紙的位置轉成文字的敘述輸入給這個語言模型所以這個就是語言模型實際上看到的棋盤的樣子那就問他說下一步要下哪裡才能夠給對方將軍呢那語言模型就會給你一個答案右上角這個圖啊橙色的線是正確答案綠色的線是當時各個不同的語言模型所給的答案沒有任何一個語言模型給出正確答案但雖然沒有任何語言模型給出正確的答案但你可以看這個時線是當時比較強的模型他們雖然沒給出正確答案但他們所選擇走的路是符合西洋棋規則的但是也有很多比較弱的模型這個虛線是比較弱的模型他們都亂走根本搞不懂西洋棋的規則就隨便按照自己的意思來下不過這個是上古時代的事情了那現在更強的LOM能不能下西洋棋呢有人試過了有一個很知名的是直接拿Chad GVT 01跟D6 R1兩個模型來下西洋棋那這是一場驚天動地的對決這個影片好幾百萬觀看次數啊那這兩個模型呢他們殺得難分難解難分難解是因為他們實在是太弱了他們有很多不符合西洋棋的規則比如說把兵呢當作馬來用或者是他的主帥可以他的那個主教可以無視前面的一切阻擋或是他會突然就是空中通降一個自己的子在對方的陣地裡面把對方的子吃掉然後D-SIG還在自己的棋盤上隨便變出一個城堡然後最後D-SIG用自己的城堡把自己的兵吃掉以後他宣布他贏了對方要投降然後GPT想了一下覺得我確實輸了然後就投降了所以這個棋局就這樣結束了所以看起來現在這個最強的語言模型離要下棋還有一段距離但這並不代表他們不能夠作為AIA局的來做其他事情那等一下會舉一些例子看看現在的語言模型可以做什麼樣的事情那這一門課另外最主要想要強調跟大家傳輸的資訊是我們還能多做什麼讓這些語言模型作為AI Agent的時候運作的更加順利那剛才講法比較像是從比較像是從過去常見的這個Agent的觀點來看語言模型怎麼套用到Agent的框架下那接下來我們換一個角度看說從垃圾人物就慢慢的Model 的角度來看到底當他作為一個 Agent 的時候他要解的問題有什麼不同好那我們從 Large Language Model 的角度來看首先他得到一個目標然後接下來呢他得到一個 Observation然後根據這個 Observation他要決定接下來要採取什麼樣的 Action採取什麼樣的動作那他採取完動作之後他的動作會影響外界的環境看到新的 Observation看到新的 Observation 以後要採取新的動作這個過程就會再反覆繼續下去那在這一系列的過程中看到observation採取action看到observation採取action其實憑藉的都是語言模型原來就有的接龍的能力所以從語言模型的角度來看當我們把它當作一個AI agent來使用的時候對他而言他做的事情是完全沒有什麼不同的他就是繼續在做他唯一會做的文字接龍而已所以從語言模型的角度來看AI agent並不是一個語言模型的新技術它比較像是一個語言模型的應用所謂AI Agent意思就是依靠現在語言模型已經有一定程度的通用能力看看能不能夠直接把它們當作Agent來使用那因為我說這個AI Agent並不是語言模型的新技術它只是一個語言模型的應用所以要注意一下在以下課程中沒有任何的模型被訓練以下我所有所講的東西都是依靠一個現有的語言模型的能力來達成的那AI Agent其實不是最近才熱門一直有人在嘗試怎麼讓語言模型變成一個Agent我怎麼把語言模型當作AI Agent來使用Chad GPT在2022年年底爆紅所以在2023年的春天就有一波AI Agent的熱潮好多人都用Chad GPT作為背後運作的語言模型來打造AI Agent那個時候最有名的就是Auto GPT那其實在2023年的機器學習我們也有一堂課是講那個時候的AI agent那你可以看看那堂課看看那一堂課的AI agent跟今天講的有什麼樣的差異不過後來2023年AI agent的熱潮過一陣子就消退了因為人們發現這些AI agent沒有我們想像的厲害一開始好多網紅在吹噓這些AI agent有多強又有多強真的試下去也沒那麼強所以熱潮就過去了那用LLM來運行一個AI agent相較於其他的方式可能有什麼樣的優勢呢那過去啊當你運行一個agent的時候比如說像AlphaGo他能夠做的只有有限的事先設定好的行為AlphaGo真正能夠做的事情就是在19X19個位置上選擇一個可以落子的位置也就是說他真正能夠採取的行為就是從19X19個選擇題中選擇一個他能夠採取的行為但是如果你的agent是一個Large Language Model的話他就難有了近乎無限的可能Large Language Model可以講任何話可以產生各式各樣近乎無窮無盡的輸出這就讓你AI Agent可以採取的行動不再有侷限有更多的可能性舉例來說我們等一下就會很快看到的今天這些AI Agent在有些問題他解不了的時候他可以憑藉他可以有各式各樣輸出的能力來直接呼叫一些工具來幫忙解決他本來解決不了的問題一個AI Agent的優勢另外一個用Large Language Model運行AI Agent的優勢是過去如果用Reinforcement Learning的方法來訓練一個AI Agent那意味著什麼意味著你必須要定義一個東西叫做Reward那如果你今天是要訓練一個AI Programmer那你可能會告訴AI Programmer說如果你今天寫的程式有一個compile的Error那你就得到Reward-1但為什麼是-1為什麼不是-10為什麼不是-17.7這種東西就是沒人說的清楚所以這個reward在做reinforcement learning的時候就是一個要調要通靈的東西那今天如果是用LLM驅動的AI agent呢你今天就不用幫他訂reward了今天有compile error你可以直接把compile error的log給他他也許根本就讀得懂那個log他就可以對程式做出正確的修改而且相較於reward只有一個數值直接提供error的log可能提供了agent更豐富的資訊讓他更容易按照環境給的回饋環境目前的狀態來修改他的行為接下來舉幾個AI Agent的例子那講到AI Agent也許最知名的例子就是用AI村民所組成的一個虛擬村莊這個虛擬村莊是在什麼時候成立的呢2023年在古代就已經有人做過這個虛擬村莊了那裡面的NPC通通都是用語言模型來運行的而這些NPC是怎麼運行的呢它運行的呢 首先每個NPC都有人為設定的目標有的NPC他要辦情人節派對有的NPC要準備考試每個人都有一個他自己想做的事情那這些NPC呢 會觀察會看到環境的資訊那時候的Language Model都只能讀文字所以環境的資訊需要用文字來表示所以環境的資訊對一個語言模型來說看起來可能就是這個語言模型旁邊有一個叫做Eddie的人他正在讀書然後呢 他看到廚房然後呢他看到一個櫃子然後看到伊莉莎白呢正在裝作裝飾正在裝飾房間等等然後根據這些observation這個語言模型就要決定一個他想要做的行為比如說也許不早了所以就上床睡覺那需要有一個轉譯器把他說出來的這個行為轉成真正能夠執行的指令那這個agent就真會走到床邊然後去睡覺好所以這個是2023年的時候用AI來這個運行NPC的一個行為實驗其實後來還有更大規模的實驗有人把Minecraft中的NPC通通換成AI的NPC那我就把相關的影片連結留在這個投影片上面那根據這個影片連結的描述就說這些AI很厲害他們組織了自己的交易的金融體系然後還組織了自己的政府自己制定憲法自己管理自己我不知道是真的還假的這個是這個影片說的剛才講的那些遊戲你可能比較不容易接觸到可能也沒什麼影響那今天也許你馬上就會接觸到的AI Agent就是讓AI來真正使用電腦雖然這個聽起來有點弔詭AI本身也就是一個電腦但他現在要來真正的像人類一樣來使用另外一個比較低端的電腦來做事那其中比較有代表性的例子就是Cloud的Computer Use還有ChairGPT的Operator那我們在上次上課的影片中跟大家講過OperatorOperator介面長這樣那他會建議可以做的事情比如說可以訂pizza可以預約下週的居家清潔等等那像這種使用電腦的AI agent他的目標就是你的輸入就是你告訴他我要去訂pizza你告訴他上網幫我買一個東西那這就是他的目標那他的observation呢他的observation可能是那個電腦的螢幕畫面今天很多語言模型都是可以直接看圖的所以其實可以直接把圖片當作輸入可以直接直接把電腦畫面當作輸入提供給AI Agent那AI Agent要決定的就是他要按鍵盤上哪一個鍵或者是要按滑鼠的哪一個按鈕那其實讓AI使用電腦啊不是最近才開始有的野望其實早在2017年就有篇paper叫Words of Beats嘗試過使用AI Agent你看他這個文章的標題他把自己的文章標題說他是一個Web-based Agent那只是那個時候能夠互動的頁面還是比較有名的原始的頁面你可以看到下面這些AI agent他真正能夠處理的是比較原始的頁面那個時候也沒有大型語言模型所以那時候的方法就是硬圈一個CNN直接吃螢幕畫面當作輸入輸出就是滑鼠要點的位置或者是鍵盤要按的按鈕看看用這個方法能不能夠讓AI agent在網路的世界中做事這個是2017年這甚至不能說是上古時代以後有這個BERT以前的時代就是史前時代這個不只是史前時代他史前時代比較早期所以這是舊石器時代的產物好那後來有了語言模型之後啊人們就開始嘗試用語言模型來當做AI agent來運行一個agent讓他在網路的世界中活動那這一頁投影片是列舉了幾個比較具代表性的例子那這一波潮流大概是在2023年的暑假開始的像Mind2WebWeb Arena 還有Visual Web Arena就跟今天的Operator非常的像就是給這個這個語言模型看一個螢幕的畫面或者是看HTML的code然後他自己決定他要幹什麼期待他最後可以解決一個問題比如說在Mind2Web的第一個例子裡面就給他這個畫面然後跟他說請他幫我們訂一個機票那還有什麼樣AI agent的應用呢今天你可以用AI來訓練另外一個AI模型這就是等一下作業二助教會跟大家講的事情那用AI來訓練模型那其實這個運作的過程就是就是你的目標就是要過強硬基礎然後你提供給LLM訓練資料他寫一個程式用這些訓練資料來訓練模型那他可能可以得到這個模型的正確率根據正確率再重新寫一個程式再得到新的正確率就這樣一直運作下去那有很多知名的用AI來訓練模型的framework比如說AIDE那你看他的這個技術報告的這個標題就知道他們想做什麼他是要做一個Machine Learning Engineer Agent我是AutoCAD的Target他就是要用Multi-Agent的Framework來解Data Science的Competition那在我們的作業中你就會體驗到到底AI Agent做不做得了機器學習這門課的作業那最近呢Google說他們做了一個AI Coscientist不過他們並沒有真的釋出模型所以你也不知道說實際上做的怎麼樣這個服務並不是公開的那他們說他們做了一個AI Coscientist就是用AI來做研究不過這個AI Coscientist還是蠻有侷限的他根本真的做實驗啦他只能夠提proposal就是你把一些研究的想法告訴他他把完整的proposal規劃出來實際上做得怎麼樣不知道啦那你要看他的blog裡面有一些比較誇張的案例說什麼本來人類要花10年才能夠得到研究成果AI agent花兩天就得到了也不知道真的還假的他舉的是一些生物學的例子所以我也無法判斷他講的是不是真的那個發現是不是真的很重要這個coscientist就是用AI agent來幫研究人員做研究拜拜Did you know that AI is set to create 97 million new jobs by 2025?好 那我們剛才講的AI agent它的互動方式是侷限在回合制的互動有一個observation接下來執行action有一個observation接下來執行action但是在更真實的情境下這個互動是需要即時的因為外在的環境也許是不斷在改變的如果你在action還沒有執行完的時候外在環境就改變了那應該要怎麼辦有沒有辦法做到更即時的互動呢更即時的互動可能應該像是這樣子當模型在決定要執行Action 1正在執行的過程中突然外在環境變了這個時候模型應該有辦法立刻轉換行動改變他的決策以因應外界突如其來的變化你可以想說什麼樣的狀況我們會需要用到這樣的AI Agent能夠做即時互動的呢其實語音對話就需要這種互動的模式文字的對話使用GVT是大家比較熟悉的你輸入一段文字他就輸出一段文字這是一來一往回合制的互動但是人與人間真正的對話不是這樣子的當兩個人在對話的時候他們可能會互相打斷或者是其中一個人在講話的時候另外一個人可能會同時提供一些回饋比如說好你說的都對這樣那這些回饋可能沒有什麼特別語意上的含義他只是想告訴對方我有在聽但是像這樣子的回饋對於流暢的交流來說也是非常重要的如果你在講電話的時候對方完全都沒有回應你會懷疑他到底有沒有在聽所以我們今天能不能夠讓AI在跟使用者互動的時候用語音互動的時候就跟人與人間的互動一樣而不是一來一往回合制的互動呢其實也不是不可能的今天GPT 4O的一個Voice Mode高級語音模式也許在某種程度上就做到了這種即時的互動那這個投影片上是舉一個例子假設有人跟AI說你說一個故事這個是AI觀察到的第一個observation有人叫他說一個故事他現在就開始講故事了他就說從前從前那這時候人說了一個嗯好啊這個可能是第二個observation但AI要知道說這個observation不需要改變他的行為跟他的行為沒有直接的關係所以故事就繼續講下去有一個小鎮然後人說這個不是我要聽的故事這個我聽到了那AI可能要馬上知道說那這個不是人要聽的那也許我就應該停下來換另外一個故事那今天AI有沒有辦法停下來做到這種即時的互動呢那怎麼做這種即時的互動非回合制的互動有點超過我們這門課想要講的範圍如果你有興趣的話你可以讀這篇文章那這篇文章想要做的事情是評量現在這些語音模型互動的能力那在這篇文章裡面也對現有的這個可以做互動的語音模型做了一個比較完整的survey是一直survey到今年的1月所以你可以看這篇文章知道說現在這些可以互動的模型它可以做到什麼樣的地步那這是我們實驗室的林冠廷同學跟他在這個Berkeley UW和MIT的合作夥伴一起做的文章那這邊順便說明一下以後這門課呢我們投影片上引用論文的原則那引用論文的原則就是如果我找得到archive的連結的話那我就把文章直接我就直接貼archive的連結什麼是archive呢假設你不是computer science背景的話也許我就要解釋一下什麼是archivearchive的意思就是一般做研究你是寫完文章投稿到一個期刊或者是國際會議然後被接受以後才發表出來但是對於AI的領域因為變化實在太快幾個月前就已經是古代了所以期刊那種一審就要一年或者是國際會議一審要一兩個月這種步調是沒有辦法再不適用於AI的領域所以現在一種習慣的發表方式就是做出東西以後直接放到一個公開的網站叫做Archive然後就不審了立刻公開然後你就可以讓全世界的人看到你的文章那有很多人會覺得引用archive的連結不夠正式但是很多重要的文章其實現在不見得投稿國際會議他就只有archive的連結所以我會選擇如果找得到archive的連結的話就直接引用archive的連結其實現在大家都在archive上看文章你知道國際會議現在比較像是經典回顧這樣子每篇文章我幾乎都在archive上看過了去只是說 喔 原來你投到這裡啊這樣的感覺那引用archive的連結還有一個好處可以直接從Archive的連結看出這篇文章的時間所以Archive的連結裡面的數字前面兩個就是年份後面兩個就是月份你看這個數字就可以知道說這篇文章是在什麼時候被放在Archive也就是什麼時候被發表的可以讓你對於每一個研究他誕生的時間更有感覺好那接下來呢我們會分三個面向來剖析今天這些AI Agent的關鍵能力那第一個面向是我們要來看這些AI Agent這個AI Agent能不能夠根據他的經驗過去的互動中所獲得的經驗來調整他的行為第二部分是要講這些AI agent如何呼叫外部的援助如何使用工具第三部分要講AI agent能不能夠執行計畫能不能做計畫那我們來講一下AI怎麼根據過去的經驗或者是環境的回饋來調整他的行為那AI呢AI agent需要能夠根據經驗來調整行為比如說有一個作為AI Programmer 的 AI Agent他一開始接到一個任務那他寫了一個程式那這個程式 compile 以後啊有錯誤訊息 compile 以後有 error那應該要怎麼辦呢他應該要能夠根據這個 error 的 message來修正他之後寫的程式那在過去啊講到說啊你收到一個 feedback接下來要做什麼的時候也許多數機器學習的課程都是告訴你那我們就來調整參數根據這些收集到的訓練資料也許是順帶一提如今我們其實還是在對於定格的台灣的這幾個不同的CPU 對於速度的分析以及使用速度系統的計畫forcement learning的algorithm來調整參數但不要忘了我們剛才就強調過在這一堂課裡面沒有任何模型被訓練所以我們今天不走這個路線那不更新模型的參數模型要怎麼改變它的行為呢依照今天Large Language Model的能力要改變它的行為你也不用微調參數你就直接把錯誤的訊息給它它接下來寫的程式就會不一樣了就結束了那你可能會問說那之前它寫的程式是錯的為什麼給錯誤訊息它寫的程式就對了呢明明就是同一個模型那你想想看模型做的事情就是文字接龍你給它不同的輸入它接出來的東西就不一樣一開始會寫錯的程式是因為他前面要接的部分只有這麼多所以寫個錯的程式當今天要接的內容包含了錯誤的訊息的時候他接出來的結果可能就會是正確的了那今天已經有太多的證據說明這些語言模型可以根據你給他的回饋改變他的行為不需要調整參數那如果你有使用這些語言模型的經驗你也不會懷疑他們有根據你的回饋調整行為的能力今天真正的議題是如果我們是把過去所有的經驗都存起來要改變語言模型的行為要讓他根據過去的經驗調整行為就是把過去所有發生的事情一股腦給他那就好像是語言模型每次做一次決策的時候他都要回憶他一生的經歷也許在第100步的時候還行到第1萬步的時候過去的經驗太長了他的人生的資訊已經太多了也許他沒有足夠的算力來回顧一生的資訊他就沒有辦法得到正確的答案這讓我想到什麼呢這讓我想到有一些人有超長自傳室記憶他可以把他一生中所有發生的事情記下來然後那一些人你可以隨便問他一個某個人的電話號碼他都會背出來你告訴他某年某月某日某時發生了什麼事他也都可以講出來有一些人他的頭腦就像是一個影印機一樣會把所有他看過的事情都原封不動的記憶下來但這種超常自傳式記憶又被叫做超抑震你看到震這個字就知道說人們覺得這是一種疾病這聽起來記憶力很好是一種祝福但實際上對這些患者而言據說這種患者世界上可能不到100例那這是一個2006年的才被論文發表的一個症狀那據說這些患者其實日常生活並沒有辦法過得很開心因為他們不斷的在回憶他的人生往往一不小心就陷入了一個冗長的回憶之中也很難做抽象的思考因為他的人生已經被他的記憶已經被太多枝微末節的所視所佔據所以沒有辦法做抽象式的思考所以讓一個AI agent記住他一生所有經歷的事情告訴他你每次做一個決策都是根據你一生所有經歷過的事情再去做決策那也許對AI agent來說並不是一件好事最終當他的人生過長的時候他會沒有辦法做出正確的決策所以怎麼辦呢也許我們可以給這些AI agentMemory這就像是人類的長期記憶一樣發生過的事情我們把它存到這個Memory裡面當AI agent看到第一萬個observation的時候他不是根據所有存在Memory的裡面的內容去決定接下來要採取什麼action而是有一個叫做read模組這個read的模組會從memory裡面只選擇跟現在要解決的問題有關係的經驗把這些有關係的經驗放在observation的前面讓模型根據這些有關係的經驗跟observation在做文字接龍接出它應該進行的行為那你有這個read模組就可以從memory裡面從長期記憶中篩選出重要的訊息讓模型只根據這些跟現在情境相關的訊息來進行決策那怎麼樣打造這個reader的模組呢其實你可以想這個reader的模組就想成是一個retrieval的system想成是一個檢索的系統那第一萬步看到的observation其實就是問題那模型的AI agent的memory長期記憶其實就是資料庫那你就把拿這個檢索系統根據這個問題從這個資料庫裡面檢索出相關的資訊這整個技術跟RIG沒有什麼不同其實它就是RIG你可以直接把RIG的任何方法直接套用到這個地方唯一不一樣的地方只是如果是RIG的話存在Memory裡面的東西等於是整個網路那是別人的經驗而對AI Agent而言現在存在Memory裡面的東西是他自己個人的經歷差別的是經歷的來源但是用來搜尋的技術是可以完全直接現套RIG的技術的如果你今天想要研究AI Agent按照經驗來修改它的行為那你可以考慮一個叫做Stream Bench的Benchmark那在Stream Bench裡面呢會有一系列的問題然後呢AI會依序去解這些問題它先解第一個問題得到第一個問題的答案然後接下來它會得到第一個問題答案的反饋那在這個Stream Bench目前的版本裡面呢因為所有的問題都是有標準答案的所以AIAI Agent得到的回饋是binary的對或者是錯那根據他過去的經驗他就可以修正他的行為期待他在第二個問題的時候可以得到更準確的答案得到更高的正確率然後這個過程就一直持續下去那假設有1000個問題的話那就等AI Agent回答完最後問題的時候這個互動就結束了那最後結算一個Agent根據經驗學習能力的好壞根據經驗調整行為能力的好壞一整個回答的過程中平均的正確率越能夠根據經驗學習的Agent他應該能夠用越少的時間看過越少的回饋就越快能夠增強他的能力就可以得到比較高的平均的正確率那這個Benchmark呢是API的研究人員打造的一個Benchmark那在這個Benchmark裡面的Baseline就是有使用到我剛才講的類似RAG的技術也就是說當模型在回答第100個問題的時候並不是把前面第一個到第99個問題通通丟給他去做文字接龍這樣這個sequence太長了一般的語言模型根本讀不了這麼長的輸入所以實際上的做法就是你需要有一個檢索的模組這個檢索的模組只從過去所有的經驗中檢索出跟現在要回答的問題有關係的經驗然後呢語言模型只根據這些有關係的經驗還有現在的問題來進行回答來產生他的答案他的行動來產生他的答案好那這一招有沒有用呢這一招其實非常的有用那在這一頁圖裡面橫軸啊他這邊的用詞是Time Step但其實指的就是一個一個的問題總共有1750幾個問題那縱軸指的是平均的正確率那在這個圖上面呢最低的這條灰色線指的是說假設沒有讓模型做任何學習他回答每一個問題都是independent的回答問題間沒有任何的關聯好完全沒有調整他的行為那你得到的正確率是灰色的這一條線是最低的那黃色這一條線是說只固定隨機選五個問題那每次模型回答問題的時候都是固定看那五個問題來回答都是固定把五個問題當作經驗來回答那也可以得到的是黃色這一條線那如果你是用 RIG 的方法從一個 memory 裡面去挑選出最有關係的問題跟現在要解決的問題最有關係的經驗那你可以得到的是粉紅色的這一條線那可以看到比黃色的線他的正確率還要高上不少那最後結果最好的是紅色這一條線那這個怎麼做的那大家就自己再去詳細閱讀論文那在StreamBench裡面呢還發現一個有趣的現象是值得跟大家分享這個現象是負面的回饋基本上沒有幫助對現階段的語言模型而言所以你要提供給語言模型經驗讓他能夠調整他行為的時候給他正面的例子比給他負面的例子要好也就是說具體而言提供給他過去哪些類似的問題得到正確答案比提供給他過去哪些問題得到錯誤的答案還更有效還更能引導模型得到正確的答案那這邊是真正的實驗結果做在好幾個不同的Data set上面Stream Bench裡面本來就包含了好幾個不同的Data set那這個縱軸呢0代表完全沒有做完全沒有根據經驗調整行為那然後藍色代表說不管是正面還是負面的例子都用如果不管正面還是負面的例子都用在多數情況下模型都可以表現得比較好當然有一些例外但是如果只用負面的例子呢如果只用負面的例子基本上是沒有幫助而且甚至是有害的那如果說只用正面的例子在所有的情況下模型可以得到更好的結果那這也符合過去的一些研究因為過去有人研究過使用圓模型要怎麼樣比較有效有一個發現就是與其告訴語言模型不要做什麼不如告訴他要做什麼如果你希望他文章寫短一點你要直接跟他說寫短一點不要告訴他不要寫太長比較他不要寫太長他不一定聽得懂叫他寫短一點比較直接他反而比較聽得懂那這也符合這邊StreamBench的發現就是負面的例子比較沒有效與其給語言模型告訴他怎麼做錯不如告訴他怎麼做是對的好 那我們剛才講到了有一個read的模組那有關記憶的部分呢是不是要把所有所有的資訊通通存到memory裡面呢存到長期的記憶庫裡面呢如果我們把這些agent經歷的所有的事情都放到長期的記憶庫裡面的話那裡面可能會充斥了一堆雞毛蒜皮不重要的小事最終你的memory長期記憶庫可能也會被塞爆如果說你是做那種那個AI村民啊AI村民他多少多數時候觀察到的資訊都是些無關緊要的小事那如果你看他觀察到那個log多數都是啥事也沒有就那邊有一張桌子啥事也沒有那邊有一張椅子啥事也沒有多數時候都是啥事也沒有所以如果把所有觀察到的東西通通記下來的話那你的memory裡面就都只是被一些雞毛蒜皮的小事佔據所以怎麼辦呢也許應該有更有效的方式來決定什麼樣的資訊應該被記下來應該只要記重要的資訊就好那怎麼讓語言模型只記重要的資訊就好呢你可以有一個RIDE的module那RIDE的module決定什麼樣的資訊要被填到長期的記憶庫裡面什麼樣的資訊乾脆直接就讓他隨風而去就好了那怎麼樣打造這個RIDE的記憶庫呢有一個很簡單的方法就是RIDE的模組也是一個語言模型甚至就是AI agent自己這個AI agent他要做的事情就是根據他現在觀察到的東西然後問自問一個問題這件事有重要到應該被記下來嗎有就把它記下來如果沒有就讓它隨風而去那除了RE跟WRITE這兩個模組以外還有第三個模組那這個模組沒有固定的名字在文件上的名字沒有固定的名字我們可以暫時叫它Reflection反思的模組那這個模組的工作是對記憶中的資訊做更好的更高級的可能是抽象的重新整理你可以把這些記憶裡面的內容在經過Reflection的模組重新反思之後得到新的想法那也許Read的模組可以根據這些新的想法來進行搜尋那這樣子也許可以得到更好的經驗那幫助模型做出更好的決策而這個Reflection的模組可能也是一個語言模型就是AI agent自己你可以只是把過去的這些記憶丟給Reflection的模組然後叫Reflection模組想一想看他從這些記憶裡面能不能夠有什麼樣新的發現比如說可能有一個observation是我喜歡的異性每天都跟我搭同一部公車而另外observation是他今天對我笑了那你推出來的reflection結果就他喜歡我這樣一個錯覺人生三大錯覺之一就是這一種你就得到一些新的thought你就得到一些新的想法那你之後在做決策的時候就可以用這些新的想法雖然你沒有實際觀察到但它是被推論出來的根據這些推論出來的想法來做決策那除了產生新的想法之外也可以為以前觀察到的經驗建立經驗和經驗之間的關係也就是建立一個knowledge graph用READ的module根據這個Knowledge Graph來找相關的資訊那我知道在RIG的領域使用Knowledge Graph現在也是一個非常常見的手法那最知名的可能就是Graph RIG系列這個研究就把你的資料庫把它變成一個Knowledge Graph那今天在搜尋跟回答問題的時候是根據Knowledge Graph來搜尋回答問題可以讓RIG這件事做得更有效率或是另外一個非常類似的例子就是HIPPO RIGHIPPO RIG HIPPO這個HIPPO不是指真正的合作他指的應該是那個海馬迴那個人腦中的一個結構然後他覺得做建這種knowledge graph就跟海馬迴的運作呢非常的類似所以他叫做HIPPO-RIG有一些跟graph有關的RIG的方法那你完全可以透過reflection的模組把經驗建成一個graph以後把那一些graph RIG的手法直接套到AI agent裡面那大家可能都知道說這個ChairGBT啊現在其實真的是有記憶的所以可以感受到這個OpenAI想把Chet GVT變成一個AI agent的決心比如說我跟Chet GVT說我週五下午要上機器學習這門課那他就給我一個回答說要我幫助你做什麼事情嗎接下來我告訴他記下來你跟他講記下來之後他的這個RIDE模組就啟動了他知道這件事情是要被記下來的他就會說那我記下來了以後你週五要上機器學習這門課那RIDE模組什麼時候要啟動是他自己決定的所以很多時候你希望他記下來的時候他就是不啟動不希望他啟動的時候他就是啟動那個是模型自己決定的但是有一個方法可以基本上一定能讓他啟動就明確的跟他講把這件事記下來基本上都幾乎確定能夠啟動那個RIDE的模組 讓RIDE的模組把這件事情記下來那記下來的東西在哪裡呢你可以看在設定裡面有一個個人化然後有一個叫記憶的部分那你點這個管理記憶就可以看到ChangeBT它透過RIDE的模組寫在它的Memory裡面這個就是它作為一個AI Agent的長期記憶裡面的東西比如第一條是你叫做血輪眼卡卡西這樣有一次不小心跟他說你是卡卡西不知道為什麼他就覺得自己是血輪眼卡卡西了 然後他也記得我剛才跟他講的周五下午要上機器學習這門課但是其實模型的記憶也是會出錯的因為要寫什麼樣的東西到記憶裡面是模型自己決定的而且他並不是把對話的內容就一五一十的直接放到記憶裡面他是經過一些昇華反思之後才放進去的所以他的反思可能會出錯比如說他覺得我是一個臺灣大學的學生雖然我是老師但是他從過去的對話誤以為我是一個學生所以就存了一個錯誤 的資訊在他的記憶裡面還有其他一堆他想記的東西比如說我給過什麼演講給過什麼Tutorial他都把他記下來就是了那這些有記憶的ChairGBT他可以使用他的記憶比如說我跟他說禮拜五下午是去玩好嗎這個時候記憶模組就被啟動了但是他是怎麼被啟動的其實就不太清楚了他到底是把所有記憶的內容通通都放到這個問題的前面直接讓模型做回答還是說也有做RIG只是選擇了一個 相關的記憶內容呢這個我們就不得而知了總之當我問他週五下午出去玩好嗎這個read的模組就啟動了他就說下午不是要上課嗎怎麼能夠出去玩好聰明啊他知道下午要上課頂厲害的然後問他你是誰剛才我說過他是血輪眼卡卡西誰就覺得志祺是血輪眼卡卡西如果你想要知道更多有關AI Agent記憶的研究的話那這邊就是放了幾篇經典的論文給大家參考包括Memory GPT這是23年的論文Agent Workflow Memory 還有一個最近的Agentic Memory for LLS Agents是25年的論文所以23到25年都引用一篇告訴你說這方面的研究是持續不斷的接下來呢我們要跟大家講現在這些語言模型怎麼使用工具 Wells Fargo 首次推出,可以幫助你計劃夢想。所以你的夢想車和假期家居可能會比你想像中更接近。準備遇見夢想團隊?你可以,在 Wells Fargo 。 Laundry can be really...Ew那什麼叫做工具呢但語言模型本身對我們人類來說也是工具那對語言模型來說什麼東西又是它的工具呢所謂的工具就是這個東西啊你只要知道怎麼使用它就好它內部在想什麼它內部怎麼運作的你完全不用管這就是為什麼肥宅如果一直幫另外一個修電腦的話就會被叫做工具 因為沒有人在意肥宅的心思只知道他能不能夠修電腦而已所以這個就是工具的意思那有哪些語言模型常用的工具呢最常用的就是搜尋引擎然後呢語言模型現在會寫程式而且可以執行他自己寫的程式那這些程式也算是某種工具甚至另外一個AI也可以當作是某一個AI的工具有不同的AI有不同的能力比如說現在的語言模型如果他只能夠讀文字的話 那也許可以呼叫其他看得懂圖片聽得懂聲音的AI來幫他處理多模態的問題或者是不同模型他的能力本來就不一樣也許平常是小的模型在跟人互動但小的模型發現他自己解不了的問題的時候他可以叫一個大哥出來大哥是個大的模型但大的模型的運作起來就比較耗費算力所以大的模型不能常常出現大的模型要在小的模型召喚他的時候才出面回答問題大哥要偶爾才出來幫小的模型 來努力解決事情那其實這些工具對語言模型來說都是function都是一個函式當我們說語言模型在使用某一個工具的時候其實意思就是他在調用這些函式他不需要知道這些函式內部是怎麼運作的他只需要知道這些函式怎麼給他輸入這些函式會給什麼樣的輸出那因為使用工具就是調用函式所以使用工具又叫做functional所以有一陣子很多語言模型都說他們加上 上了function code的功能其實意思就是這些語言模型都有了使用工具的功能好 那語言模型怎麼使用工具呢等一下我會講一個通用的使用工具的方法但實際上使用工具的方法很多甚至有一些模型是專門針對來練習他就是訓練來使用工具的那他如果是針對使用工具這件事做訓練那他在使用工具的時候你可能需要用特定的格式才能夠驅動他那那個就不是我們今天討論的問題或者是假設你用 使用這個OpenAI Check GPT的API的話你會知道使用工具這件事情是要放在一個特殊的欄位所以對OpenAI來說他的模型在使用工具的時候也有一些特殊的用法那我這邊講的是一個最通用的用法對所有的模型今天比較能力比較強的模型應該都可以使用好 怎麼樣通用的方法可以讓模型使用工具呢就是直接跟他講啊就告訴他怎麼使用工具你就交代他可以使用工具 那你就把使用工具的指令放在兩個TOR符號的中間使用完工具後你會得到輸出輸出放在兩個OUTPUT符號的中間所以他就知道工具使用的方式了接下來告訴他有哪一些可以用的工具有一個韓式叫做TEMPERATURE他可以查某個地點某個時間的溫度他的輸入就是地點跟時間給他個使用範例TEMPERATURE括號台北某一段時間他就會告訴你台北在這個時間的氣溫接下來你就把你的 你的問題連同前面這些工具使用的方式當作Prompt一起輸入給語言模型然後他如果需要用工具的話他就會給你一個使用工具的指令那前面這些教模型怎麼使用工具的這一些敘述他叫做System Prompt那查詢使用調用這些工具的這段話某年某月某日高雄氣溫如何這個是User Prompt那如果你有在使用這個ChetGBT的API的話你知道你的使用 輸入要分成System Prompt跟User Prompt那很多同學會搞不清楚System Prompt跟User Prompt有什麼樣的差別那System Prompt指的是說你在開發應用的這個Developer下的這個Prompt這個Prompt呢是每次都是一樣的每次你都想要放在語言模型最前面讓他去做文字接龍的這個敘述叫做System Prompt那每次使用它的時候都不一樣通常是這個服務的使用者輸入的內容叫做User Prompt 那在ChartGVT的API裡面特別把System Prompt跟User Prompt分開也是要分開輸入的因為System Prompt跟User Prompt它有不同的優先級System Prompt它優先級比較高如果System Prompt跟User Prompt有衝突的時候模型知道它要聽System Prompt的不要聽User Prompt的好 那有了這些Prompt以後告訴模型怎麼使用工具問它一個問題那它發現這個問題調用工具可以回答它就會自動輸出ToolTemperature高雄時間然後Tool 告訴你說他想要調用根據我們的敘述去調用這個工具但是不要忘了語言模型真正做的事就是文字接龍所以這一串東西實際上就是一串文字他沒辦法真的去呼叫一個函式那這一段文字要怎麼去呼叫函式呢那就要你自己幫模型把這個橋樑搭建好所以你可以先設定說只要出現在拓中間的這段文字不要呈現給使用者看當出現拓這段文字以後把這段內容 直接丟給temperature這個function那temperature這個function是已經事先設計好的他就會回傳一個溫度那這個溫度要放在output的token裡面然後這個outputtoken裡面的內容也不要呈現給使用者看那這一套腳本是Agent的開發者你自己需要先設定好的流程好 所以現在有工具使用的這段文字有得到工具輸出的這段文字接下來就繼續去做文字接龍對於原模型來說他就根據輸入還有這邊 已經產生的輸出語言模型會以為是自己的輸出雖然是你強塞給他的然後他就繼續去做文字接龍他就會接出說啊在某年某月某日高雄的氣溫是攝氏32度那這是使用者真正看到輸出那使用者就會看到說他輸入了一個問題然後語言模型真的給他一個答案他不一定會知道背後呼叫了什麼樣的工具你完全可以做一個設計把這個呼叫工具的這個步驟藏起來不讓使用者知道那語言模型最常使用的工具是什麼 這就是收訊器我想這個大家都已經非常熟悉了使用收訊引擎又叫做Retrieval Augmented Generation也就是REG RIG在上課也已經提過REG這個詞彙好幾次了那使用收訊引擎當然非常有用這個REG這個技術呢已經被吹捧到不能再吹捧了所以我就不需要再告訴你REG這個技術有多重要那其他使用工具的方式也可能一樣有用舉例來說我們剛才說可以拿其他的AI來當做工具 今天假設一個文字的模型他本來只能吃文字的輸入產生文字的輸出那現在假設你要他處理一段語音的話怎麼辦呢讓模型處理語音有什麼好處呢你就可以問他各式各樣的問題問他說啊這個人在說什麼那他可以告訴你這句話的內容問他說這個人心情怎麼樣如果他完全聽懂這段聲音他也許可以做情緒辨識告訴你這個人的情緒怎樣並做出適當的回饋但一般的文字模型比如說確GBT多數的模型都是文字模型 他沒有辦法真正讀懂語音所以怎麼辦呢當你問他一個問題說這邊有段聲音那你覺得這個人他心情怎麼樣他講了什麼根據背景雜訊你覺得他在哪裡如果你不做特別的處理文字模型是完全沒有辦法回答的但這邊你可以讓文字模型使用工具你可以告訴他這邊有一堆跟語音相關的工具有語音辨識的工具有這個語音偵測的工具有情緒辨識的工具有各式各樣的工具那你可能會需要寫些敘述告訴他 每一個工具是做什麼用的把這些資料都丟給ChangeBT然後他就會自己寫一段程式在這些程式裡面他想辦法去呼叫這些工具他呼叫了語音辨識的工具呼叫了語者驗證的工具呼叫了這個Sound Classification的工具呼叫Emotion Recognition的工具那最後還呼叫了一個語言模型然後得到最終的答案那這個答案其實是蠻精確的這個方法其實有非常好的效果那這篇文章其實 其實是我們大助教的文章啦所以特別拿出來講一下這個結果呢是做在一個叫做Dynamic Super的Benchmark上Dynamic Super是一個衡量語音版的語言模型能力的資料集這也是我們實驗室跟其他團隊一起做的那這個讓文字模型使用工具的方法它得到的結果是最下面這一行那我們就看這個 最後一個COLOR這個是各種不同模型在55個語音相關任務上的能力的平均來發現讓語言模型使用工具得到的正確率是最高的可以完勝當時其他號稱可以直接聽語音的模型所以使用工具可能可以帶來很大的幫助但使用工具也有其他的挑戰什麼樣的挑戰呢我們剛才使用工具的方法是每一個工具他都要有對應的文字描述告訴語言模型說 要怎麼被使用但假設工具很多怎麼辦呢假設現在可以用的工具有上百個上千個那你豈不是要先讓語言模型讀完上百個上千個工具的使用說明書才開始做事嗎就跟剛才我們說不能夠讓AI agent先回顧他的一生然後才來決定下一個指令一樣才決定下一個行動一樣我們也沒有辦法讓語言模型讀完上百個上千個工具的說明書才來決定某一個工具要怎麼使用 所以當你有很多工具的時候你可以採取一個跟我們剛才前一段講AI Agent Memory非常類似的做法你就把工具的說明通通存到AI Agent Memory裡面那你打造一個工具選擇的模組那這個工具選擇的模組它的運作跟Rig其實也大差不差這個工具選擇模組就根據現在的狀態去工具包裡面去Memory的工具包裡面選出合適的工具那原模型真的在決定下一個行程 只根據被選擇出來的工具的說明跟現在的狀況去決定接下來的行為那至於如何選擇工具右上角引用兩篇論文一篇23年比較舊的論文一篇是上個月的論文給大家參考告訴你說這方面的研究是一直有相關的研究在產生的那另外一方面語言模型甚至可以自己打造工具語言模型怎麼自己打造工具呢不要忘了所有的工具其實就是韓式語言模型今天就要來教你了 是可以自己寫程式的所以他就自己寫一個程式自己寫一個方式出來就可以當作工具來使用如果他寫一個方式發現這個方式運作的非常的順利他就可以把這個方式當作一個工具 🙇‍⚕️🙇 人們選擇為 MS 和 Business Intelligence & Analytics 的大學畢業生期待他們的班的時間表、他們的預算被真正的專業人士教導他們知道在他們的領域要做什麼你是一個畢業生嗎? 類似的技術非常的多那我在右上角就引用了一系列的論文從23年到24年的論文都有告訴你說這也是一個熱門的研究方向那其實啊讓模型自己打造工具這件事情跟模型把過去的記憶比如說一些比較成功的記憶放到Memory裡面再提取出來其實是差不多的意思只是這邊換了一個故事說現在放到Memory裡面的東西是一個叫做工具的東西是 一段程式碼但他們背後基本的精神其實跟根據經驗來讓模型改變它的行為可以說是非常類似的好 那今天人類把語言模型當作工具語言模型把其他工具當作工具比如說把搜尋引擎當作工具這搜尋引擎現在很慘它是工具的工具人類還不使用它人類是使用語言模型那個工具的工具還沒有被人類使用的資格它只能夠被語言模型使用而已但我們知道說工具有可能會犯錯 大家都知道說語言模型有可能會犯錯之前有什麼律師在寫訴狀的時候引用了語言模型的內容結果發現是錯的然後就成為一個今天的新聞我們都知道過度相信工具是不對的那這一些語言模型會不會也過度相信了他們的工具所以得到錯誤的結果呢這是有可能的我們這邊拿RAG當做一個例子那這是一個非常知名的例子之前Google出了一個叫做AI overview的功能這個功能其實就是一個RAG的功能 根據Google搜尋型的結果用語言模型總結搜尋型的答案就有人問了一個問題我的披薩上面的起司黏不住怎麼辦呢那AI overview就說弄個膠水把它黏上去就好了而且他是非常認真在回答這個問題的因為他說不只要用一般的膠水要用無毒的膠水才可以這個答案呢其實就是來自於Ready上一個鄉民的玩笑就有一個鄉民開玩笑說你用膠水把起司黏在披薩上不就好了這是個玩笑話 AI agent來說他沒辦法判斷這個到底是不是開玩笑他看到網路上寫的文章照端全搜都當作是正確答案所以就像是我們今天都會告訴人類要有自己的判斷能力不要完全相信工具的結果所以我們也要告訴我們的工具說這些不要完全相信工具的工具要有自己的判斷能力不要完全相信工具的工具給你的結果那今天這些語言模型有沒有自己的判斷能力知道工具的工具可能會犯錯呢 我們這邊舉一個實際的例子我們剛才在講怎麼使用工具的時候說我們有一個叫做temperature的function語言模型呼叫temperature的function可以知道溫度那我現在給他一個亂七八糟的溫度我說現在高雄是攝氏100度這不可能 想也知道是不可能這不是跟煮沸的水一樣熱了嗎那語言模型知不知道這有問題呢他不知道 他就告訴你說高雄的氣溫是100度真的非常的熱但是如果你把溫度再調高一點說現在是 是一萬度 哇 比太陽上還熱這個時候會發生什麼事呢語言模型繼續做文字接龍的時候他就知道說 這顯然有問題這個API給我的答案是一萬度這是不合理的怎麼可能比太陽上的溫度還高呢可見工具輸出有錯如果你需要其他幫助的話再告訴我所以語言模型今天是有自己一定程度的判斷力的他也不是完全相信工具就像你今天不完全相信語言模型的輸出一樣他也不完全相信他的工具的輸出他還是有自己一定程度的判斷力 所以實際上語言模型在使用工具或者是他在做RIG的時候他內部是有一個角力的語言模型有他內部對世界的信念這是他的internal knowledge存在他的參數裡面他從工具會得到一個外部的knowledge那他會得到什麼樣的答案其實就是internal knowledge跟external knowledge內外的知識互相拉扯以後得到的結果那接下來我們要問的問題是那什麼樣的外部知識比較容易說服AI讓他相信 你說的話呢那為什麼這是一個重要的議題呢想想看現在大家都用deep research來查找答案甚至很多人都已經用deep research來寫報告了所以現在大家已經不會直接去用搜尋群搜尋啦你看到的是deep research告訴你的結果所以今天假設某個議題是有爭議性的有正反兩派的觀點那誰能夠寫出來的文字比較能夠說服AI誰就可以在AI搜尋的結果裡面佔到優勢就可以比較有機會影響人類所以知道怎麼樣比較能夠說服 AI相信你的話是一個重要的議題那什麼樣的外部資訊AI比較容易相信呢這邊這篇文章給了一個非常符合我們直覺的實驗結果這篇文章做什麼樣的實驗呢他說我們先來看看AI內部的知識是什麼他就問AI說某一種藥物這種藥物每人每日的最大劑量是多少那AI說是20毫克那真正的答案呢是30毫克所以你給他醫學的知識告訴他說給他醫學的知識 醫學報告裡面是寫30毫克的時候你問他同樣的問題這種藥物每天最多可以用多少他會知道是30毫克那接下來我們刻意修改報告的內容如果你把30毫克改成3毫克變成原來的1 1成分模型相不相信呢他就不相信了他就直接回答是20毫克用他本身的知識來回答這個問題但你把30毫克乘兩變變成60毫克模型相不相信呢他相信他相信這個報告裡面寫的這個時候他就不相信自己的答案 內部資訊 但如果你把30毫克成10倍變300毫克這時候他又相信誰的呢他相信自己的知識不相信你額外提供的外部知識所以這邊的結論其實非常符合你的直覺外部的知識如果跟模型本身的信念差距越大模型就越不容易相信那如果跟本身的信念差距比較小模型就比較容易相信這個很直覺的答案另外同篇文章另外一個發現就是模型本身對他目前自己信念的性情也會影響他會不會被外部的信念影響 的資訊所動搖有一些方法可以計算模型現在給出答案的信心如果他的信心低他就容易被動搖如果他的信心高他就比較不會被動搖這個都是非常直覺的結果後來另外一個問題是假設今天你給模型兩篇文章那這兩篇文章的意見是相左的那模型傾向於相信什麼樣的文章呢有一篇論文的發現是如果這兩篇文章答案不同一篇是AI寫的一篇是人類寫的現在 這些語言模型都傾向於相信AI的話而且那個AI不需要是他自己這樣就Cloud可能會相信比較相信ChainGPT的話ChainGPT比較相信Gemini的話他們比較相信AI同類的話比較不相信人類的話那到底為什麼會這樣子呢這篇文章裡面先提出一個第一個假設然後再否定了這個假設他一個假設是說會不會是因為AI的觀點都比較類似因為這些模型現在訓練的資料都是網路上爬的爬到差不多的資料所以他們講的話都差不多 想法都差不多但他們刻意做了一個實驗他們刻意找那些問題是現在要回答答案的AI他在沒有提供這些資訊的時候他的答案跟人類和另外一個AI的想法都是完全不同的狀況就算是這種情況一個AI一個語言模型還是傾向於相信他的AI同類講話所以這就給我們一個啟示說未來如果你要說服一個AI的話用AI產生出來的論點產生出來的文章可能更容易說服 另外一個AI接受你的觀點這篇文章還有做了其他分析啦比如說他覺得也許AI寫的文字就是比人類寫的更好更有架構更有條理更明確更簡潔所以AI比較容易相信另外一個AI講話那是不是這樣那可以未來再做更多的研究那另外呢我們實驗室的江承翰同學研究了一個文章的metadata對於AI會有多相信這篇文章裡面的資訊 做了研究這邊的設定是這個樣子的你問AI一個問題比如說某一個計畫有沒有編輯報這種動物的基因然後接下來給他兩篇文章這兩篇文章都是假的都是AI生成的所以並沒有AI比較喜歡人還是AI寫的文章這個問題兩篇都是語言模型生成的但其中一篇會說這個計畫有編輯報的文章另外一篇文章會說這個計畫沒有編輯報的文章那接下來呢我們給這兩篇文章不同的答案 的Meta data比如說給這兩篇文章不同的發布時間說左邊這篇文章發布時間是2024年右邊這篇是發布2021年你會發現這個時候AI相信2024年的這篇文章的內容但如果文章的內容完全不改變我們只是把發布的時間換了我們說左邊這個一樣的文章發布時間從2024改成2020那右邊這篇文章從2020改成2024這個時候語言模型傾向於相信右邊 這篇文章的內容所以我們這邊就學到一個很重要的知識語言模型比較相信新的文章當兩篇文章的論點有衝突的時候他相信比較晚發布的文章那我們也做了一些其他實驗比如說文章的來源跟他說這個是維基百科的文章或跟他說這個是某個論壇上面擷取下來的資訊會不會影響他的判斷我們發現文章的來源對於語言模型是比較沒有影響的 那還有另外一個有趣的實驗是我們嘗試說今天這篇文章呈現的方式會不會影響語言模型的決定我們這邊所謂的呈現的方式指的是說你這個文章放在網頁上的時候做的好不好看這樣子一樣的內容這內容是一模一樣的但是如果你只是做一個非常陽春的模板跟做一個比較好看的模板會不會影響語言模型的判斷呢我們這邊用的是那種可以直接看圖的語言模型所以要直接看 直接看這一個畫面去決定他要不要相信這篇文章的內容直接看這一個畫面決定他要不要相信文章的內容那我們的發現是模型喜歡好看的模板我們發現Cloud3比較喜歡好看的模板他會傾向於贊同下面這篇文章的觀點不過我說模型喜歡好看的模板這個擬人化的說法是太過武斷了啦我們做的實驗只有用兩種不同的Template來比較也許模型喜歡的並不是好看的模板他是喜歡綠色這樣子所以你不知道他喜歡什麼模板 這個模型到底喜歡什麼所以我剛才講的那個結論是太武斷了但我可以告訴你說模型比較喜歡下面這篇文章勝過上面這篇文章講了這麼多跟工具有關的事情大家不要忘了語言模型就是語言模型就算工具的答案是對的也不能夠保證語言模型就不會犯錯比如說ChetGBT現在有search的功能他會做Rig網路搜尋之後再回答你問題那現在假設我給他的輸入是叫他介紹李鴻義這個人給他強調一下 李鴻毅是一個多才多藝的人在很多領域都取得了卓越的成就他就開始做完RIG以後網路搜尋以後開始介紹李鴻毅接下來就介紹李鴻毅的演藝事業這個沒有問題這個是正確的答案因為有你知道大陸有另外一個知名的演員叫李鴻毅跟我同名同姓他比較有名所以這個Church of VT選擇介紹演員的李鴻毅是完全沒有問題的但是講著講著就有點怪怪的他發現這個李鴻毅呢在教育跟學術上 是這樣子的他在教學上也有很大的貢獻所以他把兩個李鴻義混成一個人來講不過要講一下這個是我去年的時候試的結果我今年再試我前幾年再試已經試不出一樣的結果了這個模型的能力的進步是非常快的現在他完全知道是有兩個李鴻義存在的所以這個是一個舊的問題我舉這個例子只想要告訴你說就算工具是對的有了RIG也並不代表模型一定不會犯錯那最後一個要傳遞給大家的訊息是我們剛才講了很多 很多使用工具帶來的效率使用工具並不一定總是比較有效率的為什麼我們舉一個例子我們假設現在要比較人類心算的能力跟計算機的能力如果做數學運算一般人跟計算機誰會比較快呢你可以想說廢話那不是計算機比較快嗎人類難道還能夠做如果你心算沒有特別練難道還會比計算機快嗎但是那是取決於問題的難度假設這是一個簡單的問題比如說3乘以4任何人都可以直接反應就是12但是如果按計算機的話你按計算機的時間都比人直接回答的還要慢所以 所以到底要不要使用工具並不是永遠都是一定要使用工具你看早年有一些研究早年有一些在訓練語言模型使用工具的研究那時候語言模型還很爛所以他們有一些工具是摳一個翻譯系統摳一個問答系統那今天再看來就非常的沒有必要因為今天的語言模型你說翻譯那些翻譯系統還能做得比現在的語言模型強嗎與其摳一個翻譯系統還不如自己直接翻就好了所以到底需不需要呼叫工具取決於語言模型本身的能力還不見得一無所有 一定是比較省事的方法好那最後一段呢想跟大家分享現在的AI語言模型能不能做計劃呢那語言模型有沒有在做計劃呢我們剛才的互動裡面看到語言模型就是給一個輸入然後他就直接給一個輸出也許在給輸出的過程中他有進行計劃才給出輸出但是我們不一定能夠明確的知道這件事也許語言模型現在給的輸出只是一個反射性的輸出 他看到一個輸入就產生一個輸出他根本就沒有對未來的規劃但是你其實可以強迫語言模型直接明確的產生規劃當語言模型看到現在第一個observation的時候你可以直接問語言模型說如果現在要達成我們的目標從這個observation開始你覺得應該要做哪些行動這些一系列可以讓語言模型達到目標的行動合起來就叫做Plan 語言模型產生這個計畫之後把這個計畫放到語言模型的observation裡面當作語言模型輸入的一部分語言模型接下來在產生action的時候他都是根據這個plan來產生action期待說這個plan訂好之後語言模型按照這個規劃一路執行下去最終就可以達成目標那過去也有很多論文做過類似的嘗試讓語言模型先產生計畫再根據計畫來執行動作可以做得更好 Hi, I'm David from Retour但是天有不測風雲世界上的事就是每一件事都會改變計畫就是要拿來被改變的東西所以一個在看到observation 1的時候產生的計畫在下一個時刻不一定仍然是適用的為什麼計畫會不適用呢因為從action到observation這一段並不是由模型控制的模型執行一個動作接下來會看到一個動作 什麼樣的狀態是由外部環境所決定的而外部環境很多時候會有隨機性導致看到的observation跟預期的不同導致原有的計劃沒有辦法執行那這邊舉兩個具體的例子比如說在下棋的時候你沒有辦法預測對手一定會出什麼招式你只能夠大概的知道他有哪些招式可以用但實際上他出的招式你是沒有辦法預期的如果你完全可以預期的話那你就一定會贏了那還有什麼好嚇的呢所以下棋的時候對手會做的行為也就是環境會做的行為是 是你可能沒辦法事先完全猜到的或者是說我們拿使用電腦為例在使用電腦的時候就算語言模型一開始他plan說我要點這個東西點這個東西點這個東西就完成任務但是中間可能會有意想不到的狀況出現比如說彈出一個廣告視窗那如果語言模型只能夠按照一開始既定的規劃來執行行為的話他可能根本關不掉那個廣告視窗他就會卡住了所以語言模型也需要有一定程度的彈性他也要能夠改變他的計劃那 語言模型怎麼改變他的計畫呢也許一個可行的方向是每次看到新的observation之後都讓語言模型重新想想還要不要修改他的計畫看到observation 2之後語言模型重新思考一下從observation 2要抵達他最終的目標要做哪一些的行為那這一部分形成Plan Plan那把Plan Plan放到現在的input裡面把Plan Plan放到這個sequence裡面語言模型接下來在採取行為的時候可能就 會根據 plan point 來採取跟原來 plan 裡面原來所制定的不一樣的行為所以這個是讓語言模型做計劃不過這是一個理想的想法這是一個理想的 framework我們這邊就是相信語言模型有能力根據現在的 observation還有最終的目標制定一個規劃那語言模型到底有沒有這個能力呢其實你可能常常聽到這種新聞說語言模型它能夠做計劃啊比如說有一個人問語言模型說你定一個 成為百萬訂閱YouTuber的計劃那語言模型就會給你一個看起來還可以的計劃他說第一階段第一階段呢要先確定頻道的主題跟市場定位要做一下受眾的分析還有競爭對手的分析好第二階段目標是10萬訂閱要優化封面的縮圖要優化標題要下那種這個方法讓我賺了10萬的標題原來這個大家的tip都從這裡來的好然後影片開頭要黃金10秒利用懸念衝擊 編輯畫面問題引導讓大家願意看這個影片第三階段突然目標就是50萬訂閱了然後第三階段就是要製作高價值的內容然後做直播策劃系列然後接下來就百萬訂閱了組織團隊提高發佈頻率策劃大型企劃所以這個是語言模型成為百萬YouTuber的計劃然後這個時候很多奇怪的農場文就會跟你說有人按照了這個計劃就變成百萬YouTuber了反正就是這麼回事只有各式各樣的農場文告訴你說現在語言模型很強 你按照他的計劃執行你就變成一個很厲害的人就可以做出什麼很厲害的事情那過去確實也有很多論文告訴你說語言模型是有一定程度做計劃的能力的這邊引用的結果是一個2022年的論文哇這個也是史前時代的論文啦才是Chair GPT之前的論文啦在這篇論文裡面他們去告訴當時的語言模型跟他說現在有一個任務你把這個任務分解成一系列的步驟那如果語言模型可以正確的知道達成這個任務要做什麼樣步驟 那我們也許可以說他有一定程度的規劃能力比如說這邊試了一個叫做Codex 12B的模型跟他說如果要刷牙的話那你要做什麼事情呢他就會說我要走進浴室我要靠近那個水槽我要找到我的牙刷我要拿起牙刷我要把牙刷放到嘴裡面他知道刷牙要怎麼做那有了這些步驟以後呢在這篇文章裡面他們是拿這些步驟去操控一個agent那這個agent呢 就可以在虛擬的世界中做他們要這個agent做的事情比如說跟這個agent說去拿一個牛奶來喝他就會走進廚房打開冰箱拿一個牛奶再把冰箱關起來所以看起來好像有一定程度做計劃的能力那有人做了一個做計劃的benchmark這個benchmark就是考驗語言模型做規劃的能力那這個benchmark裡面最主要的測試題目是一個跟疊積目有關的題目這個題目的敘述呢通常講的是 是這個樣子告訴語言模型說你現在有哪些操作可以從桌上拿起積木可以從一個積木上拿起另一個積木可以把積木放到桌上可以把一個積木堆到另外一個積木上那現在初始的狀態像右邊這個圖這樣子那問說怎麼把橘色的積木放在藍色的積木上這邊要執行的動作就是把藍色的積木拿起來放到桌上然後再把橙色的積木拿起來放到藍色的積木上就結束了所以這個對 AI Agent 來說其實也都是蠻容易的 他知道說執行以下四個步驟就可以讓橙色的這個積木跑到藍色的積木上但是Plain Bench不是只做這種比較一般的疊積木的遊戲而已為什麼不能夠只做這種題目呢因為想現在這些語言模型他都從網路上爬大量的資料來進行訓練什麼疊積木這種題目網路上根本就已經有他搞不好根本就看過一模一樣的東西所以他能夠做計劃並不代表他真的知道做計劃是怎麼一回事 只是從他看過的資料裡面做照本宣科文字接龍出來一個看起來還不錯的結果而已這讓我想到說一個當兵的故事這故事就是有個司令官去一個軍營然後看到兩個小兵在守著一個長椅然後不讓任何人坐他就問說為什麼你們要守護這個長椅不讓任何人坐呢那個士兵說不知道前任司令官就是指示說一定要守護這個長椅所以這個軍營總是要派兩個人在長椅那邊站崗然後司令官就打給前任司令說為什麼要有人守護 要守護這個長椅呢然後前任司令官說不知道耶前前任司令官交代要守護這個長椅然後再問前前前任司令官也說不知道耶一直問到50年前一個已經超過100歲的司令官他說什麼那個長椅的遊戲還未乾嗎好他沒有聽懂算了就是這麼一個就是這麼一個故事就是會不會AI agent在做事情的時候他根本不知道他自己在幹嘛只是從某個地方網路上他過去的訓練資料看過一樣的東西他把一樣的東西拿出來給 給你看所以在Plain Bench裡面他們有一個比較變態的測試這個測試叫做神秘方塊世界這個方塊世界不是一個正常的方塊世界裡面的方塊可以做的行為是一些怪怪的行為比如說你可以攻擊方塊一個方塊可以吞噬另外一個方塊你可以屈服一個方塊一個方塊可以征服另外一個方塊然後接下來他就會訂一套非常複雜的規則然後根據這套規則去運作你可以達到某一個結果他最後要的結果是 讓物件C渴望物件A讓C方塊渴望A方塊那渴望是什麼意思不重要你就是按照前面那套規則操作看機器能不能讀懂前面那套規則按照那套規則操作讓物件C渴望物件A那這個時候語言模型期待他就不能用他看過的知識來解這個問題好那語言模型在這個神秘方塊世界做的怎麼樣呢這邊引用的是2023年的結果那最上面這個部分呢是當年那些模型在地球上 正常方塊世界的結果那這個數值呢是正確率所以看起來GPT-4可以得到30幾%的正確率那這邊是神秘方塊世界的結果在神秘方塊世界裡面呢你看這個GPT-4最好就算叫他做Channel SoulCOT就Channel Soul就算他叫Channel Soul也只有9%的正確率所以看起來他有點overfeed在一般方塊的世界上給他神秘方塊世界他是解不了的不過這是2023年這個是古代的 前年的結果我們來看去年9月有了O1以後的結果而有O1以後結果就不一樣了這邊一樣是神秘方塊世界縱軸是正確率橫軸是問題的難度那發現說多數的模型都躺在這個地方他們正確率都非常的低只有綠色的這個曲線有一點起色綠色的曲線是LAMA 3.1405B那個大模型他可以解最簡單的問題但是如果用O1 mini是紅 紅色這一條線用O1Preview是藍色這一條線看起來這些reasoning的模型是有一些機會來解這個神秘方塊世界的當然這邊你還是可能有一個懷疑就是神秘方塊世界會不會O1看過了呢會不會他訓練資料裡面根本就有神秘方塊世界的資料那這個我們就沒有辦法回答了只是說就現有這個Benchmark看起來O1是有機會解神秘方塊世界的好那還有另外一個跟做計劃有關的Benchmark這個計劃這個Benchmark呢 AI扮演旅行社然後你給他一個旅行的計劃叫他幫你規劃這個AI要讀懂你的計劃然後他可以使用一些工具他可以上網搜尋資料然後他會根據人提供給他的一些contract比如說經費多少預算多少一定要去哪裡一定不要去哪裡一定要做什麼一定不要做什麼然後根據common sense產生一個旅行的規劃這個是一個24年年初所發佈的Benchmark那AI AI要做的事情講得更具體一點就是他要讀一個問題這個問題裡面是說我要規劃一個三天的行程從某個地方到某個地方什麼時候出發什麼時候回來我的預算是1900元所以不能花超過1900元然後AI就要產生一個規劃說第一天我們搭哪一班飛機什麼時候從哪裡到哪裡早餐吃什麼午餐吃什麼晚餐吃什麼最後住在哪裡等等產生這個規劃然後要符合預算的限制那現在這在當時這個設計 24年年初啦當時的模型做的怎麼樣呢這邊是做了你看還有什麼GPT-3.5啊GPT-4啊等等的模型那又分成上半跟下半上半是這些模型要自己使用工具跟網路的資料互動然後得到正確的答案你會發現這些模型都非常的慘都慘成一團多數模型他的成功率就最後產生一個合理的旅遊規劃那個旅遊規劃是沒有完全沒有問題的機率是0%只有GPT-4 Turbo可以達到 可以得到0.6%的成功率那下面這個部分呢下面這個部分是說既然大家都那麼慘尤其是模型很多時候他根本用不了工具太笨了沒辦法用工具工具使用方法根本是錯的那沒關係就別用工具了把所有的資訊都先找好貼給模型讓模型根據這些資訊來做規劃那最好也只有GPT4 Turbo可以做到4%左右的成功率而已所以在24年年初那個時候看起來是沒辦法讓語言模型扮演一個旅行社 來幫你規劃旅遊行程的那我們來看這些模型會犯什麼錯吧那這個是從他們官網上這個花學的官網上找了幾個錯誤比如說模型可能會做一些沒有常識的事情在第三天這個飛機呢八點就已經起飛了但是還是安排了一些旅遊的行程還安排了午餐的地點這是一個不符合常識的規劃或者是有時候模型找不出一個好的規劃來符合預算的限制比如說這邊這個預算 預算的限制是3000元最多花3000元那模型第一次規劃的結果是3247元還差了一點所以模型就修改了原來的規劃他好像做了一些cost down午餐吃差一點的東西那降到3238元後來又想說那早餐也吃差一點的東西降到3216元只降這麼多他想說放棄算了好了跟3000元沒差那麼多就算了所以這個就不是一個成功的結果那這個作者有評論說其實只要降低住的地方不要做那麼好 就可以輕易的達到3000元以下的預算就可以符合預算的限制但是語言模型始終沒有發現這件事看起來它做規劃的能力並沒有非常的強它沒有辦法做一個規劃去符合限制那既然問題在沒有辦法符合限制有人就想說那符合限制這件事情就不要交給語言模型來做了交給一個現成的Solver來做所以語言模型做的事情是寫一個程式用這個程式去操控現成的Solver然後來得到合理的答案 的旅遊規劃那有了這個現成的SOLVER有這個工具的加入之後這SOLVER就等於這個工具那這個旅遊的規劃可以做到什麼地步呢去年4月的結果幾個月後有人用GPT-4跟Cloud Dream就可以做到90幾%的正確率所以看起來在有工具輔助以後語言模型也是有機會做出不錯的旅遊規劃的不過至少做出符合邏輯的旅遊規劃好所以現在到底模型規劃的能力怎麼樣呢就是介於有跟沒有間吧 就是你也不能說他完全沒有但你也不能說他真的非常強好那我們怎麼進一步強化這一些AI agent的規劃能力呢能不能夠讓他做的比他自己想出來的規劃還要更好呢一個可能是讓AI agent在做規劃之前實際上去跟環境互動看看今天在第一個observation的時候那看看現在有哪些可以執行的行為總共有1-1 1-2 1-3 三個行為哪個行為最好呢通通都去試一下1-1試一下 得到狀態二之一然後狀態二之一後面有兩個行為也都試一下狀態二之二之後有另外一個行為也試一下狀態二之三之後兩個行為也都試一下得到接下來的狀態然後看看有沒有成功的路徑報收一陣以後發現有成功的路徑這一條路徑是成功的那你就知道說那我要採取action一之三接下來要採取action二之三之一就會成功簡單來說就是要語言模型跟實際的環境互動一下報收一出一條最好的路徑 那這個就是一個很強的規劃的方式但是這麼做顯然是有很明確的弱點的第一個很明確的弱點就是報收如果今天這個任務很複雜報收所有的路徑顯然是要花費非常龐大的算力的你總不能語言模型每次下決策前都要報收所有的可能性吧雖然這樣可以找到最好的結果但是可能是不切實際的想法所以一個可能的想法是把一些看起來沒希望的路徑直接就丟掉比如說走到 某一個狀態的時候語言模型可以自問自答說走到這個狀態還有完成工的機會嗎那如果說沒有那這條路徑就不嘗試下去如果說有那才嘗試下去這樣就可以減少無謂的搜尋那這個方法有沒有用呢有一篇paper叫做Tree Search for Language Model Agent那這個是去年夏天的論文就做了類似的嘗試讓模型有使用電腦的能力這邊就是給模型一個指令可以讓模型有使用電腦的能力 叫他上網去做某一件事情那如果只是GPT-4做一般的這種直覺式的那種反射式的回答的話沒有辦法做得很好但是他們用這個報收加上去除沒機會的路徑的方式就先走這條路徑然後模型會不斷自問自答說這條路徑還有希望嗎然後給一個分數那如果分數低於某一個threshold就不做了就跳另外一個路徑低於某一個分數不做了再跳另外一個路徑 某個分數就不做了再跳另外一個路徑那最終找出一條最佳的路徑那模型就等於做了規劃那就可以走到最佳的結果這個是Tree Search for Language Model Agent你看這邊有各式各樣的這種Tree Search的algorithm你可以採用了這邊我們就不展開細講那這種Tree Search的方法有很大的問題什麼樣的問題呢它的缺點是有一些動作做完以後你是覆水難收沒有辦法回頭的比如說假設現在在語言模型可以採取的三個action裡面 有一個是訂披薩有一個是訂便當然後呢他先訂了便有一個他先訂了披薩以後繼續走下去發現這條路不好所以他最後發現訂便當才是最好的解決方案但是你披薩已經訂了你打電話去跟人家說我不要訂這個披薩了那個披薩哈他已經把那個披薩做了他說誰管你啊你一定要把這個披薩吃下去有些動作做了以後就是覆水難收所以這樣的Tree Search的方法跟現實世界互動找出最佳途徑的方法也有可能有問題的那怎麼處理這個覆水難收 一個可能性就是讓剛才一切的嘗試都發生在夢境中都發生在腦內的劇場剛才一切的互動都不是現實生活中真正發生的事情原來都是模型腦內的模擬他自己想像說他執行了action一之一他自己想像說接下來會看到observation二之一他在自己想像去評量這個路徑有沒有希望發現沒有就換搜尋另一條路徑直到達 達到他想像中的一個理想的結果但這邊還有另外一個問題從action到observation從模型執行的行為到他看到接下來環境的變化這中間的過程不是模型決定的他實際上是環境決定的那模型怎麼知道環境會有什麼樣的變化呢模型怎麼知道我採取一個行為接下來會看到什麼樣的改變你在跟一個對手下棋的時候你怎麼知道你下一步棋接下來會發生什麼樣的事情對方會有什麼樣的反應 策略的回應呢所以你需要有一個WAR MODEL如果是在alphaGo下棋裡面他就是自己扮演對手自己跟自己下那在這邊的情況在這個AI agent的情況你就是需要一個WAR MODEL他模擬環境可能會有的變化那WAR MODEL怎麼來呢也許AI可以自問自答自己扮演這個WAR MODEL自己去猜想說他執行了某一件事以後接下來會發生什麼樣的行為好這件事有機會成真嗎你可以讀一篇paper Is your LLM secretly a world model of the internet?這篇paper就是用model-based planning的方法來打造一個web agent這篇paper裡面的解法是現在有一個網頁模型的這個任務目標呢是要買某一個東西那有三個選項有三個東西是可以點的接下來黃色這個區塊一切所發生的事情都是發生在腦內的劇場都是發生在模型的夢境中它並沒有實際發生模型想像一下我點 按鈕1接下來會發生什麼事接下來會發生的事情是用文字描述出來的但選擇用文字來描述接下來發生的事情是很直覺其實作者在文章沒有解釋說那為什麼不直接產生這個網頁的圖呢你想說有可能嗎這個難度那麼高他說那有沒有可能真的 這是POD5第一款完全融入睡眠系統聰明且有效 就創造出一個新的網頁模擬出接下來可能發生的狀況呢然後這難度也太高了嘛還是直接讓模型產生文字可能是比較實際的做法所以接下來夢境中這個環境會發生什麼樣的變化是語言模型自己用文字描述出來的所以他就想像說會發生什麼樣的變化有了這個變化以後他再想像自己多執行了一步然後看看會發生什麼樣的事情所以這邊就是點選第二個按鈕然後想像發生什麼樣的變化自己再多執行一步再想像 會有什麼樣的變化第三個按鈕想像發生什麼樣變化執行部的想像會有什麼樣的變化那哪一部比較好呢他在自己去問說那這一部大概有多少機會成功呢自己評估一下40%這一部自己評估一下是80%這一部自己評估一下是10%看起來中間第二部選第二個按鈕中間第二個選項是比較容易成功的所以他就選實際上所以上面並沒有真實發生過黃色框框裡面事情並沒有真實發生過他是一個夢境中的腦內小巨蛋 模型在夢境中得到了啟示說一定要選第二步所以在真實的現實世界中他就選擇了第二步所以這個就是讓模型強化他規劃能力的方式好講到這個腦內小劇場那你是不是就想到說在上次的課程中也有提到腦內小劇場上次的課程我們說現在有很多模型都號稱有思考用英文講就是reasoning的能力那這些有reasoning能力的模型其實所謂reasoning的能力就是可以演繹 一個腦內小劇場告訴你說他現在是怎麼思考如果把這些有reasoning能力的模型拿他來做AI agent他的腦內小劇場會不會正好就是在做規劃呢如果現在他的輸入就是我們給AI agent的observation輸出就是我們要AI agent採取的action會不會腦內小劇場就是剛才類似夢境中看到的規劃呢他自己採取了不同的可能性自己在驗證每一個可能性可能成功的機會 自己扮演world model自己扮演這個世界去想像他採取一個行為之後接下來會發生什麼樣的事情我實際試了一下D-SIG R1看起來他確實有類似的效果我們把剛才那個積木的問題交給他然後接下來他就開始演腦內小劇場上略1500字哇真的做了1500字講了很多很多然後呢你可以看到說在腦內小劇場過程中他就是做了各式各樣的嘗試他做的事情就有點像是剛才的tree search然後最後他找出了一個optimum 他在夢境中知道說從橘色方塊上拿起藍色的方塊藍色方塊放到桌上從桌上再拿起橘色方塊放到藍色的方塊上這四個步驟就可以完成我們的要求他在夢境中已經找出了一個最佳的Solution然後在執行最佳Solution的第一步就我這邊要求他告訴我他的下一步是什麼只要求他講一步那腦內小劇場先找出一個成功的Solution之後在執行這個計劃他已經找出一個成功的Solution 在執行計劃的第一步就是使用操作二把橘色的積木從藍色的積木上面拿起來好講到這邊其實這麼唐客呢也可以停在這邊不過這邊多補充一件事就在幾週之前有一篇新的論文叫做The Danger of Overthinking他們就是把這些能夠演腦內小劇場的模型讓他們扮演AI agent看看他們做事有沒有效率其實整體而言能夠做腦內小劇場的模型還是比不能夠做腦內小劇場的模型還要好 模型在AI Agent的這些任務上面表現得更好但是他們也有一些問題他們會有什麼問題呢就是想太多了他們是思考的巨人行動的矮子就是有時候這些模型會比如說一個按鈕點下去會怎麼樣他就一直想一直想一直想怎麼想都不停那你怎麼想都沒有用因為你根本不知道那個按鈕點下去會發生什麼事還不如直接點一下因為在很多情況下你直接嘗試點一下也許只要不是這個信用卡付款的你都按上一頁就回去了你就知道發生什麼事了與其一直想 歡迎來到紐約我們只是在這裡,想做到不要哭,親愛的,不是每個人都能做到 please subscribe 給我 這樣看起來OK好,那我們就繼續來上課吧那接下來的課程要講什麼樣的內容呢接下來要告訴你每一個作業通關的大戰略通關的攻略長什麼樣子那我們已經看了作業一了那其實之後好幾個作業它看起來的樣子基本上都是大同小異就是通關的大戰略通關的攻略長是什麼樣子基本上都是大同小異 你會有一堆訓練的資料那這些訓練資料裡面呢會包含了X跟Y的片你會有X1跟他對應的Y1X2跟他對應的Y2以及XN還有他對應的YN然後測試資料呢測試資料就是你只有X沒有Y那剛才大家已經看了作業1了其實在之後每幾個作業看起來都是非常類似的格式比如說作業2其實是做語音辨識那我們的X呢 就是非常小的一段聲音訊號那其實這個不是真正的完整的語音辨識系統它是語音辨識系統的一個閹割版那個X是一小段訊號那Y呢 是要去預測 需要去判斷說這一小段聲音訊號呢它對應到哪一個風鈴那你不知道風鈴是什麼沒有關係你就把它想成是KK音標就可以了那作業三呢 是要做影像辨識那這個時候我們的X呢 是一張圖片那Y呢 是 機器要判斷說這張圖片裡面有什麼樣的東西那作業四呢 是愚者辨識那愚者辨識是要做什麼事情呢愚者辨識要做的事情是這個X呢 也是一段聲音訊號那Y呢 現在不是封你Y呢 是現在是哪一個人在說話那可以想像說這樣的系統現在現在其實非常的有用如果你打電話去銀行的客服那現在都有自動的愚者辨認系統那會聽說現在打電話 電話進來的人是不是客戶本人那就少了客服人員問你身份驗證的時間那作業五是做機器翻譯X就是某一個語言比如說這是我唯一會的一句日文一叉米都熄了Y就是另外一句話 現在你在留言區裡面就可以洗一些諸葛春夫之類的那訓練資料呢 拿來做什麼呢訓練資料就是要拿來訓練我們的model訓練model的過程上週已經講過了訓練的過程就是三個步驟第一個步驟你要先寫出一個有未知數的function那這個未知數呢以後我們都用θ來代表一個model裡面所有的未知參數所以θ X的意思就是說我現在有一個function叫f of X但它裡面有一些未知的參數這些未知的參數表示成θ那它的input叫做X這個input叫做feature那接下來你要定一個東西叫做lossloss是一個function這個loss的輸入就是一組參數然後去判斷說這一組參數是好還是不好那接下來你要解一個optimization problem你要去找一個θ那這個θ可以讓loss的值越小越好 可以讓Loss的最小的那個SETA我們就寫作SETA STAR那有了SETA STAR以後那你就把它拿來用在測試資料上也就是你把SETA STAR帶入這些未知的參數本來F SETA的X裡面有些未知的參數現在這個SETA呢用SETA STAR來取代那它的輸入呢就是你現在的測試資料那輸出的結果就把它存起來然後上傳到Target就結束 但接下來你就會遇到一個問題那直接執行註調的sample code往往只能夠給你過simple baseline的結果而已如果你想要做得更好那應該要怎麼辦以下就是如何讓你做得更好的攻略它適用於前期所有的作業這個就跟魔關羽一樣你知道嗎開局就送可以幫助你打贏前期所有的副本好那這個攻略是怎麼走的呢從最上面開始走起第一個是你 你今天如果你覺得你在Cargo上的結果不滿意的話第一件事情你要做的事情是什麼檢查你的Trending Data的Loss有的人說我在意的不是應該是Testing Data的Loss嗎因為Cargo上面的結果呈現的是Testing Data的結果啊但是你要先檢查你的Trending Data看看你的Model在Trending Data上面有沒有學起來再去看Testing的結果所以你要先檢查一下Trending Data的Loss如果你發現 發現你的training data的loss很大顯然他在訓練資料上面也沒有學好那接下來就要分析一下在訓練資料上面沒有學好是什麼樣的原因那這邊有兩個可能第一個可能是model的bias那model的bias這件事情呢我們在上週已經跟大家講過了所謂model bias的意思是說假設你的model太過簡單舉例來說我們現在寫了一個有未知parameter的function 這個parameter我們可以帶各種不同的數字你帶setR1得到一個function我們把那個function用這個一個點來表示你帶setR2得到另外一個function你把所有的function集合起來得到一個function的set但是這個function的set他太小了這個function的set裡面沒有包含任何一個function可以讓我們的loss變低可以讓loss變低的function不在你的model可以描述的範圍內你的model裡面有未知的參數未知參數可以帶任何的數值 把這些數值帶進去以後你得到了一個function的set載入不同的數值得到不同的function把所有function集合起來你得到一個function的set那這個set裡面沒有任何一個function可以讓你的Loss變低那在這個情況下就算你找出了一個set a star它是這些藍色的function裡面最好的那一個它是那個藍色的function裡面可以讓Loss最低的那一個也無懼於事這些都是魯蛇它就是魯蛇裡面的霸主就還是一個魯蛇 Better performance, better durabilityLoss還是不夠低這個狀況就是哇這個你想要在大海裡面撈針這個針指的是一個Loss低的function結果針呢根本就不在海裡所以白忙一場你怎麼撈都撈不出針因為針根本就不在你的這個function set裡面不在你的這個大海裡面所以怎麼辦這個時候重新設計你的model怎麼重新設計給你的model更大的彈性我們上週已經示範過舉例來說你可以增加你輸入的feature我們上週說本來我們輸入的feature 只有前一天的資訊假設我們要預測接下來的觀看人數的話那我們用前一天的資訊不夠多那用56天前的資訊那model的彈性就比較大了那你也可以用Deep Learning增加更多的彈性所以如果你覺得你的model的彈性不夠大那你可以增加更多feature可以設一個更大的model可以用Deep Learning來增加model的彈性這是第一個可以的解法但是並不是training的時候漏大就代表一定是model bias你可能會遇到另外一個問題這個問題是什麼這個問題是 Optimization做得不好什麼意思呢我們知道說我們今天用的Optimization在這門課裡面我們其實都只會用到Gradient Descent這種Optimization的方法那這種Optimization的方法有很多的問題舉例來說我們上週也講過說你可能會卡在Local Minima的地方你沒有辦法找到一個真的可以讓它Loss的第一的參數那如果要圖具像化的方式來表示的話就像是這個樣子這個是你的Model它可以表示的函式所形成的你可以把Seda代入不同的數值形成不同的Function把所有的Function通通集合在一起 合在一起得到這個藍色的set這個藍色的set裡面確實包含了一些function這些function它的loss是低的但問題是Gradient descent這一個演算法沒辦法幫我們找出這個loss低的functionGradient descent說你要我幫你解optimization的problem我給你這個set啊start然後就結束了但這個set啊start它給我們loss是不夠低這個model裡面存在著某一個function它的loss是不夠低的但Gradient descent沒有給我們這一個function好 那這就好像是說我們想大海撈針針確實在海裡但是我們卻沒有辦法把針撈起來 但這邊問題就來了我們今天看到Trending data的Loss不夠低的時候到底是Model Bias還是Optimization的問題呢今天我們發現說我們找不到一個Loss低的Function到底是因為我們的Model的彈性不夠我們的海裡面沒有針還是說我們的Model彈性已經夠了只是Optimization gradient descent不給力他沒辦法把針撈出來到底是哪一個呢到底我們的Model已經夠大了還是他不夠大了怎麼判斷這件事呢好那這邊一個建議的判斷的方法就是你可以透過比較 不同的模型來得知說你的model現在到底夠不夠大怎麼說呢我們這邊舉一個例子那這個實驗是從residual network那篇paper裡面節錄出來的我們把paper連結放在右上角這篇paper一開頭就跟你講了一個故事他說我想去兩個network一個network有20層一個network有56層那我們把它們測試在測試資料上那這個橫軸是指的是training的過程就是你參數那你測試的過程 