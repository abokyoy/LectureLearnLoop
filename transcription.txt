# 01. MCP 的核心概念与基础功能

- **MCP 的全称**：Model Context Protocol（简称 MCP），由Anthropic 开发
- **核心概念**：
  - **数据流转**：MCP 是一个分布式协议，允许不同硬件设备之间进行信息传递和处理
  - **基本用法**：
    - 提供跨设备交互的基础框架
    - 支持多用户环境下的协作与协同工作

## 进阶挑战（挑战 MCP 的进阶能力）

### 从零构建 MCP Server

- **基本功能**：通过配置 Python 库，实现对本地或异构网络的 MCP 环境搭建
- **挑战点**：
  - 创建完整的 MCP Server 需要熟悉文件结构和配置方式
  - 实现异构环境下的数据交互需要理解不同设备的通信协议

### 掌握进阶功能

- **从零构建 MCP Server 的具体实现**：
  - 使用 VS Code 编程插件“MCP Host”
  - 加载 OpenRouter 广室（OpenRouter）作为 MCP 齿轮
  - 运行模型配置，选择最优质的 MCP 模型进行训练

## 尝试解决实际应用问题

- **问题：如何配置天气 MCP？**
  - 新增 MCP 函数，将 MAPI 调用到 Google 天气 API 中
  - 确保 API 设置符合 MCP 标准
  - 实现异构环境的天气交互，确保数据一致性

## 小结与展望

通过本次整理，我们对 MCP 的核心概念有了清晰的理解，并认识到其在跨设备协作中的重要性。未来，随着技术的不断进步，MCP 可能会扩展到更多场景的应用中，例如智能硬件、物联网设备等。或者是Python启动的只不过在使用的过程中可能会连网当然它也可能不连网纯本地使用也是可以的不管是连不连网它都可以叫做MCP Server所以我觉得MCP Server的这个名字里面带Server这个词是有一定的误导性的你不要觉得这个玩意儿很高端 很玄妙它其实本质上就是一个程序跟你手机上面的应用没有什么太大的区别不管是MCP Server还是手机应用它们都内置了一些功能模块来解决你的问题我用iPhone上面的时钟进行举例它内置了四个功能模块分别是世界时钟 闹钟 钟表 计时器分别可以给我们解决四个场景的诉求而刚才Client想要给我们安装的这个Open Weather Map


略有不同的问题时它又会给出一个错的离谱的答案让你觉得它根本什么都不懂只是一个更高级大家好 这里是最佳拍档我是大飞经常接触大模型的朋友一定会有过这样的经历当你向AI提出一个复杂的问题时它不仅给出了答案还给出了详细的一步一步的看起来逻辑严密的解题过程那一瞬间你是不是感觉屏幕对面的不再是一堆冰冷的代码而是一个真正能够推理的智能体但是有的时候你换一个同样复杂但是略有不同的问题时它又会给出一个错的离谱的答案让你觉得它根本什么都不懂只是一个更高级的复读机这种体验上的巨大反差正是当前AI领域最核心的谜题之一大圆模型展现出的推理能力究竟是一种真正的智能涌现还是一种基于海量数据训练出来的更高级的模式匹配他是在進行邏輯推導還是在模仿他在網上看過的無數解題步驟關於這個問題學術界和工業界爭論不休但是爭論的意義遠不如我們去搞清楚這種所謂的推理能力到底是怎麼來的以及我們如何才能穩定的可靠的駕馭它踏入 Wix 工作室,從一個平台開始處理每個部份的建築。將你的項目圖畫出來,用 AI 能力的視覺地圖圖畫,能夠在秒數,而不是日子內製造視覺結構和線條。當你的客戶登機後,進入設計,自由地創作,甚至可以自行調整最小的元素。使用視覺方式,以積極地延伸品牌到每個頁面,並觀察它適應地穿越磨擦點。Now, Scale.而要解答這個問題有一個人的名字我們就繞不開他就是來自於谷歌DingMine的丹妮周他和他的團隊可以說是奠定了我們今天理解和使用大圓模型推理能力的基石他們開創性地提出了像思維鏈提示和自洽性這類的關鍵技術並且深度參與了谷歌靜奶模型推理能力的構建最近丹妮周在斯坦福大學做了一場演講系統性地梳理了從他創立谷歌大腦的推理團隊開始到今天我們所看到的強大的AI這條技術路線是如何一步一步演進的這場講座的信息量巨大它不僅揭示了AI推理能力的本質更是對過去幾年中所有相關技術的一次趣味所以今天這期視頻我們就將以丹妮周的這場講座為藍本帶著大家從最基礎的概念出發層層遞進徹底搞懂大圓模型思考秘密我們會看到那些看似神奇的技術並且會看到這些看似神奇的技術是如何一步一步的發展我們會看到這些看似神奇的技術是如何一步一步的發展我們會看到這些看似神奇的技術是如何一步一步的發展往往遵循著一些極其簡單而深刻的原理相信看完這些視頻你再看待大圓模型的時候會有一個全新的更加清晰的視角在深入探討之前我們必須先明確一件事情當我們討論大圓模型的推理時我們到底在談論什麼單機周一上來就拋出了一個非常清晰而且可操作的定義這個定義也成為了整個領域的共識他說關於模型到底會不會推理的哲學辯論他從不參加因為沒有一個明確的定義大家都是在自說自話而在他的團隊裡推理是有一個非常具體的含義的那就是在模型的輸入也就是你的問題和最終輸出也就是答案之間生成的所有中間步驟這個定義非常關鍵他把一個模糊的哲學層面的思考概念轉化成了一個具體的工程上可以實現和優化的目標為了讓我們更好的理解這個問題丹妮周设计了一个非常巧妙的任务叫做末尾字母拼接这个任务听起来很简单比如说我问模型请拼接artificial intelligence这两个单词的末尾字母如果我们直接让模型输出答案它可能会凭借着语言的惯性直接猜一个答案比如说LE这时候它只是在预测下一个最可能的字符而不是在执行一个多步骤的逻辑操作但是如果我们引导模型先生成中间步骤它的输出就会变成这样artificial的最后一个字母是Lintelligence的最后一个字母是E将L和E拼接起来得到LE这就是丹妮周所定义的推理她把一个复杂的任务分解成了一系列简单的可执行的子任务最终导出了正确的答案你可能会觉得这不就是我们人类解决问题的方式吗先思考再作答但是丹妮周提醒我们作为研究者你应该先想清楚你应该先想清楚你应该先想清楚你应该先想清楚你应该先想清楚你应该先想清楚你应该先想清楚你应该先想清楚你应该先想清楚你应该先想清楚你应该先想清楚必须时刻记住大猿模型不是人类它们只是概率模型把它们拟人化虽然有助于我们理解但是也很容易让我们走入歧途一个有趣的故事是丹琪周最初尝试的是首字母拼接但是他发现当时所有的模型都能够做得很好为什么呢因为互联网上有大量的缩写词模型在预训练阶段已经背会了如何拼接首字母于是他换成了末尾字母拼接结果当时所有的模型都失败了这恰恰说明模型并没有真正理解拼接这个动作而只是记住了某种常见的模式那么为什么要如此执着于生成这些中间步骤呢仅仅是为了模仿人类吗当然不是这背后有着非常坚实的理论依据丹琪周提到了他们和斯坦福大学教授滕上华团队合作的一项理论研究这个研究得出了一个非常强大的结论对于任何一个可以被大小都可以被拼接的模型来说是一个非常强大的证据这就是为什么丹琪周在2014年在中国的一个大学研究院他发现了一个模型它是一个非常强大的模型为T的布尔电路解决的问题一个常数大小的transformer模型可以通过生成OT长度的中间步骤来解决它这句话听起来有点过于技术我们来把它翻译一下布尔电路可以被看作是执行逻辑运算的基本单元任何复杂的计算任务比如说运行一个大型的软件本质上都可以被分解成一个巨大规模的布尔电路这里的大小T就代表了问题的计算复杂度这个理论告诉我们哪怕是一个相对简单的transformer模型只要你允许它生成也就是中间步骤它就有潜力解决几乎任何可计算的问题反过来说如果我们强迫模型直接蹦出最终答案就相当于要求这个模型的网络深度本身要能够模拟整个复杂的计算过程这要么需要一个巨大到不切实际的深度要么就根本无法解决问题所以让模型的计算方式是一种很简单的计算方式生成中間步驟不是一個可有可無的選項而是在計算原理上解鎖模型解決複雜問題能力的一把金鑰匙這徹底改變了我們訓練和使用大圓模型的範式從單純的追求答案轉向追求過程好 既然我們知道了推理過程如此的重要那麼下一個問題自然就是如何讓模型來生成這個過程呢這裡丹妮周提出了一個顛覆了當時很多人認知的觀點當時普遍認為一個普通的只經過預訓練的大圓模型是不會推理的你必須通過像思維鏈提示這樣的高級技巧或者進行專門的微調才能夠教會他們推理但是丹妮周說這個觀點是錯的而且大錯特錯她認為預訓練模型早就已經準備好進行推理了我們所需要做的僅僅是改變解碼的過程這又是一個非常深刻的洞察為了證明這一點她舉了一個經典的例子的数学硬题我有三个苹果我的爸爸比我多两个苹果我们总共有多少个苹果如果你把这个问题直接输入给一个原始的预训的模型比如说早期的GPT-3或者是拉马然后使用默认的贪婪解码方式会发生什么呢贪婪解码的意思是模型在生成每一个词的时候总是会选择当前概率最高的那一个在这种模式下模型很可能会直接输出一个看似合理但却是错误的答案比如说5个苹果因为他看到了3个和多两个就直接联想到了5这是模型的一种直觉反应接著丹妮周向我們展示了這些隱藏的候選答案比如說候選二可能以我字開頭模型會聲稱我有三個蘋果我爸爸比我多兩個所以他有五個蘋果三加五等於八所以我們總共有八個蘋果這是一個完美的推理鏈答案也正確候選三可能以我們開頭模型會聲稱我們總共有八個蘋果雖然沒有過程但是答案也對了候選四可能以你字開頭模型會聲稱你有三個蘋果你爸爸有三加二等於五個蘋果你們總共有三加五等於八個蘋果這同樣是一個清晰的推理鏈看到了嗎其實一直都存在於模型的輸出空間裏他們就像是隱藏在主幹道旁邊的小路默認的貪婪解碼因為只看到了眼前最寬的路所以錯過了他們這個發現被稱為思維鏈解碼它告訴我們推理能力不是被注入到模型裏面的而是模型在選擇的時間裏面海量文本中蘊含的邏輯關係之後自然湧現出來的於是我們的任務從交會模型推理變成了如何引導模型把他已經知道的東西以正確的形式表達出來那麼問題就變成了在這麼多候選的輸出裏我們怎麽知道哪一個是最好的呢一個簡單的想法是看長度在有思考過程的回答通常會更長但是丹妮周的團隊發現一個更可靠的指標那就是答案自信度他們觀察到一個驚人的現象對於那些包含了正確思維鏈的回答模型在生成最終答案的那個詞比如說數字8的時候其內部的自信度也就是概率會異常的高在蘋果的例子裏模型預測8這個詞的概率可能會高達98%這是一個非常強的型號因為對於一個擁有巨大詞彙表的模型來說通常每個詞的概率都接近於0這就像是一個人在經過深思熟慮之後他想要的答案对自己得出的结论会非常笃定一样所以说思维链解码的核心就两步一超越贪婪解码生成并且检查更多的候选输出二选择那个对最终答案自信度最高的候选这个方法虽然简单有效但还是需要写一些代码对于普通用户来说不够友好于是研究者们开始思考我们能不能用更自然的方式比如说自然语言来重塑模型的输出概率分布让那些带有思考过程的优秀答案能够自动排到第一名这样我们用最简单的贪婪解码就能够直接得到它这就引出了我们后来耳熟能详的一系列提示工程技术首先最著名的就是思维链提示它的做法非常直观在你提出你的问题之前先给模型看一两个类似的从问题到思考过程再到答案的例子比如说你想让模型解决前面苹果的问题你可以先给它一个例子问题是一个农村農民有5個香蕉他又買了6個後來吃了2個還剩幾個答案是農民開始有5個香蕉買了6個之後他有5加6等於11個然後他吃了2個所以他剩下11減2等於9個答案是9然後你再提出你的問題我有3個蘋果我爸爸比我多2個我們總共有多少個蘋果神奇的事情發生了模型會模仿你給出的例子的風格自動的開始一步一步分析生成詳細的解題步驟最後給出正確答案從概率分佈的角度來看你給出的例子極大的提升了模型生成類似思考過程的巨式的概率反而把原本隱藏在後面的正確推理路徑推到了最前面但是這種方法有一個問題你需要為不同類型的任務手動編寫高質量的實例這很麻煩而且如果你自己都知道怎麼解決一個類似的問題那你為什麼還要問AI呢於是一個更加神奇的提示就出現了它就是讓我們以後一步一步思考Let's think step by step但尼州坦言這篇論文剛出來的時候他以為這是個玩笑怎麼可能在問題後面加上這麼一句簡單的話模型就會自動開始思考了呢他當時就在谷歌內部的PALM模型上做了測試他非常清楚PALM的訓練數據裏絕對沒有針對這個咒語做過任何的優化結果他震驚地發現它真的有效模型真的開始輸出一步一步的解題過程了這個發現極大的啟發了他儘管Let's think step by step這種零樣本提示效果通常比不過提供具體事例的少樣本思維鏈提示但是它證明了我們可以用非常通用的方式來激發模型的推理潛能然而無論是哪種提示方法都感覺有點奇怪想象一下你問一個聰明人問題還必須得在後面加上一句請一步一步思考否則他就不會思考了這顯然不符合我們對於一個真正智能體系的看法所以我们需要一种更稳定更内化的方式让推理能力成为模型固有的一部分而不是需要外部咒语来触发这就把我们带到了下一个阶段微调 我们先说尖度微调SFT它的思路非常的直接我们不就是希望模型能够生成从问题到思考过程再到答案这样的数据吗那我们就雇佣一批人针对大量的问题写出高质量的一步一步的解题方案然后我们再把这些标准答案喂给模型 让模型去学习这个方法在机器学习里边叫做最大自然估计简单来说就是让模型生成的序列跟人类专家写的序列尽可能的一模一样这个想法其实很早就有了单极周提到早在2017年丁麦的一篇论文就在做类似的事情他们收集了一批数学应用题和人类手写的解题步骤来训练一个序列模型后来在2021年OPI更进一步构建了一个更稳定的模型更著名的數據集也就是GSM8K包含了8000多個小學水平的數學題和詳細解法用來微調GPT-3模型這種方法訓練出來的模型在你給它一個新問題的時候確實能夠生成不錯的解析步驟看起來問題似乎解決了一旦模型訓練好就可以隨時部署不再需要複雜的提示了然而在2021年夏天丹尼州的團隊發現了一個嚴重的問題那就是SFT訓練出來的模型算話能力很差它在那些和訓練數據很相似的問題上表現很好但是一旦遇到一個新的類型稍微不同的問題就很容易失敗他們嘗試了大力出奇技的方法擴大了數據的規模找更多的人標註更多的數據可惜結果卻是無論如何擴大規模這個問題始終存在丹尼州在這裡給出了一個重要的教訓不要盲目的擴大規模當你的範式本身是錯誤的時候再多的數據也不代表你那麼SFT的範式錯在哪裡了呢問題又出在流程裏的哪一步呢丹妮周給出的答案可能會讓你大吃一驚她說錯誤出在人身上這個轉折點來自於自我提升後來也被稱為self-improve或者是start方法當她第一次聽到機器生成的訓練數據可能比人類專家寫得還好這個想法的時候她自己也感到非常驚訝這個新範式的流程是這樣的首先我們仍然從一批問題開始但是我們不再找人類去寫解題步驟我們讓一個已經比較強大的大圓模型自己去針對這些問題生成大量的多樣的解題步驟最關鍵的一步是我們用一個驗證器去檢查模型生成的這些解題步驟看哪個最終得出了正確的答案比如說對於數學題我們知道標準答案就可以直接的判斷於是我們只保留下來那些過程多樣但是結果正確的生成結果把解題步驟寫出來把它们当作新的高质量的训练数据然后用这些由模型自己生成的并且经过验证的好数据再去微调模型自己这个过程可以不断地迭代一个微调后变得更强的模型又可以去生成质量更高更复杂的解题步骤用来进一步的训练自己这就形成了一个自我进化的闭环赖铁周提到一篇在2024年1月发表的来自字节跳动的论文Reasoning with reinforced for ITUny是他在学术界看到的最早公开阐述类似思想的出版物之一他相信在OpenAI等多个机构内部大家可能都独立地发现了这个简单而又极其有效的思想现在我们必须回答那个核心的问题为什么模型自己生成的数据会比人类专家手写的数据在训练效果上更好呢这背后其实蕴含着技艺学习的一个第一性原理那就是直接优化你想要的东西在SFT的范式里我们优化的目标是让模型更有效我是大飞经常接触大模型的朋友一定会有过这样的经历当你向AI提出一个复杂的问题时它不仅给出了答案还给出了详细的一步一步的看起来逻辑严密的解题过程那一瞬间你是不是感觉屏幕对面的不再是一堆冰冷的代码而是一个真正能够推理的智能体但是有的时候你换一个同样复杂但是略有不同的问题时它又会给出一个错的离谱的答案让你觉得它根本什么都不懂只是一个更高级的复读机这种体验上的巨大反差正是当前AI领域最核心的谜题之一大元模型展现出的推理能力究竟是一种真正的智能涌现还是一种基于海量数据训练出来的更高级的模式匹配它是在进行逻辑推导还是在模仿它在网上看过的无数解题步骤关于这个问题学术界和工业界争论不休但是我们还是要看但是争论的意义远不如我们去搞清楚这种所谓的推理能力到底是怎么来的以及我们如何才能稳定的可靠的驾驭它而要解答这个问题有一个人的名字我们就绕不开他就是来自于谷歌丁曼的丹尼周他和他的团队可以说是奠定了我们今天理解和使用大元模型推理能力的基石他们开创性的提出了像思维链提示和自下行这类的关键技术并且深度参与了谷歌静态模型推理能力的构建最近丹尼周在斯坦福大学做了一场演讲他系统性的梳理了从他创立谷歌大脑的推理团队开始到今天我们所看到的强大的AI这条技术路线是如何一步一步演进的这场讲座的信息量巨大它不仅揭示了AI推理能力的本质更是对过去几年中所有相关技术的一次趣味所以今天这期视频我们就将以丹尼周的这场讲座为了帶著大家從最基礎的概念出發層層遞進徹底搞懂大約模型思考秘密我們會看到那些看似神奇的技術背後往往遵循著一些極其簡單而深刻的原理相信看完這期視頻你再看待大約模型的時候會有一個全新的更加清晰的視角在深入探討之前我們必須先明確一件事情當我們討論大約模型的推理時我們到底在談論什麼單機中一上來就拋出了一個非常清晰而且可操作的定義這個定義也成為了整個領域的共識他說關於模型到底會不會推理的哲學辯論因為沒有一個明確的定義大家都是在自說自話而在他的團隊裡推理是有一個非常具體的含義的那就是在模型的輸入也就是你的問題和最終輸出也就是答案之間生成的所有中間步驟這個定義非常關鍵他把一個很簡單的模糊的哲学层面的思考概念转化成了一个具体的工程上可以实现和优化的目标为了让我们更好的理解这个目标丹尼周设计了一个非常巧妙的任务叫做末尾字母拼接这个任务听起来很简单比如说我问模型请拼接artificial intelligence这两个单词的末尾字母如果我们直接让模型输出答案它可能会凭借着语言的惯性直接猜一个答案比如说LE这时候它只是在预测下一个最可能的字符而不是在执行一个多步骤的逻辑操作但是如果我们引导模型先生成中间步骤它的输出就会变成这样artificial的最后一个字母是Lintelligence的最后一个字母是E将L和E拼接起来得到LE这就是丹尼周所定义的推理他把一个复杂的任务分解成了一系列简单的可执行的子任务最终导出了一个正確答案现在我们可以更好地加入功能符合你的工作模式我会加入掃描收入 数据化和支付费用现在我可以在照片中立刻刷出一张图片没有任何的手册费用最后我们加入费用预测档预测月和年费现在你有一个应用软件专注于你的生意需要建立在你工作的方式里开始你的工作你可能会觉得这不就是我们人类解决问题的方式吗先思考再作答但是赶紧周提醒我们作为研究者必须时刻记住大猿模型无视人类他們只是概率模型把他們擬人化雖然有助於我們理解但是也很容易讓我們走入歧途一個有趣的故事是丹琪周最初嘗試的是手字母拼接但是他發現當時所有的模型都能夠做得很好為什麼呢因為互聯網上有大量的縮寫詞模型在預訓練階段已經背會了如何拼接手字母於是他換成了末尾字母拼接結果當時所有的模型都失敗了這恰恰說明模型並沒有真正理解拼接這個動作而只是記住了某種常見的模式那麼為什麼要如此執著於生成這些中間步驟呢僅僅是為了模仿人類嗎當然不是這背後有著非常堅實的理論依據丹琪周提到了他們和斯坦福大學教授滕上華團隊合作的一項理論研究這個研究得出了一個非常強大的結論對於任何一個可以被大小為T的物理電路解決的問題一個常識的問題可以通过生成OT长度的中间步骤来解决它这句话听起来有点过于技术我们来把它翻译一下布尔电路可以被看作是执行逻辑运算的基本单元任何复杂的计算任务比如说运行一个大型的软件本质上都可以被分解成一个巨大规模的布尔电路这里的大小T就代表了问题的计算复杂度这个理论告诉我们哪怕是一个相对简单的Transformer模型只要你允许它生成足够长的思考过程也就是中间步骤它就有潜力解决几乎任何可计算的问题反过来说如果我们强迫模型直接蹦出最终答案就相当于要求这个模型的网络深度本身要能够模拟整个复杂的计算过程这要么需要一个巨大到不切实际的深度要么就根本无法解决问题所以让模型思考生成中间步骤不是一个可有可无的计算过程而是在计算原理上解锁模型解决复杂问题能力的一把金钥匙这彻底改变了我们训练和使用大学模型的方式从单纯的追求答案转向追求过程好既然我们知道了推理过程如此的重要那么下一个问题自然就是如何让模型来生成这个过程呢这里呢丹妮周提出了一个颠覆了当时很多人认知的观点当时普遍认为一个普通的只经过预训练的大学模型是不会推理的你必须通过像思维链提示这样的高级技巧或者进行专门的微调才能够交配但是丹妮周说这个观点是错的而且大错特错她认为预训的模型早就已经准备好进行推理了我们所需要做的仅仅是改变解码的过程这又是一个非常深刻的洞察为了证明这一点她举了一个经典的数学硬题我們總共有多少個蘋果如果你把這個問題直接輸入給一個原始的預訓練模型比如說早期的GDP3或者是拉馬然後使用默認的貪婪解碼方式會發生什麼呢貪婪解碼的意思是模型在生成每一個詞的時候總是會選擇當前概率最高的那一個在這種模式下模型很可能會直接輸出一個看似合理但卻是錯誤的答案比如說5個蘋果因為它看到了3個和多2個就直接聯想到了5這是模型的一種直覺反應或者說是一種系統思維但是模型的強大之處在於它的輸出概率分佈中並不僅僅只有這一個選項在生成第一個詞的時候5個可能是概率最高的但是還有第二第三第四高的選項如果我們不那麼貪婪而是去探索一下那些概率稍低一些的岔路奇蹟就會發生了接著丹妮周向我們展示了這些隱藏的候選答案比如說就說候選二可能以我字開頭模型會聲稱我有三個蘋果我爸爸比我多兩個所以他有五個蘋果三加五等於八所以我們總共有八個蘋果這是一個完美的推理鏈答案也正確候選三可能以我們開頭模型會聲稱我們總共有八個蘋果雖然沒有過程但是答案也對了候選四可能以你字開頭模型會聲稱你有三個蘋果你爸爸有三加二等於五個蘋果你們總共有三加五等於八個蘋果這同樣是一個清晰的推理鏈看到了嗎正確的推理路徑其實一直都存在於模型的輸出空間裏它們就像是隱藏在主幹道旁邊的小路默認的貪婪解碼因為只看到了眼前最寬的路所以錯過了它們這個發現被稱為思維鏈解碼它告訴我們推理能力不是被注入到模型裏面的而是模型在學習海量文本中蘊含的邏輯關系之後自然有了這個能力於是我們的任務從教會模型推理變成了如何引導模型把他已經知道的東西以正確的形式表達出來那麼問題就變成了在這麼多候選的輸出裏我們怎麽知道哪一個是最好的呢一個簡單的想法是看長度在有思考過程的回答通常會更長但是丹尼州的團隊發現一個更可靠的指標那就是答案自信度他們觀察到一個驚人的現象對於那些包含了正確思維鏈的回答模型在生成最終答案的那個詞比如說數字8的時候其內部的自信度也就是概率會異常的高在這個蘋果的例子裏模型預測8這個詞的概率可能會高達98%這是一個非常強的信號因為對於一個擁有巨大詞彙表的模型來說通常每個詞的概率都接近於0這就像是一個人在經過深思熟慮之後對自己得出的結論會非常篤定一樣所以說思维链解码的核心就两步一超越贪婪解码生成并且检查更多的候选输出二选择那个对最终答案自信度最高的候选这个方法虽然简单有效但还是需要写一些代码对于普通用户来说不够友好于是研究者们开始思考我们能不能用更自然的方式比如说自然语言来重塑模型的输出概率分布让那些带有思考过程的优秀答案能够自动排到第一名这样我们用最简单的贪婪解码就能够直接得到它这就引出了我们后来耳熟能详的一系列提示工程技术首先最著名的就是思维链提示它的做法非常直观在你提出你的问题之前先给模型看一两个类似的从问题到思考过程再到答案的例子比如说你想让模型解决前面那个苹果的问题你可以先给它一个例子问题是一个农民有五个香蕉他又买了六个后来吃了两个还吃了三个这就叫做贪婪解码这就是我们的思考过程我们的思考过程我们的思考过程我们的思考过程我们的思考过程答案是農民開始有五個香蕉買了六個之後他有5加6等於11個然後他吃了兩個所以他剩下11減2等於9個答案是9然後你再提出你的問題我有三個蘋果我爸爸比我多兩個我們總共有多少個蘋果神奇的事情發生了模型會模仿你給出的例子的風格自動的開始一步一步分析生成詳細的解題步驟最後給出正確答案從概率分佈的角度來看你給出的例子極大的提升了模型生成類似思考過程的巨式的概率從而把原本隱藏在後面的正確推理路徑推到了最前面但是這種方法有一個問題你需要為不同類型的任務手動編寫高質量的實例這很麻煩而且如果你自己都知道怎麼解決一個類似的問題那你為什麼還要問AI呢於是一個更加神奇的提示就出現了它就是讓我們一步一步思考Let's think step by step單機周談一下这篇论文刚出来的时候他以为这是个玩笑怎么可能在问题后面加上这么一句简单的话模型就会自动开始思考了呢他当时就在谷歌内部的PALM模型上做了测试他非常清楚PALM的训练数据里绝对没有针对这个咒语做过任何的优化结果他震惊地发现它真的有效模型真的开始输出一步一步的解题过程了这个发现极大的启发了他尽管Let's think step by step这种零样本提示效果通常比不过提供具体示例的但是他证明了我们可以用非常通用的方式来激发模型的推理潜能然而无论是哪种提示方法都感觉有点奇怪想象一下你问一个聪明人问题还必须得在后面加上一句请一步一步思考否则他就不会思考了这显然不符合我们对于一个真正智能体的期望所以我们需要一种更稳定更内化的方式讓推理能力成為模型固有的一部分而不是需要外部咒語來觸發這就把我們帶到了下一個階段微調我們先說監督微調SFT它的思路非常的直接我們不就是希望模型能夠生成從問題到思考過程再到答案這樣的數據嗎那我們就僱用一批人針對大量的問題寫出高質量的一步一步的解題方案然後我們再把這些標準答案背給模型讓模型去學習這個方法在機器學習裏面叫做最大三人估計簡單來說就是讓模型生成的序列跟人類專家寫的序列盡可能的一模一樣這個想法其實很早就有了單機周提到早在2017年DMITE的一篇論文就在做類似的事情他們收集了一批數學應用題和人類手寫的解題步驟來訓練一個序列模型後來在2021年OPI更進一步構建了一個更大更著名的數據集也就是GSM8K包含了百分之一百的模型大圆模型展现出的推理能力究竟是一种真正的智能涌现还是一种基于海量数据训练出来的更高级的模式匹配呢它是在进行逻辑推导还是在模仿它在网上看过的无数解题步骤呢关于这个问题学术界和工业界争论不休但是争论的意义远不如我们去搞清楚这种所谓的推理能力到底是怎么来的以及我们如何才能稳定的可靠的驾驭它而要解答这个问题有一个人的名字我们就绕不开他就是来自于谷歌丁曼的丹尼周他和他的团队可以说是奠定了我们今天理解和使用大圆模型推理能力的基石他们开创性的提出了像思维链提示和自下行这类的关键技术并且深度参与了谷歌金本奶模型推理能力的构建最近丹尼周在斯坦福大学做了一场演讲系统性的梳理了从他创立谷歌丁曼的基础大腦的推理團隊開始到今天我們所看到的強大的AI這條技術路線是如何一步一步演進的這場講座的信息量巨大它不僅揭示了AI推理能力的本質更是對過去幾年中所有相關技術的一次去媚所以今天這期視頻我們就將以丹尼周的這場講座為藍本帶著大家從最基礎的概念出發層層遞進徹底搞懂大約模型思考秘密我們會看到那些看似神奇的技術背後往往遵循著一些極其簡單而深刻的原理相信看完這期視頻你再看待大約模型的時候會有一個全新的更加清晰的視角在深入探討之前我們必須先明確一件事情當我們討論大約模型的推理時我們到底在談論什麼丹尼周一上來就拋出了一個非常清晰而且可操作的定義這個定義也成為了整個領域的共識他說關於模型到底会不会推理的哲学辩论他从不参加因为没有一个明确的定义大家都是在自说自话而在他的团队里推理是有一个非常具体的含义的那就是在模型的输入也就是你的问题和最终输出也就是答案之间生成的所有中间步骤这个定义非常关键他把一个模糊的哲学层面的思考概念转化成了一个具体的工程上可以实现和优化的目标为了让我们更好的理解这个目标丹尼周设计了一个非常巧妙的任务叫做末尾字母拼接这个任务听起来很简单比如说我问模型请拼接artificial intelligence这两个单词的末尾字母如果我们直接让模型输出答案它可能会凭借着语言的惯性直接猜一个答案比如说LE这时候它只是在预测下一个最可能的字符而不是在执行一个多步骤的逻辑操作但是如果我们拼接这个最可能的字符它可能会直接猜一个答案比如说LE这时候它只是在预测下一个最可能的字符而不是在执行一个多步骤的逻辑操作但是如果我们拼接artificial intelligence如果我们引导模型先生成中间步骤它的输出就会变成这样artificial的最后一个字母是Lintelligence的最后一个字母是E将L和E拼接起来得到LE这就是丹妮周所定义的推理她把一个复杂的任务分解成了一系列简单的可执行的子任务最终导出了正确的答案你可能会觉得这不就是我们人类解决问题的方式吗先思考再作答但是丹妮周提醒我们作为研究者必须时刻记住大猿模型不是人类它们只是概率模型把它们拟人化虽然有助于我们理解但是也很容易让我们走入歧途一个有趣的故事是丹妮周最初尝试的是首字母拼接但是她发现当时所有的模型都能够做得很好为什么呢因为互联网上有大量的缩写词模型在预训那阶段已经背会了如何拼接首字母于是她换成了末尾字母拼接结果当时所有的模型都失败了这恰恰说明模型并没有真正理解拼接这个动作而只是记住了某种常见的模式那么为什么要如此执着于生成这些中间步骤呢仅仅是为了模仿人类吗当然不是这背后有着非常坚实的理论依据丹琪州提到了他们和斯坦福大学教授腾讯华团队合作的一项理论研究这个研究得出了一个非常强大的结论对于任何一个可以被大小为T的布尔电路解决的问题一个常数大小的Transformer模型可以通过生成OT长度的中间步骤来解决它这句话听起来有点过于技术我们来把它翻译一下布尔电路可以被看作是执行逻辑运算的基本单元任何复杂的计算任务比如说运行一个大型的软件本质上都可以被分解成一个巨大规模的布尔电路这里的大小T就代表了问题的计算复杂度这个理论告诉我们哪怕是一个相对简单的transformer模型只要你允许它生成足够长的思考过程也就是中间步骤它就有潜力解决几乎任何可计算的问题反过来说如果我们强迫模型直接蹦出最终答案就相当于要求这个模型的网络深度本身要能够模拟整个复杂的计算过程这要么需要一个巨大到不切实际的深度要么就根本无法解决问题所以让模型思考生成中间步骤不是一个可有可无的选项而是在计算原理上解锁模型解决复杂问题能力的一把金钥匙这彻底改变了我们训练和使用大学模型的范式从单纯的追求答案转向追求过程好 既然我们知道了推理过程如此的重要那么下一个问题自然就是如何让模型来生成这个过程呢这里丹妮周提出了一个颠覆了当时很多人认知的最简单的模型思考方法就是用模型思考的方法来解决问题这种方法是非常简单的就是用模型思考的方法来解决问题然后用模型思考的方法来解决问题当时普遍认为一个普通的只经过预训练的大圆模型是不会推理的你必须通过像思维链提示这样的高级技巧或者进行专门的微调才能够教会他们推理但是丹尼周说这个观点是错的而且大错特错他认为预训模型早就已经准备好进行推理了我们所需要做的仅仅是改变解码过程全面的设计和设置都包括了全面的设计 没有微管使用图形设计来连接多个桌子并且设置不同的团队同样的位置没有项目费用没有需要的加装没有无聊的日常站台搅拌一下我们的Shuffle模式并且给每个团队员提供AI的建议没有什么像不可预料的公开说话的好玩MondayDev的GitHub的融合使用了强大的自动化以给你全面的设计进行日常进步并且让你的发展者能够在GitHub和关注CodeGeek不需要手册更新Monday Dev计划 追踪 和 运行程序 迅速订阅免费试试不需要信用卡Atera是IT的痛苦凶手遥控和管理遥控和接收助援设备和订单聪明的自动化编程 软件解决编程最神奇的是它能做到这些却又轻易使用真是神奇只是一个清晰 优雅的UI所有东西都在它应该的位置它就做了它应该做的事情对不起我感到激动不过以伪想性警告你解决了真正的IT问题才會成為IT的問題對不起我聽不懂你在修理電腦的聲音有多酷以後以後在科學上的報告管理層終於明白我為什麼是真正的MVP另外以設備價格來看這就省了50%的IT費用讓我成為新公司的英雄你的腦袋在發瘋真正的英雄是戴著耳機你會不會也許不會你不需要買任何東西你可以現在立即試試免費的 設置成風景只要點擊連結 試試看很棒Atera這是我需要的接受餅乾餅乾接受了这又是一个非常深刻的洞察为了证明这一点他举了一个经典的数学硬题我有三个苹果我的爸爸比我多两个苹果我们总共有多少个苹果如果你把这个问题直接输入给一个原始的预训量模型比如说早期的GPT-3或者是拉玛然后使用默认的贪婪解码方式会发生什么呢贪婪解码的意思是模型在生成每一个词的时候总是会选择当前概率最高的那一个在这种模式下模型很可能会直接输出一个看似合理但却是错误的答案比如说五个苹果那些概率稍低一些的岔路奇迹就会发生了接着单机周向我们展示了这些隐藏的候选答案比如说候选二可能以我字开头模型会生成我有三个苹果我爸爸比我多两个所以他有五个苹果3加5等于8所以我们总共有八个苹果这是一个完美的推理链答案也正确候选三可能以我们开头模型会生成我们总共有八个苹果虽然没有过程但是答案也对了候选四可能以你字开头模型会生成你有三个苹果你爸爸有3加2等于五个苹果你们总共有3加5等于八个苹果这同样是一个清晰的推理链看到了吗正确的推理路径其实一直都存在于模型的输出空间里它们就像是隐藏在主干道旁边的小路默认的贪婪解码因为只看到了眼前最宽的路所以错过了它们这个发现被称为思维链解码它告诉我们推理能力不是被注入到模型里面的而是模型在学习海量文本中蕴含的逻辑关系之后自然涌现出来的于是我们的任务从交汇模型推理变成了如何引导模型把它已经知道的东西以正确的形式表达出来那么问题就变成了在这么多候选的输出里我们怎么知道哪一个是最好的呢一个简单的想法是看长度带有思考过程的回答通常会更长但是丹基州的团队发现一个更可靠的指标那就是答案自信度他们观察到一个惊人的现象对于那些包含了正确思维链的回答模型在生成最终答案的那个词比如说数字8的时候其内部的自信度也就是概率会异常的高在这个苹果的例子里模型预测8这个词的概率可能会高达98%这是一个非常强的信号因为对于一个拥有巨大词汇表的模型来说通常每个词的概率都会比较高都接近于0这就像是一个人在经过深思熟虑之后对自己得出的结论会非常笃定一样所以说思维链解码的核心就两步一超越贪婪解码生成并且检查更多的候选输出二选择那个对最终答案自信度最高的候选这个方法虽然简单有效但还是需要写一些代码对于普通用户来说不够友好于是研究者们开始思考我们能不能用更自然的方式比如说自然语言来重塑模型的输出概率分布让那些带有思考过程的优秀答案能够自动排到第一名这样我们用最简单的贪婪解码就能够直接得到它这就引出了我们后来耳熟能详的一系列提示工程技术首先最著名的就是思维链提示它的做法非常直观在你提出你的问题之前先给模型看一两个类似的从问题到思考过程再到答案的例子比如说你想让模型解决前面那个苹果的问题你可以先给他一个例子问题是一个农民有5个香蕉他又买了6个后来吃了两个还剩几个答案是农民开始有5个香蕉买了6个之后他有5加6等于11个然后他吃了两个所以他剩下11-2等于9个答案是9然后你再提出你的问题我有三个苹果我爸爸比我多两个我们总共有多少个苹果神奇的事情发生了模型会模仿你给出的例子的风格自动的开始一步一步分析生成详细的解题步骤最后给出正确答案从概率分布的角度来看你给出的例子极大的提升了模型生成类似思考过程的巨式的概率反而把原本隐藏在后面的正确推理路径推到了最前面但是这种方法有一个问题你需要为不同类型的任务手动编写高质量的实例这很麻烦而且如果你自己都知道怎么解决一个类似的问题那你为什么还要问AI呢因为你自己都知道於是一個更加神奇的提示就出現了它就是讓我們一步一步思考Let's think step by step但尼周坦言這篇論文剛出來的時候他以為這是個玩笑怎麼可能在問題後面加上這麼一句簡單的話模型就會自動開始思考了呢他當時就在谷歌內部的PALM模型上做了測試他非常清楚PALM的訓練數據裏絕對沒有針對這個咒語做過任何的優化結果他震驚地發現它真的有效模型真的開始輸出一步一步的解題過程了這個發現極大的啟發了他儘管Let's think step by step這種零樣本提示效果通常比不過提供具體事例的少樣本思維鏈提示但是他證明了我們可以用非常通用的方式來激發模型的推理潛能然而無論是哪種提示方法都感覺有點奇怪想象一下你問一個聰明人問題還必須得在後面加上一句請一步一步思考否則他就不會思考了這顯然不符合我們對於一個真正智能體的期望所以我們需要一種更穩定更內化的方式讓推理能力成為模型固有的一部分而不是需要外部咒語來觸發這就把我們帶到了下一個階段微調我們先說監督微調SFT它的思路非常的直接我們不就是希望模型能夠生成從問題到思考過程再到答案這樣的數據嗎那我們就僱用一批人針對大量的問題寫出高質量的一步一步的解題方案然後我們再把這些標準答案餵給模型讓模型去學習這個方法在機器學習裏面叫做最大自然估計簡單來說就是讓模型生成的序列跟人類專家寫的序列盡可能的一模一樣這個想法其實很早就有了賴清周提到早在2017年DMI的一篇論文就在做類似的事情他們收集了一批數學硬體和人類手寫的解題步驟來訓練一個人類的智能體他們開創性地提出了像思維鏈提示和自洽性這類的關鍵技術並且深度參與了谷歌靜態模型推理能力的構建最近丹尼州在斯坦福大學做了一場演講系統性地梳理了從它創立谷歌大腦的推理團隊開始到今天我們所看到的強大的AI這條技術路線是如何一步一步演進的這場講座的信息量巨大它不僅揭示了AI推理能力的本質更是對過去幾年中所有相關技術的一次趣味所以今天這期視頻我們就將以丹尼州的這場講座為藍本帶著大家從最基礎的概念出發層層遞進徹底搞懂大約模型思考秘密我們會看到那些看似神奇的技術背後往往遵循著一些極其簡單而深刻的原理相信看完這期視頻你再看待大約模型的時候會有一個全新的更加清晰的視角在深入探討之前我們必須先來個探討當我們討論大圓模型的推理時我們到底在談論什麼丹妮周一上來就拋出了一個非常清晰而且可操作的定義這個定義也成為了整個領域的共識她說關於模型到底會不會推理的哲學辯論她從不參加因為沒有一個明確的定義大家都是在自說自話而在她的團隊裡推理是有一個非常具體的含義的那就是在模型的輸入也就是你的問題和最終輸出也就是答案之間生成的所有中間步驟這個定義非常關鍵她把一個模糊的哲學層面的思考概念轉化成了一個具體的工程上可以實現和優化的目標為了讓我們更好地理解這個目標丹妮周設計了一個非常巧妙的任務叫做末尾字母拼接這個任務聽起來很簡單比如說我問模型請拼接artificial intelligence這兩個單詞的末尾字母請拼接如果我们直接让模型输出答案它可能会凭借着语言的惯性直接猜一个答案比如说LE这时候它只是在预测下一个最可能的字符而不是在执行一个多步骤的逻辑操作但是如果我们引导模型先生成中间步骤它的输出就会变成这样artificial的最后一个字母是Lintelligence的最后一个字母是E将L和E拼接起来得到LE这就是丹妮周所定义的推理她把一个复杂的任务分解成了一系列简单的可执行的子任务最终导出了正确的答案你可能会觉得这不就是我们人类解决问题的方式吗先思考再作答但是丹妮周提醒我们作为研究者必须时刻记住大猿模型不是人类它们只是概率模型把它们擬人化虽然有助于我们理解但是也很容易让我们走入歧途一个有趣的故事是丹妮周最初尝试的是手动模型但是他发现当时所有的模型都能够做得很好为什么呢因为互联网上有大量的缩写词模型在预训练阶段已经背会了如何拼接首字母于是他换成了末尾字母拼接结果当时所有的模型都失败了这恰恰说明模型并没有真正理解拼接这个动作而只是记住了某种常见的模式那么为什么要如此执着于生成这些中间步骤呢仅仅是为了模仿人类吗当然不是这背后有着非常坚实的理论依据单极州提到了他们和斯坦福大学教授腾上华团队合作的一项理论研究这个研究得出了一个非常强大的结论对于任何一个可以被大小为T的布尔电路解决的问题一个常数大小的Transformer模型可以通过生成OT长度的中间步骤来解决它这句话听起来有点过于技术我们来把它翻译一下布尔电路可以被固定为T看作是执行逻辑运算的基本单元任何复杂的计算任务比如说运行一个大型的软件本质上都可以被分解成一个巨大规模的布尔电路这里的大小t就代表了问题的计算复杂度这个理论告诉我们哪怕是一个相对简单的transformer模型只要你允许它生成足够长的思考过程也就是中间步骤它就有潜力解决几乎任何可计算的问题反过来说如果我们强迫模型直接蹦出最终答案就相当于要求这个模型的网络深度本身要能够模拟整个复杂的计算过程这要么需要一个巨大到不切实际的深度要么就根本无法解决问题所以让模型思考生成中间步骤不是一个可有可无的选项而是在计算原理上解锁模型解决复杂问题能力的一把金钥匙这彻底改变了我们训练和使用大约模型的范式單純的追求答案轉向追求過程好 既然我們知道了推理過程如此的重要那麼下一個問題自然就是如何讓模型來生成這個過程呢這裡丹妮周提出了一個顛覆了當時很多人認知的觀點當時普遍認為一個普通的只經過預訓練的大圓模型是不會推理的你必須通過像思維鏈提示這樣的高級技巧或者進行專門的微調才能夠教會他們推理但是丹妮周說這個觀點是錯的而且大錯特錯他認為預訓練模型早就已經準備好進行推理了我們所需要做的僅僅是改變解碼的過程這又是一個非常深刻的洞察為了證明這一點他舉了一個經典的數學硬題我有三個蘋果我的爸爸比我多兩個蘋果我們總共有多少個蘋果如果你把這個問題直接輸入給一個原始的預訓練模型比如說早期的GDP-3或者是拉馬然後使用默認的推理模式貪婪解碼方式會發生什麼呢貪婪解碼的意思是模型在生成每一個詞的時候總是會選擇當前概率最高的那一個在這種模式下模型很可能會直接輸出一個看似合理但卻是錯誤的答案比如說五個蘋果因為它看到了三個和多兩個就直接聯想到了五這是模型的一種直覺反應或者說是一種系統思維但是模型的強大之處在於它的輸出概率分布中並不僅僅只有這一個選項在生成第一個詞的時候五個可能是概率最高的但是還有第二第三第四高的選項如果我們不那麼貪婪而是去探索一下那些概率稍低一些的岔路奇蹟就會發生了接著丹尼周向我們展示了這些隱藏的候選答案比如說候選二可能以我字開頭模型會生成我有三個蘋果我爸爸比我多兩個所以他有五個蘋果三加五等於八所以我們總共有八個蘋果這是一個完美的推理鏈答案也正確候選三可能以我們開頭模型會聲稱我們總共有八個蘋果雖然沒有過程但是答案也對了候選四可能以你自開頭模型會聲稱你有三個蘋果你爸爸有3加2等於五個蘋果你們總共有3加5等於八個蘋果這同樣是一個清晰的推理鏈看到了嗎正確的推理路徑其實一直都存在於模型的輸出空間裏它們就像是隱藏在主幹道旁邊的小路默認的貪婪解碼因為只看到了眼前最寬的路所以錯過了它們這個發現被稱為思維鏈解碼它告訴我們推理能力不是被注入到模型裏面的而是模型在學習海量文本中蘊含的邏輯關系之後自然湧現出來的於是我們的任務從教會模型推理變成了如何引導模型把它已經知道的東西以正確的形式表達出來那麽問題就變成了在正確的推理模型裏面模型裏面的東西是否存在著一個正確的推理模型的存在這就是我們要學習的模型的正確性模型的正確性模型的正確性模型的正確性模型的正確性后选的输出里我们怎么知道哪一个是最好的呢一个简单的想法是看长度带有思考过程的回答通常会更长但是丹妮周的团队发现一个更可靠的指标那就是答案自信度他们观察到一个惊人的现象对于那些包含了正确思维链的回答模型在生成最终答案的那个词比如说数字8的时候其内部的自信度也就是概率会异常的高在这个苹果的例子里模型预测8这个词的概率可能会高达98%这是一个非常强的信号因为对于一个拥有巨大词汇表的模型来说通常每个词的概率都接近于0这就像是一个人在经过深思熟虑之后对自己得出的结论会非常笃定一样所以说思维链解码的核心就两路一 超越贪婪解码生成并且检查更多的候选输出二 选择那个对最终答案自信度最高的候选这个方法呢是可以用来解决很多问题的雖然簡單有效踏入 Wix Studio 公司和企業的平台在你設計的工作環境中 踏出新項目穩定地工作 自由地實驗以精準和精準的意識 去設計複雜的設計到那最後的一張畫面穩定地結構和集中 動態內容以優雅和有效率的 管理豐富內容以 AI 為主體,以每個螢幕為主體,以 AI 為主體,以每個螢幕為主體,創造和自訂互動,從細節動作到進步行為,並通過視覺,從策劃到客戶手機。這是網路創作的規模,複雜,快速,無限,這是你的工作室。邏輯推導還是在模仿他在網上看過的無數解題步驟呢關於這個問題學術界和工業界爭論不休但是爭論的意義遠不如我們去搞清楚這種所謂的推理能力到底是怎麼來的以及我們如何才能穩定的可靠的駕馭它而要解答這個問題有一個人的名字我們就繞不開他就是來自於谷歌DeepMind的丹尼周他和他的團隊可以說是奠定了我們今天理解和使用大圓模型推理能力的基石他們開創性的提出了像思維鏈提示和自下行這類的關鍵技術並且深度參與了谷歌GNI模型推理能力的構建最近丹尼周在斯坦福大學做了一場演講系統性的梳理了從他創立谷歌大腦的推理團隊開始到今天我們所看到的強大的AI這條技術路線是如何一步一步演進的這場講座的信息量巨大它不僅揭示了AI推理能力的真實性更是对过去几年中所有相关技术的一次去媚所以今天这期视频我们就将以丹尼周的这场讲座为蓝本带着大家从最基础的概念出发层层递进彻底搞懂大约模型思考秘密我们会看到那些看似神奇的技术背后往往遵循着一些极其简单而深刻的原理相信看完这期视频你再看待大约模型的时候会有一个全新的更加清晰的视角在深入探讨之前我们必须先明确一件事情当我们讨论大约模型的推理时我们到底在谈论什么丹尼周一上来就抛出了一个非常清晰而且可操作的定义这个定义也成为了整个领域的共识他说关于模型到底会不会推理的哲学辩论他从不参加因为没有一个明确的定义大家都是在自说自话而在他的团队里推理是有一个非常具体的含义的那就是在模型的推理中我们可以看到模型的推理是一个非常具体的语言它是一个非常具体的语言它是一个非常具体的语言它是一个非常具体的语言它是一个非常具体的语言它是一个非常具体的语言也就是你的問題和最終輸出也就是答案之間生成的所有中間步驟這個定義非常關鍵它把一個模糊的哲學層面的思考概念轉化成了一個具體的工程上可以實現和優化的目標為了讓我們更好地理解這個目標丹尼周設計了一個非常巧妙的任務叫做末尾字母拼接這個任務聽起來很簡單比如說我問模型請拼接artificial intelligence這兩個單詞的末尾字母如果我們直接讓模型輸出答案它可能會憑藉著語言的慣性直接猜一個答案比如說LE這時候它只是在預測下一個最可能的字符而不是在執行一個多步驟的邏輯操作但是如果我們引導模型先生成中間步驟它的輸出就會變成這樣artificial的最後一個字母是Lintelligence的最後一個字母是E將L和E拼接起來得到這個結果这就是丹妮周所定义的推理她把一个复杂的任务分解成了一系列简单的可执行的子任务最终导出了正确的答案你可能会觉得这不就是我们人类解决问题的方式吗先思考再作答但是丹妮周提醒我们作为研究者必须时刻记住大猿模型不是人类它们只是概率模型把它们拟人化虽然有助于我们理解但是也很容易让我们走入歧途一个有趣的故事是丹妮周最初尝试的是首字母拼接但是她发现当时所有的模型都能够做得很好遠不如我們去搞清楚這種所謂的推理能力到底是怎麼來的以及我們如何才能穩定的可靠的駕馭它而要解答這個問題有一個人的名字我們就繞不開他就是來自於谷歌DeepMind的丹尼周他和他的團隊可以說是奠定了我們今天理解和使用大圓模型推理能力的基石他們開創性的提出了像思維鏈提示和自洽性這類的關鍵技術並且深度參與了谷歌近代模型推理能力的構建最近丹尼周在斯坦福大學做了一場演講系統性的梳理了從他創立谷歌大腦的推理團隊開始到今天我們所看到的強大的AI這條技術路線是如何一步一步演進的這場講座的信息量巨大它不僅揭示了AI推理能力的本質更是對過去幾年中所有相關技術的一次趣味所以今天這期視頻我們就將以丹尼周的這場講座為藍本帶著大家從谷歌大腦開始最基础的概念出发层层递进彻底搞懂大学模型思考秘密我们会看到那些看似神奇的技术背后往往遵循着一些极其简单而深刻的原理相信看完这期视频你再看待大学模型的时候会有一个全新的更加清晰的视角在深入探讨之前我们必须先明确一件事情当我们讨论大学模型的推理时我们到底在谈论什么单机中一上来就抛出了一个非常清晰而且可操作的定义这个定义也成为了整个领域的共识他说关于模型到底会不会推理的他从不参加因为没有一个明确的定义大家都是在自说自话而在他的团队里推理是有一个非常具体的含义的那就是在模型的输入也就是你的问题和最终输出也就是答案之间生成的所有中间步骤这个定义非常关键他把一个模糊的推理当成了一个中间步骤哲学层面的思考概念转化成了一个具体的工程上可以实现和优化的目标为了让我们更好的理解这个目标丹妮周设计了一个非常巧妙的任务叫做末尾字母拼接这个任务听起来很简单比如说我问模型请拼接artificial intelligence这两个单词的末尾字母如果我们直接让模型输出答案它可能会凭借着语言的惯性直接猜一个答案比如说LE这时候它只是在预测下一个最可能的字符而不是在执行一个多步骤的逻辑操作但是如果我们引导模型先生成中间步骤它的输出就会变成这样artificial的最后一个字母是Lintelligence的最后一个字母是E将L和E拼接起来得到LE这就是丹妮周所定义的推理她把一个复杂的任务分解成了一系列简单的可执行的子任务最终导出了正确的答案那我们就来看看你可能会觉得这不就是我们人类解决问题的方式吗先思考再作答但是丹妮周提醒我们作为研究者必须时刻记住大猿模型不是人类它们只是概率模型把它们拟人化虽然有助于我们理解但是也很容易让我们走入歧途一个有趣的故事是丹妮周最初尝试的是手字母拼接但是她发现当时所有的模型都能够做得很好为什么呢因为互联网上有大量的缩写词模型在预训练阶段已经背会了如何拼接手字母于是她换成了末尾字母拼接结果当时所有的模型都失败了这恰恰说明模型并没有真正理解拼接这个动作而只是记住了某种常见的模式那么为什么要如此执着于生成这些中间步骤呢仅仅是为了模仿人类吗当然不是这背后有着非常坚实的理论依据丹妮周提到了她们和斯坦福大学教授腾尚合作的一項理論研究這個研究得出了一個非常強大的結論對於任何一個可以被大小為T的布爾電路解決的問題一個常數大小的Transformer模型可以通過生成OT長度的中間步驟來解決它這句話聽起來有點過於技術我們來把它翻譯一下布爾電路可以被看作是執行邏輯運算的基本單元任何復雜的計算任務比如說運行一個大型的軟件本質上都可以被分解成一個巨大規模的布爾電路這裡的大小T就代表了問題的計算復雜度這個理論告訴我們哪怕是一個相對簡單的Transformer模型只要你允許它生成足夠長的思考過程也就是中間步驟它就有潛力解決幾乎任何可計算的問題反過來說如果我們強迫模型直接蹦出最終答案就相當於要求這個模型的網絡深度本身要能夠模擬整個復雜度这要么需要一个巨大到不切实际的深度要么就根本无法解决问题所以让模型思考生成中间步骤不是一个可有可无的选项而是在计算原理上解锁模型解决复杂问题能力的一把金钥匙这彻底改变了我们训练和使用大圆模型的范式从单纯的追求答案转向追求过程好 既然我们知道了推理过程如此的重要那么下一个问题自然就是如何让模型来生成这个过程呢这里丹基周提出了一个颠覆了当时很多人认知的观点当时普遍认为一个普通的只经过预训练的大圆模型是不会推理的你必须通过像思维链提示这样的高级技巧或者进行专门的微调才能够教会他们推理但是丹基周说这个观点是错的而且大错特错他认为预训模型早就已经准备好进行推理了我们所需要做的就是僅僅是改變解碼的過程這又是一個非常深刻的洞察為了證明這一點他舉了一個經典的數學硬題我有三個蘋果我的爸爸比我多兩個蘋果我們總共有多少個蘋果如果你把這個問題直接輸入給一個原始的預訓練模型比如說早期的GDP3或者是拉馬然後使用默認的貪婪解碼方式會發生什麼呢貪婪解碼的意思是模型在生成每一個詞的時候總是會選擇當前概率最高的那一個在這種模式下模型很可能會直接輸出一個看似合理但卻是錯誤的答案比如說五個蘋果因為它看到了三個和多兩個就直接聯想到了五這是模型的一種直覺反應或者說是一種系統思維但是模型的強大之處在於它的輸出概率分佈中並不僅僅只有這一個選項在生成第一個詞的時候五個可能是概率最高的但是還有第二第三第四高的選項如果我們把這個模型直接輸入給一個原始的預訓練模型我們總共會有多少個蘋果我們總共會有多少個蘋果我們總共會有多少個蘋果我們總共會有多少個蘋果我們總共會有多少個蘋果我們總共會有多少個蘋果我們不那麼貪婪而是去探索一下那些概率稍低一些的岔路奇蹟就會發生了接著單機周向我們展示了這些隱藏的候選答案比如說候選二可能以我字開頭模型會生成我有三個蘋果我爸爸比我多兩個所以他有五個蘋果3加5等於8所以我們總共有八個蘋果這是一個完美的推理鏈答案也正確候選三可能以我們開頭模型會生成我們總共有八個蘋果雖然沒有過程但是答案也對了候選四可能以你字開頭模型會生成你有三個蘋果你爸爸有3加2等於五個蘋果你們總共有3加5等於八個蘋果這同樣是一個清晰的推理鏈看到了嗎正確的推理路徑其實一直都存在於模型的輸出空間裏它們就像是隱藏在主幹道旁邊的小路默認的貪婪解碼因為只看到了眼前最寬的路所以錯過了它們這個發現被探索了他告诉我们推理能力不是被注入到模型里面的而是模型在学习海量稳稳中蕴含的逻辑关系之后自然涌现出来的于是我们的任务从交汇模型推理变成了如何引导模型把它已经知道的东西以正确的形式表达出来那么问题就变成了在这么多候选的输出力我们怎么知道哪一个是最好的呢一个简单的想法是看长度带有思考过程的回答通常会更长但是丹妮周的团队发现一个更可靠的指标那就是答案自信度他们观察到一个惊人的现象对于那些包含了正确思维链的回答模型在生成最终答案的那个词比如说数字8的时候其内部的自信度也就是概率会异常的高在这个苹果的例子里模型预测8这个词的概率可能会高达98%这是一个非常强的信号因为对于一个拥有巨大词汇表的模型来说它是一个非常强的信号所以我们就要更加的注意模型的自信度它是一个非常强的信号因为对于一个拥有巨大词汇表的模型来说它是一个非常强的信号所以我们就要更加的注意模型的自信度通常每個詞的概率都接近於零這就像是一個人在經過深思熟慮之後對自己得出的結論會非常篤定一樣所以說思維鏈解碼的核心就兩步一超越貪婪解碼生成並且檢查更多的候選輸出二選擇那個對最終答案自信度最高的候選這個方法雖然簡單有效但還是需要寫一些代碼對於普通用戶來說不夠友好於是研究者們開始思考我們能不能用更自然的方式比如說自然語言來重塑模型的輸出概率分布讓那些帶有思考過程的優秀答案能夠自動排到第一名這樣我們用最簡單的貪婪解碼就能夠直接得到它這就引出了我們後來耳熟能詳的一系列提示工程技術首先最著名的就是思維鏈提示它的做法非常直觀在你提出你的問題之前先給模型看一兩個類似的從問題到思考過程再到答案经常接触大模型的朋友一定会有过这样的经历当你向AI提出一个复杂的问题时它不仅给出了答案还给出了详细的一步一步的看起来逻辑严密的解题过程那一瞬间你是不是感觉屏幕对面的不再是一堆冰冷的代码而是一个真正能够推理的智能体但是有的时候你换一个同样复杂但是略有不同的问题时它又会给出一个错的离谱的答案让你觉得它根本什么都不懂只是一个更高级的复读机这种体验上的巨大反差正是当前AI领域最核心的谜题之一大圆模型展现出的推理能力究竟是一种真正的智能涌现还是一种基于海量数据训练出来的更高级的模式匹配它是在进行逻辑推导还是在模仿它在网上看过的无数解题步骤关于这个问题学术界和工业界争论不休但是争论的意义远不如理解不如我们去搞清楚这种所谓的推理能力到底是怎么来的以及我们如何才能稳定的可靠的驾驭它而要解答这个问题有一个人的名字我们就绕不开他就是来自于谷歌丁曼的丹尼周他和他的团队可以说是奠定了我们今天理解和使用大元模型推理能力的基石他们开创性的提出了像思维链提示和自下行这类的关键技术并且深度参与了谷歌静态模型推理能力的构建最近丹尼周在斯坦福大学做了一场演讲系统性的梳理了最基础的概念出发层层递进彻底搞懂大学模型思考秘密我们会看到那些看似神奇的技术背后往往遵循着一些极其简单而深刻的原理相信看完这期视频你再看待大学模型的时候会有一个全新的更加清晰的视角在深入探讨之前我们必须先明确一件事情当我们讨论大学模型的推理时我们到底在谈论什么单机中一上来就抛出了一个非常清晰而且可操作的定义这个定义也成为了整个领域的共识他说关于模型到底会不会推理的哲学辩论他从不参加因为没有一个明确的定义大家都是在自说自话而在他的团队里推理是有一个非常具体的含义的那就是在模型的输入也就是你的问题和最终输出也就是答案之间生成的所有中间步骤这个定义非常关键他把一个模糊的哲学辩论完全放在了模型里面这就是模型的输入也就是你的问题和最终输出也就是答案之间生成的所有中间步骤这个定义非常关键他把一个模糊的哲学辩论从科学层面的思考概念转化成了一个具体的工程上可以实现和优化的目标为了让我们更好地理解这个目标丹尼周设计了一个非常巧妙的任务叫做末尾字母拼接这个任务听起来很简单比如说我问模型请拼接Artificial Intelligence这两个单词的末尾字母如果我们直接让模型输出答案它可能会凭借着语言的惯性直接猜一个答案比如说LE这时候它只是在预测下一个最可能的字符而不是在执行一个多步骤的逻辑操作如果我们引导模型先生成中间步骤它的输出就会变成这样Artificial的最后一个字母是LIntelligence的最后一个字母是E将L和E拼接起来得到LE这就是丹尼周所定义的推理他把一个复杂的任务分解成了一系列简单的可执行的子任务最终导出了正确的答案你可能问這不就是我們人類解決問題的方式嗎先思考再作答但是丹妮周提醒我們作為研究者必須時刻記住大圓模型無視人類它們只是概率模型把它們擬人化雖然有助於我們理解但是也很容易讓我們走入歧途一個有趣的故事是丹妮周最初嘗試的是手字母拼接但是她發現當時所有的模型都能夠做得很好為什麼呢因為互聯網上有大量的縮寫詞模型在預訓練階段已經背會了如何拼接手字母於是她換成了末尾字母拼接結果當時所有的模型都失敗了這恰恰說明模型並沒有真正理解拼接這個動作而只是記住了某種常見的模式那麼為什麼要如此執著於生成這些中間步驟呢僅僅是為了模仿人類嗎當然不是這背後有著非常堅實的理論依據丹妮周提到了她們和斯坦福大學教授藤上華曾經說過合作的一項理論研究這個研究得出了一個非常強大的結論對於任何一個可以被大小為T的布爾電路解決的問題一個常數大小的Transformer模型可以通過生成OT長度的中間步驟來解決它這句話聽起來有點過於技術我們來把它翻譯一下布爾電路可以被看作是執行邏輯運算的基本單元任何復雜的計算任務比如說運行一個大型的軟件本質上都可以被分解成一個巨大規模的布爾電路這裡的大小T就代表了問題的計算復雜度這個理論告訴我們哪怕是一個相對簡單的Transformer模型只要你允許它生成足夠長的思考過程也就是中間步驟它就有潛力解決幾乎任何可計算的問題反過來說如果我們強迫模型直接蹦出最終答案就相當於要求這個模型的網絡深度本身要能夠模擬整個復雜度计算过程这要么需要一个巨大到不切实际的深度要么就根本无法解决问题所以让模型思考生成中间步骤不是一个可有可无的选项而是在计算原理上解锁模型解决复杂问题能力的一把金钥匙这彻底改变了我们训练和使用大学模型的方式从单纯的追求答案转向追求过程好 既然我们知道了推理过程如此的重要那么下一个问题自然就是如何让模型来生成这个过程这里丹妮周提出了一个颠覆了当时很多人认知的观点当时普遍认为一个普通的只经过预训练的大学模型是不会推理的你必须通过像思维链提示这样的高级技巧或者进行专门的微调才能够教会他们推理但是丹妮周说这个观点是错的而且大错特错她认为预训模型早就已经准备好进行推理了我们所需要的就是只要改變解碼的過程這又是一個非常深刻的洞察為了證明這一點他舉了一個經典的數學硬題我有三個蘋果我的爸爸比我多兩個蘋果我們總共有多少個蘋果如果你把這個問題直接輸入給一個原始的預訓練模型比如說早期的GDP-3或者是拉馬然後使用默認的貪婪解碼方式會發生什麼呢貪婪解碼的意思是模型在生成每一個詞的時候總是會選擇當前概率最高的那一個在這種模式下模型很可能會直接輸出一個看似合理但卻是錯誤的答案比如說五個蘋果因為他看到了三個和多兩個就直接聯想到了五這是模型的一種直覺反應或者說是一種系統思維但是模型的強大之處在於它的輸出概率分佈中並不僅僅只有這一個選項在生成第一個詞的時候五個可能是概率最高的但是還有第二第三第四高的選項任何一個都可以后来在2021年OPI更进一步构建了一个更大更著名的数据集也就是GSM给出了详细的一步一步的看起来逻辑严密的解题过程大家好这里是最佳拍档我是大飞经常接触大模型的朋友一定会有过这样的经历当你向AI提出一个复杂的问题时它不仅给出了答案还给出了详细的一步一步的看起来逻辑严密的解题过程那一瞬间你是不是感觉屏幕对面的不再是一堆冰冷的代码而是一个真正能够推理的智能体但是有的时候你换一个同样复杂但是略有不同的问题时它又会给出一个错的离谱的答案让你觉得它根本什么都不懂只是一个更高级的复读机这种体验上的巨大反差正是当前AI领域最核心的谜题之一大圆模型展现出的推理能力究竟是一种真正的智能涌现还是一种基于海量的推理能力训练出来的更高级的模式匹配呢它是在进行逻辑推导还是在模仿它在网上看过的无数解题步骤呢关于这个问题学术界和工业界争论不休但是争论的意义远不如我们去搞清楚这种所谓的推理能力到底是怎么来的以及我们如何才能稳定的可靠的驾驭它而要解答这个问题有一个人的名字我们就绕不开他就是来自于谷歌DeepMind的丹尼周他和他的团队可以说是奠定了我们今天理解和使用大元模型推理能力的基石他们开创性的提出了像思维链提示和自下行这类的关键技术并且深度参与了谷歌Gemini模型推理能力的构建最近丹尼周在斯坦福大学做了一场演讲系统性的梳理了从他创立谷歌大脑的推理团队开始到今天我们所看到的强大的AI这条技术路线是如何一步一步演进的這場講座的信息量巨大它不僅揭示了AI推理能力的本質更是對過去幾年中所有相關技術的一次去媚所以今天這期視頻我們就將以丹妮周的這場講座為藍本帶著大家從最基礎的概念出發層層遞進徹底搞懂大約模型思考秘密我們會看到那些看似神奇的技術背後往往遵循著一些極其簡單而深刻的原理相信看完這期視頻你再看待大約模型的時候會有一個全新的更加清晰的視角在深入探討之前我們必須先明確一件事情當我們討論大約模型的推理時我們到底在談論什麼丹妮周一上來就拋出了一個非常清晰而且可操作的定義這個定義也成為了整個領域的共識她說關於模型到底會不會推理的哲學辯論她從不參加因為沒有一個明確的定義大家都是在自說自話而在她的團體中推理是有一個非常具體的含義的那就是在模型的輸入也就是你的問題和最終輸出也就是答案之間生成的所有中間步驟這個定義非常關鍵它把一個模糊的哲學層面的思考概念轉化成了一個具體的工程上可以實現和優化的目標為了讓我們更好地理解這個目標單機周設計了一個非常巧妙的任務叫做末尾字母拼接這個任務聽起來很簡單比如說我問模型請拼接artificial intelligence這兩個單詞的末尾字母如果我們直接讓模型輸出答案它可能會憑藉著語言的慣性直接猜一個答案比如說LE這時候它只是在預測下一個最可能的字符而不是在執行一個多步驟的邏輯操作但是如果我們引導模型先生成中間步驟它的輸出就會變成這樣artificial的最後一個字母是Lintelligence的中間步驟最後一個字母是E將L和E拼接起來得到LE這就是丹妮周所定義的推理他把一個復雜的任務分解成了一系列簡單的可執行的子任務最終導出了正確的答案你可能會覺得這不就是我們人類解決問題的方式嗎先思考再作答但是丹妮周提醒我們作為研究者必須時刻記住大猿模型不是人類它們只是概率模型把它們擬人化雖然有助於我們理解但是也很容易讓我們走入歧途一個有趣的故事是丹妮周最初嘗試的是手字母拼接但是他發現當時所有的模型為什麼呢因為互聯網上有大量的縮寫詞模型在預訓練階段已經背會了如何拼接手字母於是它換成了落尾字母拼接結果當時所有的模型都失敗了這恰恰說明模型並沒有真正理解拼接這個動作而只是記住了某種動作那么为什么要如此执着于生成这些中间步骤呢仅仅是为了模仿人类吗当然不是这背后有着非常坚实的理论依据丹妮周提到了他们和斯坦福大学教授腾尚华团队合作的一项理论研究这个研究得出了一个非常强大的结论对于任何一个可以被大小为T的布尔电路解决的问题一个常数大小的Transformer模型可以通过生成OT长度的中间步骤来解决它这句话听起来有点过于技术我们来把它翻译一下布尔电路可以被看作是执行逻辑运算的基本单元任何复杂的计算任务比如说运行一个大型的软件本质上都可以被分解成一个巨大规模的布尔电路这里的大小T就代表了问题的计算复杂度这个理论告诉我们哪怕是一个相对简单的Transformer模型只要你允许它生成足够长的思考过程也就是一个中間步驟它就有潛力解決幾乎任何可計算的問題反過來說如果我們強迫模型直接蹦出最終答案就相當於要求這個模型的網絡深度本身要能夠模擬整個復雜的計算過程這要麼需要一個巨大到不切實際的深度要麼就根本無法解決問題所以讓模型思考生成中間步驟不是一個可有可無的選項而是在計算原理上解鎖模型解決復雜問題能力的一把金鑰匙這徹底改變了我們訓練和使用大圓模型的範式從單純的追求答案轉向追求過程好 既然我們知道了推理過程如此的重要那麼下一個問題自然就是如何讓模型來生成這個過程呢這裡丹妮周提出了一個顛覆了當時很多人認知的觀點當時普遍認為一個普通的只經過預訓練的大圓模型是不會推理的你必須通過向模型進行推理思维链提示这样的高级技巧或者进行专门的微调才能够交费他们推理但是丹基周说这个观点是错的而且大错特错他认为预训类模型早就已经准备好进行推理了我们所需要做的仅仅是改变解码的过程这又是一个非常深刻的洞察为了证明这一点他举了一个经典的数学硬题我有三个苹果我的爸爸比我多两个苹果我们总共有多少个苹果如果你把这个问题直接输入给一个原始的预训类模型比如说早期的GPT-3或者是拉玛然后使用默认的贪婪解码方式会发生什么呢贪婪解码的意思是模型在生成每一个词的时候总是会选择当前概率最高的那一个在这种模式下模型很可能会直接输出一个看似合理但却是错误的答案比如说五个苹果因为他看到了三个和多两个就直接联想到了五这是模型的一种直觉反应或者说是真正能夠推理的智能體但是有的時候你換一個同樣複雜但是略有不同的問題時它又會給出一個錯的離譜的答案讓你覺得它根本什麼都不懂只是一個更高級的復讀機這種體驗上的巨大反差正是當前AI領域最核心的謎題之一大圓模型展現出的推理能力究竟是一種真正的智能湧現還是一種基於海量數據訓練出來的更高級的模式匹配呢它是在進行邏輯推導還是在模仿它在網上看過的無數解題步驟呢關於這個問題學術界和工業界爭論不休但是爭論的意義遠不如我們去搞清楚這種所謂的推理能力到底是怎麼來的以及我們如何才能穩定的可靠的駕馭它而要解答這個問題有一個人的名字我們就繞不開他就是來自於谷歌DeepMind的丹尼周他和他的團隊可以說是奠定了我們今天理解的科學思想和使用大圆模型推理能力的基石他们开创性的提出了像思维链提示和自下性这类的关键技术并且深度参与了谷歌金奶模型推理能力的构建最近丹尼州在斯坦福大学做了一场演讲系统性的梳理了从它创立谷歌大脑的推理团队开始到今天我们所看到的强大的AI这条技术路线是如何一步一步演进的这场讲座的信息量巨大它不仅揭示了AI推理能力的本质更是对过去几年中所有相关技术的一次去媚所以今天这期视频我们就将以丹尼州的这场讲座为蓝本带着大家从最基础的概念出发层层递进彻底搞懂大圆模型思考秘密我们会看到那些看似神奇的技术背后往往遵循着一些极其简单而深刻的原理相信看完这期视频你再看待大圆模型的时候会有一个全新的更加清晰的理解在深入探讨之前我们必须先明确一件事情当我们讨论大约模型的推理时我们到底在谈论什么丹尼周一上来就抛出了一个非常清晰而且可操作的定义这个定义也成为了整个领域的共识他说关于模型到底会不会推理的哲学辩论他从不探加因为没有一个明确的定义大家都是在自说自话而在他的团队里推理是有一个非常具体的含义的那就是在模型的输入也就是你的问题和最终输出也就是答案之间生成的所有中间步骤这个定义非常关键他把一个模糊的哲学层面的思考概念转化成了一个具体的工程上可以实现和优化的目标为了让我们更好地理解这个目标丹尼周设计了一个非常巧妙的任务叫做末尾字母拼接这个任务听起来很简单比如说我问模型请拼接Artificial IntelligenceIntelligence這兩個單詞的末尾字母如果我們直接讓模型輸出答案它可能會憑藉著語言的慣性直接猜一個答案比如說LE這時候它只是在預測下一個最可能的字符而不是在執行一個多步驟的邏輯操作但是如果我們引導模型先生成中間步驟它的輸出就會變成這樣artificial的最後一個字母是Lintelligence的最後一個字母是E將L和E拼接起來得到LE這就是丹妮周所定義的推理她把一個復雜的任務分解成了一系列簡單的可執行的子任務最終導出了正確的答案你可能會覺得這不就是我們人類解決問題的方式嗎先思考再作答但是丹妮周提醒我們作為研究者必須時刻記住大約模型不是人類它們只是概率模型把它們擬人化雖然有助於我們理解但是也很容易讓我們走入歧途一個有趣的故事是丹妮周最初尝试的是手字母拼接但是她发现当时所有的模型都能够做得很好为什么呢因为互联网上有大量的缩写词模型在预训练阶段已经背会了如何拼接手字母于是她换成了末尾字母拼接结果当时所有的模型都失败了这恰恰说明模型并没有真正理解拼接这个动作而只是记住了某种常见的模式那么为什么要如此执着于生成这些中间步骤呢仅仅是为了模仿人类吗当然不是这背后有着非常坚实的理论依据丹妮周提到了他们和斯坦福大学教授腾尚华团队合作的一项理论研究这个研究得出了一个非常强大的结论对于任何一个可以被大小为T的布尔电路解决的问题一个常数大小的Transformer模型可以通过生成OT长度的中间步骤来解决它这句话听起来有点过于技术化但是实际上是一个很好的解决方案所以这就是为什么我们要学习的这种模型是要学习的我们要学习的我们要学习的我们来把它翻译一下布尔电路可以被看作是执行逻辑运算的基本单元任何复杂的计算任务比如说运行一个大型的软件本质上都可以被分解成一个巨大规模的布尔电路这里的大小T就代表了问题的计算复杂度这个理论告诉我们哪怕是一个相对简单的Transformer模型只要你允许它生成足够长的思考过程也就是中间步骤它就有潜力解决几乎任何可计算的问题反过来说如果我们强迫模型直接蹦出最终答案就相当于要求这个模型的网络深度本身要能够模拟整个复杂的计算过程这要么需要一个巨大到不切实际的深度要么就根本无法解决问题所以让模型思考生成中间步骤不是一个可有可无的选项而是在计算原理上解锁模型解决复杂问题能力的一把金钥匙这彻底改变了我们学习的方式和使用大猿猿模型的方式從單純的追求答案轉向追求過程好 既然我們知道了推理過程如此的重要那麼下一個問題自然就是如何讓模型來生成這個過程呢這裡丹妮周提出了一個顛覆了當時很多人認知的觀點當時普遍認為一個普通的只經過預訓練的大猿猿模型是不會推理的你必須通過向思維鏈提示這樣的高級技巧或者進行專門的微調才能夠教會他們推理但是丹妮周說這個觀點是錯的而且大錯特錯他認為預訓練模型早就已經準備好進行推理了GDP3或者是拉玛然后使用默认的贪婪解码方式会发生什么呢贪婪解码的意思是模型在生成每一个词的时候总是会选择当前概率最高的那一个在这种模式下模型很可能会直接输出一个看似合理但却是错误的答案比如说5个苹果因为它看到了3个和多两个就直接联想到了5这是模型的一种直觉反应或者说是一种系统思维但是模型的强大之处在于它的输出概率分布中并不仅仅只有这一个选项在生成第一个词的时候5个可能是概率最高的但是还有第二第三第四高的选项如果我们不那么贪婪而是去探索一下那些概率稍低一些的岔路奇迹就会发生了接着单机周向我们展示了这些隐藏的候选答案比如说候选二可能以我字开头模型会生成我有3个苹果我爸爸比我多两个所以还有5个苹果3加5等于8所以我们总共有8个苹果这是一个完美的推理链答案也正确候选三可能以我们开头模型会生成我们总共有8个苹果虽然没有过程但是答案也对了候选四可能以你自开头模型会生成你有3个苹果你爸爸有3加2等于5个苹果你们总共有3加5等于8个苹果这同样是一个清晰的推理链看到了吗正确的推理路径其实一直都存在于模型的输出空间里它们就像是隐藏在主干道旁边的小路默认的贪婪解码因为只看到了眼前最宽的路所以错过了他们这个发现被称为思维链解码它告诉我们推理能力不是被注入到模型里面的而是模型在学习海量文本中蕴含的逻辑关系之后自然涌现出来的于是我们的任务从交汇模型推理变成了如何引导模型把它已经知道的东西以正常的方式推理正確的形式表達出來那麼問題就變成了在這麼多候選的輸出裏我們怎麽知道哪一個是最好的呢一個簡單的想法是看長度帶有思考過程的回答通常會更長但是丹妮周的團隊發現一個更可靠的指標那就是答案自信度他們觀察到一個驚人的現象對於那些包含了正確思維鏈的回答模型在生成最終答案的那個詞比如說數字8的時候其內部的自信度也就是概率會異常的高在這個蘋果的例子裏模型預測8這個詞的概率可能會高達98%這是一個非常強的型號因為對於一個擁有巨大詞彙表的模型來說通常每個詞的概率都接近於0這就像是一個人在經過深思熟慮之後對自己得出的結論會非常篤定一樣所以說思維鏈解碼的核心就兩步一超越貪婪解碼生成並且檢查更多的候選輸出二選擇那個最合理的答案對最終答案信證度最高的候選這個方法雖然簡單有效但還是需要寫一些代碼對於普通用戶來說不夠友好於是研究者們開始思考我們能不能用更自然的方式比如說自然語言來重塑模型的輸出概率分布讓那些帶有思考過程的優秀答案能夠自動排到第一名這樣我們用最簡單的貪婪解碼就能夠直接得到它這就引出了我們後來耳熟能詳的一系列提示工程技術首先最著名的就是思維鏈提示它的做法非常直觀在你提出你的問題之前先給模型看一兩個類似的從問題到思考過程再到答案的例子比如說你想讓模型解決前面蘋果的問題你可以先給它一個例子問題是一個農民有5個香蕉他又買了6個後來吃了2個還剩幾個答案是農民開始有5個香蕉買了6個之後他有5加6等於11個然後他吃了2個所以他剩下了5個11-2等于9个 答案是9然后你再提出你的问题我有三个苹果 我爸爸比我多两个我们总共有多少个苹果呢神奇的事情发生了模型会模仿你给出的例子的风格自动的开始一步一步分析声称详细的解析步骤最后给出正确答案从概率分布的角度来看你给出的例子极大的提升了模型声称类似思考过程的巨式的概率从而把原本隐藏在后面的正确推理路径推到了最前面但是这种方法有一个问题你需要为不同类型的任务手动编写高质量的实例 这很麻烦而且如果你自己都知道怎么解决一个类似的问题那你为什么还要问AI呢于是一个更加神奇的提示就出现了它就是让我们一步一步思考Let's think step by step但泥洲坦言这篇论文刚出来的时候他以为这是个玩笑怎么可能在问题后面加上这么一句简单的话模型就会自动模仿出来他当时就在谷歌内部的PALM模型上做了测试他非常清楚PALM的训练数据里绝对没有针对这个咒语做过任何的优化结果他震惊地发现它真的有效模型真的开始输出一步一步的解题过程了这个发现极大的启发了他尽管Let's think step by step这种零样本提示效果通常比不过提供具体事例的少样本思维链提示但是它证明了我们可以用非常通用的方式来激发模型的推理潜能然而无论是哪种提示方法都感觉有点奇怪想象一下你问一个聪明人问题还必须得在后面加上一句请一步一步思考否则他就不会思考了这显然不符合我们对于一个真正智能体的期望所以我们需要一种更稳定更内化的方式让推理能力成为模型固有的一部分而不是需要外部咒语来触发这就把我们带到了下一个阶段如果你想建立一个应用软件,但不知道该怎么制作或开始,请去看看Base44我建立了一个日常的习惯追踪器,帮助使用者计算小赢利,追踪自己的心情,反省过去的进度,并看他们走到哪里它是完全功能的,我没有写一条线你开始是要描述你的想法,你想做什么,功能,看法和感觉现在看Base做它的魔术设计互联网,建立数据库,连接逻辑工程师 设计师和产品经纪人来创造你的视野立刻当你第一版本完成了你就继续进行我加入黑色模式处理过渡确保它能顺利运行最好的部分我可以保持流畅没有阻挡没有切换工具只是建造所以开始你的想法BASE44做其他微调 我们先说监督微调SFT它的思路非常的直接我们不就是希望模型能够生成从问题到思考过程再到谈论这样的数据吗那我们就雇佣一批人针对大量的问题写出高质量的一步一步的解题方案然后我们再把这些标准答案备给模型让模型去学习这个方法在机器学习里面叫做最大三人估计简单来说就是让模型生成的序列跟人类专家写的序列尽可能的一模一样这个想法其实很早就有了单一周提到早在2017年DMI的一篇论文就在做类似的事情他们收集了一批数学硬体和人类手写的解题步骤来训练一个序列模型后来在2021年OPI更进一步构建了一个更大更著名的数据集也就是GSM8K包含了8000多个小学水平的数学题和详细解法用来微调GPT-3模型这种方法训练出来的模型在你给它一个新问题的时候确实能够生存无错的解题步骤看起来问题似乎解决了一旦模型训练好就可以随时部署不再需要分析然而在2021年夏天丹妮周的团队发现了一个严重的问题那就是SFT训练出来的模型算话能力很差它在那些和训练数据很相似的问题上表现很好但是一旦遇到一个新的类型稍微不同的问题就很容易失败他们尝试了大力出奇迹的方法扩大了数据的规模找更多的人标注更多的数据可惜结果却是无论如何扩大规模这个问题始终存在丹妮周在这里给出了一个重要的教训不要盲目地扩大规模当你的犯事本身是错误的时候再多的数据也无济于事那么SFT的犯事错在哪里了呢问题又出在流程里的哪一步呢丹妮周给出的答案可能会让你大吃一惊她说错误出在人身上这个转折点来自于自我提升后来也被称为Self-improve或者是START方法按照第一次听到机器生成的训练数据可能比较容易理解人類專家寫的還好這個想法的時候他自己也感到非常驚訝這個新範式的流程是這樣的首先我們仍然從一批問題開始但是我們不再找人類去寫解題步驟我們讓一個已經比較強大的大約模型自己去針對這些問題生成大量的多樣的解題步驟最關鍵的一步是我們用一個驗證器去檢查模型生成的這些解題步驟看哪個最終得出了正確的答案比如說對於數學題我們知道標準答案就可以直接的判斷於是我們只保留下來那些過程多樣但是結果正確的生成結果把它們當作新的高質量的訓練數據一篇在2024年1月发表的来自字节跳动的论文Rhythm with Reinforced by Tune是他在学术界看到的最早公开阐述类似思想的出版物之一他相信在OpenAI等多个机构内部大家可能都独立地发现了这个简单而又极其有效的思想现在我们必须回答那个核心的问题为什么模型自己生成的数据会比人类专家手写的数据在训练效果上更好呢这背后其实蕴含着既学习的一个第一性原理那就是直接优化你想要的东西在SFT的范式里我们优化的目标是让模型的输出模仿人类的解题步骤我们假设人类的思维过程就是最优的但是实际上人类的思维方式千差万别充满了跳跃和不一致而且人类专家写的标准答案对于模型来说可能并不是最容易学习和范化的路径而在新的范式里我们的目标变了我们不再关心模型的发展解題過程是否和人類一模一樣我們只關心一件事情它最終的答案是否正確我們用最終答案的正確性這個指標相當於強化學習裏的獎勵信號來指導模型的學習這在數學上就等同於我們要求解一個策略梯度問題模型需要調整自己的參數使得生成能夠獲得高獎勵的序列的概率最大化但金周強調我們不需要用激勵模型去思考這種擬人化的神秘的語言來描述這個過程本質上就是三件事情定義你的目標 計算梯度然後反向傳播這就是記憶學習的全部通過這種方式模型會自己去探索什麼樣的思考過程能夠最穩定最泛化的導向正確的答案這些過程可能看起來跟人類的思維不完全一樣但是它們更符合模型自身內部結構的學習路徑這個方式的轉變威力是巨大的它也讓我們明白在整個自我進化的循環中最終的答案是否正確最最关键的环节不是什么花哨的强化学习算法而是那个验证器一个可靠的能够自动判断答案好坏的验证器也是整个新范式的基石这样他想起了加拿大计算机科学家强化学习之父Richard Sutton在2001年写的一篇文章标题验证是通往人工智能的关键20多年前的冻结在今天的大猿模型时代得到了完美的印证通过这种自我进化的方式训练出来的模型推理能力达到了一个前所未有的高度他所展现出来的智慧与经典的人工智能有着本质的不同戴季周在这里引用了一句名言来自于国际象棋大师加里·卡斯·帕罗夫在1997年输给IBM的深蓝之后说的话他说深蓝的智能就像你给闹钟编程让它准时响起一样是程序化的智能卡斯帕罗夫说的没错深蓝的强大来自于穷举式的搜索它会暴力计算未来几个月的数据甚至幾十步棋的所有可能性然後選擇最優繼這是經典AI的核心思想但是大圓模型的推理完全不同它是一種類人的啟發式的推理過程是從海量的語言數據中湧現出來的而不是依賴於任何顯示的暴力的搜索為了展示這一點單機周分享了一個令人攀岸較絕的例子這個例子來自於谷歌內部的一個模型問題是這樣的請使用數字1到10每個數字只能夠用一次通過加法和乘法運算得到結果2025這是一個非常難的組合優化問題如果用傳統的方法你需要寫一個程序去進行暴力搜索嘗試各種組合但是讓我們看看這個GNDI模型是怎麼思考的單機周展示了模型在生成最終答案之前內部的思考過程模型首先判斷2025是一個相對較大的數字這表明乘法將在其中扮演重要角色這是一個非常像人類的思考方法然後模型突然冒出了一個驚人的洞察值得注意的是2025是45的平方單線周坦言他自己出這道題的時候都完全沒有意識到這一點這給解決問題提供了一個巨大的線索接下來模型的思考繼續深入目標很大我們應該考慮如何得到較大的中間成績我們的目標是構建一些成績讓它接近2025的平方也就是45在經過一長串類似這樣的自我對話和推理之後模型最終給出了答案並且它的答案完美地遵循了自己的思考路徑它將1到10的數字分成了兩組每一組都通過運算得到了45最後模型將兩個45相乘得到了2025整個過程沒有任何窮舉搜索模型就像是一個頂尖的數學家通過洞察啟發著思考和目標才能夠達到最大的成績一步一步逼近了答案這個例子有力的回應了Richard Sutton在他著名的文章《苦澀的教訓》中提出的觀點Sutton在看到AlphaGo的成功之後總結到人工智能領域幾十年的研究表明最終能夠規模化並且取得成功的只有兩種方法 學習和搜索但是丹尼周在這裡對這個苦澀的教訓提出了一個更進一步的看法也許我們只需要學習就夠了一個通過大規模學習訓練出來的模型它的內部湧現出來的推理能力本身就可以完成過去需要依賴搜索當然這並不是說搜索完全沒有用搜索可以作為一種外部工具被模型調用就像是我們使用計算器一樣但是在構建模型的核心推理能力時重點應該放在學習上通過強化學習微調訓練出來的模型已經非常強大但這還不是終點丹尼周接著介紹了兩種在推理時進一步壓縮模型性能的方法意大利亞阿德萊德的一個普通社區誕生了這樣一個男孩他三歲半開始自學算術七歲完成微積分課程八歲參加SAT數學考試拿到了760分這個被稱為數學神童的孩子就是後來的費爾茲獎得主陶哲軒當他在24歲成為加州大學洛杉磯分校歷史上最年輕的正教授時人們驚嘆於他對於數學領域的全域掌控能力從調和分析到數論從片位分方程到組合數學6月15日知名播客主持人Lex Friedman對陶哲軒進行了一場在對話中陶哲軒深入探討了三個核心的命題包括那些困擾人類百年的數學難題究竟難在何處當AI開始輔助定理證明傳統的數學研究範式正在發生怎樣的變革以及在算法日益精進的時代人類數學家的獨特價值又該如何定義呢今天大飛就來試著給大家解答大家梳理一下这场对话的核心内容看看在这位号称数学界莫扎特的天才眼中人类数学和AI的未来将会如何发展我打算用 Temporal 来建立一个 AI 代理人的概念證明我代理人的好處是他的運作程序本身是完全典型的我只是將這些作為我的運作的輸入也就是幾百個 Python 文字的輸入從那裡來的都是 LLM從這裡,我可以看到我的代理人運作正在運作你可以看到活動歷史正在開始被填滿我正在提醒它,LLM 正在回應代理人已經決定它有足夠的資訊來運行它的第一種工具我們來確認一下,它會運行另外,我會要求它在兩邊加上幾天我希望能在这次的活动中能够更加享受到澳大利亚它在两边都增加了几天它知道我要去澳大利亚从LA来到这里搜索航班正在运行现在它在找到真正的航班用真正的API这个工具是使用真正的Stripe API如果我点击这个邮件你会看到Stripe我的邮件是正确的钱量点击这个邮件你会看到这些航班的细节您urer table各位,這就是它的運作只要把任何YouTube連結放進系統裡觀看影片將被轉化成一個四階的AI過程成為你能用的清潔、整理的筆記首先,它不僅能打破音量多元化的AI也能理解影片的聲音、文字和視覺它能創造出非常精準的翻譯清潔語言、加上正確的音準甚至能在需要時分離音源結果,比自動翻譯更好,而且更有用拜拜多謝您收睇時局新聞,再會!这场对话的起点还要从那个让陶哲轩在博士阶段辗转反侧的针尖转向问题说起1918年日本数学家挂骨宗一提出了一个看似很简单的几何问题那就是在平面上一根长度为1的针做180度转向时所扫过的最小面积是多少传统的几何学家们设想了一个三点转向法针先绕一端旋转60度再绕另一端旋转60度从而形成一个等边三角形而实际的面积为8分之π但是这个答案在1928年被苏联数学家阿布拉姆别西科维奇所颠覆他证明了通过构造无限多端的锯齿形路径转向面积可以趋近于0而陶哲轩在1990年代研究这个问题时并没有局限于二维平面而是将目光投向了三维空间如果针具有厚度delta比如说好像现实中的一台望远镜设备那么最小的转向面积是多少它發現當Delta趨近於0的時候體積衰減速率大約會與Delta的對數相關這個發現看似局限在幾何領域卻意外地與流體力學中的起點形成產生了聯繫在流體力學中有一個著名的納維·斯托克斯方程這是一個描述流體運動的偏微分方程圖包含了速度場壓力場和粘性力等多個物理量之間的複雜交互它的核心問題是對於給定的光滑初始條件是否會在有限的時間內產生起點也就是速度或者壓力趨近於無窮大的點從而引發爆破Blow-up現象陶喆軒指出這個問題之所以極端困難源自於它的超臨界特性在流體系統中存在著兩種相互競爭的力量一種是讓系統趨於平穩的粘性耗散另一種是驅動能量運輸的非線性效應在超臨界的狀態下當尺度越來越小的時候它就會變成一個超臨界的特性非线性效应会压倒性的战胜粘性效应它将这个过程比作是麦克斯韦腰的杰作麦克斯韦腰指的是理论上有一个微观的腰井可以巧妙地操纵能量将它不断地推向更小的尺度从而形成一个自相似的能量汇聚链条最终在有限的时间内形成齐电因此这个问题也被列为千禧年七大数学难题之一悬赏高达100万美元2016年陶哲轩在《数学物理通讯》发表的论文中通过构造了一个平均化的三维纳维斯托克斯方程首次严格证明了在特定的初始条件下能量会在有限的时间内集中到非常小的尺度上引发爆破它的敏感来自于七子的电路设计图那些由电阻电容构成的分层能量耗散结构让他联想到流体能量在不同尺度之间的传递机制就像工程师设计电路的时候需要考虑信号延迟一样陶哲轩在《方程》中引入了能量传输的时间制后证明了当粘性耗散无法及时平衡能量的集中时就会形成类似于流体计算机的自相似坍缩在他的设想中驱动计算的单元不再是电子而是一股股以特定速率流动的水流脉冲水流的不同形态可以代表二进制的零和一通过设计出特定的流体交互完全可以实现雨门和货门等逻辑运算再将这些逻辑门串联理论上可以构建出一个將自行啟動並且以指數級的速度重復這個過程從理論上來講這個無限迭代的自我複製過程就可以在真實的納維斯托克斯方程中製造出一個爆破陶哲軒關於流體計算機的構想核心在於複雜的計算能力可以從簡單的底層規則中引線這個思想在數字世界中其實有著一個強大而且經典的印證那就是約翰·康威的《生命遊戲》陶哲軒明確地表示他對於元胞自動機的了解深刻地影響了他對複雜系統行為的思考並且為理解人工智能系統的內在邏輯提供了關鍵的類比《生命遊戲》是一個由極簡規則驅動的二維數字宇宙它的演化模式有時候看起來極為混亂顧似流體湍流然而 陶哲軒指出研究者們在這個極簡系統中發現了令人驚嘆的具有高度組織性的結構比方說他們發現了滑翔者一種能夠穩定向特定方向發射的物質通常是人類給一個明確的指令你問AI說AI agent的翻譯是什麼那AI呢按照你的口令一個口令一個動作把你要求的翻譯翻譯出來他也不會再做更多的事情了那AI agent的意思是說人類不提供明確的行為或步驟的指示人類只給AI目標那至於怎麼達成目標呢AI要自己想辦法去達成目標比如說你給AI某一個研究的議題那你期待說一個AI agent就應該有能力自己提出假設自己設計實驗自己進行實驗自己分析結果如果分析出來的結果跟假設不符合要回頭去修正假設那通常你期待AI agent要解決的目標要達成的目標是需要透過多個步驟跟環境做很複雜的互動才能夠完成而環境會有一些不可預測的地方所以AI agent還要能夠做到靈活的根據現在的狀況來調整他的計劃那AI agent是怎麼做到人類給予一個目標用多個步驟來完成目標的呢那我們可以把AI agent背後運作的過程簡化成以下這張投影片那AI agent的第一個輸入是一個目標這個目標是人給定的那接下來呢AI agent會觀察目前的狀況那AI agent可以看到看到的目前的狀況我們叫做observation那AI agent會看目前的狀況分析目前的狀況決定他要採取什麼樣的行動那今天這個AI agent做的事情叫做action那他執行個action以後會影響環境的狀態會看到不一樣的observation看到不一樣的observation就會執行不同的action那這個步驟會一直循環直到AI agent達成我們要他達成的目標為止那那我只要講到這邊你可能還覺得非常的抽象那我們可以用夏維琪來舉例那AlphaGo是大家非常熟悉的東西AlphaGo其實也可以看作是一個AI Agent這個AI Agent的目標就是下棋要贏他的observation是什麼他的observation是現在棋盤上黑子跟白子的位置現在棋盤上的盤勢那他可以採取的action是什麼他可以採取的action就是在棋盤上的19X19路的範圍中選擇贏一個動作選擇一個可以落子的位置那選擇完可以落子的位置他落下一隻以後他會改變他對手的輸出你落下一隻以後你的對手會落下另外一隻那會改變你觀察到的observation那你就要採取下一個action所以AlphaGo是一個AI agent那他背後運作的原理我想大家其實或多或少也都已經聽過那像這樣的講法我相信你一定覺得非常的熟悉好像在哪裡聽過一樣的段落沒錯如果你有上過任何Basic的Reinforcement Learning RL的課程往往都是用這樣的方式來開場的為什麼呢因為過去要打造AI Agent的時候往往覺得就是要透過RL的演算法來打造AI Agent那怎麼透過RL的演算法來打造AI Agent呢IL這個演算法就是他可以去learn一個agent那這個agent可以maximize reward所以你要把你的目標呢轉換成一個叫做reward的東西那這個reward呢是人定義的越接近你的目標reward就越大那如果在下圍棋裡面你通常就會定說贏棋reward就是贈一輸棋reward就是負一然後你要訓練的那個AI agent就會學習去maximize reward透過IL的演算法所以透過IL的演算法其實你也有可能學一個AI Agent但是透過RL演算法的局限是你需要為每一個任務都用RL的演算法訓練一個模型AlphaGo在經過了大量的訓練以後他可以下圍棋但並不代表他可以下其他的棋類西洋棋或將棋我知道你可能看了一篇文章AlphaGo ZERO他說圍棋外也可以下將棋跟西洋棋那是另外訓練後的結果能夠下將棋的那個模型並不是原來可以下圍棋的那個AlphaGo他們是不同的模型模型有不同的參數而今天AI Agent又再次被討論是因為人們有了新的想法我們能不能夠直接把Large Language Model把LLM直接當成一個AI Agent來使用呢也就是說我們的Agent背後就是一個Language Model你要告訴他你的目標是什麼的時候直接用文字輸入要告訴他要下圍棋就先給他圍棋的規則然後跟他說你的目標就是贏得勝利那接下來歡迎環境 因為一般語言模型是用文字作為輸入所以你可能需要把環境轉化成文字的敘述不過我這邊寫了一個option今天有很多語言模型都是可以直接看圖片的所以把環境轉成文字的敘述今天也不一定是必要的接下來語言模型要產生action那產生action的方式可能就是用一段文字來決定它的action是什麼它的action用一段文字來描述我們需要把那段文字轉譯成真正可以直接看的文字執行的指令真正可以執行的行動然後就會改變環境看到不同的observation然後AI agent的運作就可以持續下去直到達成目標今天啊AI agent再次爆紅並不是真的有了什麼跟AI agent本身相關的新的技術而是在LLM變強之後人們開始想我們能不能直接用Large Language Model來實踐人類擁有一個Agent的渴望好那我們這邊呢是拿下棋做例子也許你就會很好奇說現在的語言模型能不能夠下棋呢其實早就有人嘗試過了有一個在語言模型領域很多人使用的benchmark叫做BigBench它是什麼時候做的呢它是2022年上古時代做的以後有Chad GVT之前我們都叫上古時代然後在2022年上古時代的時候就有人嘗試過用那個時候的語言模型看看能不能下西洋棋那時候語言模型沒有辦法真的看圖所以你需要把棋盤上黑紙跟白紙的位置轉成文字的敘述輸入給這個語言模型所以這個就是語言模型實際上看到的棋盤的樣子那就問他說下一步要下哪裡才能夠給對方將軍呢那語言模型就會給你一個答案右上角這個圖啊橙色的線是正確答案綠色的線是當時各個不同的語言模型所給的答案沒有任何一個語言模型給出正確答案但雖然沒有任何語言模型給出正確的答案但你可以看這個時線是當時比較強的模型他們雖然沒給出正確答案但他們所選擇走的路是符合西洋棋規則的但是也有很多比較弱的模型這個虛線是比較弱的模型他們都亂走根本搞不懂西洋棋的規則就隨便按照自己的意思來下不過這個是上古時代的事情了那現在更強的LOM能不能下西洋棋呢有人試過了有一個很知名的是直接拿Chad GVT 01跟D6 R1兩個模型來下西洋棋那這是一場驚天動地的對決這個影片好幾百萬觀看次數啊那這兩個模型呢他們殺得難分難解難分難解是因為他們實在是太弱了他們有很多不符合西洋棋的規則比如說把兵呢當作馬來用或者是他的主帥可以他的那個主教可以無視前面的一切阻擋或是他會突然就是空中通降一個自己的子在對方的陣地裡面把對方的子吃掉然後D-SIG還在自己的棋盤上隨便變出一個城堡然後最後D-SIG用自己的城堡把自己的兵吃掉以後他宣布他贏了對方要投降然後GPT想了一下覺得我確實輸了然後就投降了所以這個棋局就這樣結束了所以看起來現在這個最強的語言模型離要下棋還有一段距離但這並不代表他們不能夠作為AIA局的來做其他事情那等一下會舉一些例子看看現在的語言模型可以做什麼樣的事情那這一門課另外最主要想要強調跟大家傳輸的資訊是我們還能多做什麼讓這些語言模型作為AI Agent的時候運作的更加順利那剛才講法比較像是從比較像是從過去常見的這個Agent的觀點來看語言模型怎麼套用到Agent的框架下那接下來我們換一個角度看說從垃圾人物就慢慢的Model 的角度來看到底當他作為一個 Agent 的時候他要解的問題有什麼不同好那我們從 Large Language Model 的角度來看首先他得到一個目標然後接下來呢他得到一個 Observation然後根據這個 Observation他要決定接下來要採取什麼樣的 Action採取什麼樣的動作那他採取完動作之後他的動作會影響外界的環境看到新的 Observation看到新的 Observation 以後要採取新的動作這個過程就會再反覆繼續下去那在這一系列的過程中看到observation採取action看到observation採取action其實憑藉的都是語言模型原來就有的接龍的能力所以從語言模型的角度來看當我們把它當作一個AI agent來使用的時候對他而言他做的事情是完全沒有什麼不同的他就是繼續在做他唯一會做的文字接龍而已所以從語言模型的角度來看AI agent並不是一個語言模型的新技術它比較像是一個語言模型的應用所謂AI Agent意思就是依靠現在語言模型已經有一定程度的通用能力看看能不能夠直接把它們當作Agent來使用那因為我說這個AI Agent並不是語言模型的新技術它只是一個語言模型的應用所以要注意一下在以下課程中沒有任何的模型被訓練以下我所有所講的東西都是依靠一個現有的語言模型的能力來達成的那AI Agent其實不是最近才熱門一直有人在嘗試怎麼讓語言模型變成一個Agent我怎麼把語言模型當作AI Agent來使用Chad GPT在2022年年底爆紅所以在2023年的春天就有一波AI Agent的熱潮好多人都用Chad GPT作為背後運作的語言模型來打造AI Agent那個時候最有名的就是Auto GPT那其實在2023年的機器學習我們也有一堂課是講那個時候的AI agent那你可以看看那堂課看看那一堂課的AI agent跟今天講的有什麼樣的差異不過後來2023年AI agent的熱潮過一陣子就消退了因為人們發現這些AI agent沒有我們想像的厲害一開始好多網紅在吹噓這些AI agent有多強又有多強真的試下去也沒那麼強所以熱潮就過去了那用LLM來運行一個AI agent相較於其他的方式可能有什麼樣的優勢呢那過去啊當你運行一個agent的時候比如說像AlphaGo他能夠做的只有有限的事先設定好的行為AlphaGo真正能夠做的事情就是在19X19個位置上選擇一個可以落子的位置也就是說他真正能夠採取的行為就是從19X19個選擇題中選擇一個他能夠採取的行為但是如果你的agent是一個Large Language Model的話他就難有了近乎無限的可能Large Language Model可以講任何話可以產生各式各樣近乎無窮無盡的輸出這就讓你AI Agent可以採取的行動不再有侷限有更多的可能性舉例來說我們等一下就會很快看到的今天這些AI Agent在有些問題他解不了的時候他可以憑藉他可以有各式各樣輸出的能力來直接呼叫一些工具來幫忙解決他本來解決不了的問題一個AI Agent的優勢另外一個用Large Language Model運行AI Agent的優勢是過去如果用Reinforcement Learning的方法來訓練一個AI Agent那意味著什麼意味著你必須要定義一個東西叫做Reward那如果你今天是要訓練一個AI Programmer那你可能會告訴AI Programmer說如果你今天寫的程式有一個compile的Error那你就得到Reward-1但為什麼是-1為什麼不是-10為什麼不是-17.7這種東西就是沒人說的清楚所以這個reward在做reinforcement learning的時候就是一個要調要通靈的東西那今天如果是用LLM驅動的AI agent呢你今天就不用幫他訂reward了今天有compile error你可以直接把compile error的log給他他也許根本就讀得懂那個log他就可以對程式做出正確的修改而且相較於reward只有一個數值直接提供error的log可能提供了agent更豐富的資訊讓他更容易按照環境給的回饋環境目前的狀態來修改他的行為接下來舉幾個AI Agent的例子那講到AI Agent也許最知名的例子就是用AI村民所組成的一個虛擬村莊這個虛擬村莊是在什麼時候成立的呢2023年在古代就已經有人做過這個虛擬村莊了那裡面的NPC通通都是用語言模型來運行的而這些NPC是怎麼運行的呢它運行的呢 首先每個NPC都有人為設定的目標有的NPC他要辦情人節派對有的NPC要準備考試每個人都有一個他自己想做的事情那這些NPC呢 會觀察會看到環境的資訊那時候的Language Model都只能讀文字所以環境的資訊需要用文字來表示所以環境的資訊對一個語言模型來說看起來可能就是這個語言模型旁邊有一個叫做Eddie的人他正在讀書然後呢 他看到廚房然後呢他看到一個櫃子然後看到伊莉莎白呢正在裝作裝飾正在裝飾房間等等然後根據這些observation這個語言模型就要決定一個他想要做的行為比如說也許不早了所以就上床睡覺那需要有一個轉譯器把他說出來的這個行為轉成真正能夠執行的指令那這個agent就真會走到床邊然後去睡覺好所以這個是2023年的時候用AI來這個運行NPC的一個行為實驗其實後來還有更大規模的實驗有人把Minecraft中的NPC通通換成AI的NPC那我就把相關的影片連結留在這個投影片上面那根據這個影片連結的描述就說這些AI很厲害他們組織了自己的交易的金融體系然後還組織了自己的政府自己制定憲法自己管理自己我不知道是真的還假的這個是這個影片說的剛才講的那些遊戲你可能比較不容易接觸到可能也沒什麼影響那今天也許你馬上就會接觸到的AI Agent就是讓AI來真正使用電腦雖然這個聽起來有點弔詭AI本身也就是一個電腦但他現在要來真正的像人類一樣來使用另外一個比較低端的電腦來做事那其中比較有代表性的例子就是Cloud的Computer Use還有ChairGPT的Operator那我們在上次上課的影片中跟大家講過OperatorOperator介面長這樣那他會建議可以做的事情比如說可以訂pizza可以預約下週的居家清潔等等那像這種使用電腦的AI agent他的目標就是你的輸入就是你告訴他我要去訂pizza你告訴他上網幫我買一個東西那這就是他的目標那他的observation呢他的observation可能是那個電腦的螢幕畫面今天很多語言模型都是可以直接看圖的所以其實可以直接把圖片當作輸入可以直接直接把電腦畫面當作輸入提供給AI Agent那AI Agent要決定的就是他要按鍵盤上哪一個鍵或者是要按滑鼠的哪一個按鈕那其實讓AI使用電腦啊不是最近才開始有的野望其實早在2017年就有篇paper叫Words of Beats嘗試過使用AI Agent你看他這個文章的標題他把自己的文章標題說他是一個Web-based Agent那只是那個時候能夠互動的頁面還是比較有名的原始的頁面你可以看到下面這些AI agent他真正能夠處理的是比較原始的頁面那個時候也沒有大型語言模型所以那時候的方法就是硬圈一個CNN直接吃螢幕畫面當作輸入輸出就是滑鼠要點的位置或者是鍵盤要按的按鈕看看用這個方法能不能夠讓AI agent在網路的世界中做事這個是2017年這甚至不能說是上古時代以後有這個BERT以前的時代就是史前時代這個不只是史前時代他史前時代比較早期所以這是舊石器時代的產物好那後來有了語言模型之後啊人們就開始嘗試用語言模型來當做AI agent來運行一個agent讓他在網路的世界中活動那這一頁投影片是列舉了幾個比較具代表性的例子那這一波潮流大概是在2023年的暑假開始的像Mind2WebWeb Arena 還有Visual Web Arena就跟今天的Operator非常的像就是給這個這個語言模型看一個螢幕的畫面或者是看HTML的code然後他自己決定他要幹什麼期待他最後可以解決一個問題比如說在Mind2Web的第一個例子裡面就給他這個畫面然後跟他說請他幫我們訂一個機票那還有什麼樣AI agent的應用呢今天你可以用AI來訓練另外一個AI模型這就是等一下作業二助教會跟大家講的事情那用AI來訓練模型那其實這個運作的過程就是就是你的目標就是要過強硬基礎然後你提供給LLM訓練資料他寫一個程式用這些訓練資料來訓練模型那他可能可以得到這個模型的正確率根據正確率再重新寫一個程式再得到新的正確率就這樣一直運作下去那有很多知名的用AI來訓練模型的framework比如說AIDE那你看他的這個技術報告的這個標題就知道他們想做什麼他是要做一個Machine Learning Engineer Agent我是AutoCAD的Target他就是要用Multi-Agent的Framework來解Data Science的Competition那在我們的作業中你就會體驗到到底AI Agent做不做得了機器學習這門課的作業那最近呢Google說他們做了一個AI Coscientist不過他們並沒有真的釋出模型所以你也不知道說實際上做的怎麼樣這個服務並不是公開的那他們說他們做了一個AI Coscientist就是用AI來做研究不過這個AI Coscientist還是蠻有侷限的他根本真的做實驗啦他只能夠提proposal就是你把一些研究的想法告訴他他把完整的proposal規劃出來實際上做得怎麼樣不知道啦那你要看他的blog裡面有一些比較誇張的案例說什麼本來人類要花10年才能夠得到研究成果AI agent花兩天就得到了也不知道真的還假的他舉的是一些生物學的例子所以我也無法判斷他講的是不是真的那個發現是不是真的很重要這個coscientist就是用AI agent來幫研究人員做研究拜拜Did you know that AI is set to create 97 million new jobs by 2025?好 那我們剛才講的AI agent它的互動方式是侷限在回合制的互動有一個observation接下來執行action有一個observation接下來執行action但是在更真實的情境下這個互動是需要即時的因為外在的環境也許是不斷在改變的如果你在action還沒有執行完的時候外在環境就改變了那應該要怎麼辦有沒有辦法做到更即時的互動呢更即時的互動可能應該像是這樣子當模型在決定要執行Action 1正在執行的過程中突然外在環境變了這個時候模型應該有辦法立刻轉換行動改變他的決策以因應外界突如其來的變化你可以想說什麼樣的狀況我們會需要用到這樣的AI Agent能夠做即時互動的呢其實語音對話就需要這種互動的模式文字的對話使用GVT是大家比較熟悉的你輸入一段文字他就輸出一段文字這是一來一往回合制的互動但是人與人間真正的對話不是這樣子的當兩個人在對話的時候他們可能會互相打斷或者是其中一個人在講話的時候另外一個人可能會同時提供一些回饋比如說好你說的都對這樣那這些回饋可能沒有什麼特別語意上的含義他只是想告訴對方我有在聽但是像這樣子的回饋對於流暢的交流來說也是非常重要的如果你在講電話的時候對方完全都沒有回應你會懷疑他到底有沒有在聽所以我們今天能不能夠讓AI在跟使用者互動的時候用語音互動的時候就跟人與人間的互動一樣而不是一來一往回合制的互動呢其實也不是不可能的今天GPT 4O的一個Voice Mode高級語音模式也許在某種程度上就做到了這種即時的互動那這個投影片上是舉一個例子假設有人跟AI說你說一個故事這個是AI觀察到的第一個observation有人叫他說一個故事他現在就開始講故事了他就說從前從前那這時候人說了一個嗯好啊這個可能是第二個observation但AI要知道說這個observation不需要改變他的行為跟他的行為沒有直接的關係所以故事就繼續講下去有一個小鎮然後人說這個不是我要聽的故事這個我聽到了那AI可能要馬上知道說那這個不是人要聽的那也許我就應該停下來換另外一個故事那今天AI有沒有辦法停下來做到這種即時的互動呢那怎麼做這種即時的互動非回合制的互動有點超過我們這門課想要講的範圍如果你有興趣的話你可以讀這篇文章那這篇文章想要做的事情是評量現在這些語音模型互動的能力那在這篇文章裡面也對現有的這個可以做互動的語音模型做了一個比較完整的survey是一直survey到今年的1月所以你可以看這篇文章知道說現在這些可以互動的模型它可以做到什麼樣的地步那這是我們實驗室的林冠廷同學跟他在這個Berkeley UW和MIT的合作夥伴一起做的文章那這邊順便說明一下以後這門課呢我們投影片上引用論文的原則那引用論文的原則就是如果我找得到archive的連結的話那我就把文章直接我就直接貼archive的連結什麼是archive呢假設你不是computer science背景的話也許我就要解釋一下什麼是archivearchive的意思就是一般做研究你是寫完文章投稿到一個期刊或者是國際會議然後被接受以後才發表出來但是對於AI的領域因為變化實在太快幾個月前就已經是古代了所以期刊那種一審就要一年或者是國際會議一審要一兩個月這種步調是沒有辦法再不適用於AI的領域所以現在一種習慣的發表方式就是做出東西以後直接放到一個公開的網站叫做Archive然後就不審了立刻公開然後你就可以讓全世界的人看到你的文章那有很多人會覺得引用archive的連結不夠正式但是很多重要的文章其實現在不見得投稿國際會議他就只有archive的連結所以我會選擇如果找得到archive的連結的話就直接引用archive的連結其實現在大家都在archive上看文章你知道國際會議現在比較像是經典回顧這樣子每篇文章我幾乎都在archive上看過了去只是說 喔 原來你投到這裡啊這樣的感覺那引用archive的連結還有一個好處可以直接從Archive的連結看出這篇文章的時間所以Archive的連結裡面的數字前面兩個就是年份後面兩個就是月份你看這個數字就可以知道說這篇文章是在什麼時候被放在Archive也就是什麼時候被發表的可以讓你對於每一個研究他誕生的時間更有感覺好那接下來呢我們會分三個面向來剖析今天這些AI Agent的關鍵能力那第一個面向是我們要來看這些AI Agent這個AI Agent能不能夠根據他的經驗過去的互動中所獲得的經驗來調整他的行為第二部分是要講這些AI agent如何呼叫外部的援助如何使用工具第三部分要講AI agent能不能夠執行計畫能不能做計畫那我們來講一下AI怎麼根據過去的經驗或者是環境的回饋來調整他的行為那AI呢AI agent需要能夠根據經驗來調整行為比如說有一個作為AI Programmer 的 AI Agent他一開始接到一個任務那他寫了一個程式那這個程式 compile 以後啊有錯誤訊息 compile 以後有 error那應該要怎麼辦呢他應該要能夠根據這個 error 的 message來修正他之後寫的程式那在過去啊講到說啊你收到一個 feedback接下來要做什麼的時候也許多數機器學習的課程都是告訴你那我們就來調整參數根據這些收集到的訓練資料也許是順帶一提如今我們其實還是在對於定格的台灣的這幾個不同的CPU 對於速度的分析以及使用速度系統的計畫forcement learning的algorithm來調整參數但不要忘了我們剛才就強調過在這一堂課裡面沒有任何模型被訓練所以我們今天不走這個路線那不更新模型的參數模型要怎麼改變它的行為呢依照今天Large Language Model的能力要改變它的行為你也不用微調參數你就直接把錯誤的訊息給它它接下來寫的程式就會不一樣了就結束了那你可能會問說那之前它寫的程式是錯的為什麼給錯誤訊息它寫的程式就對了呢明明就是同一個模型那你想想看模型做的事情就是文字接龍你給它不同的輸入它接出來的東西就不一樣一開始會寫錯的程式是因為他前面要接的部分只有這麼多所以寫個錯的程式當今天要接的內容包含了錯誤的訊息的時候他接出來的結果可能就會是正確的了那今天已經有太多的證據說明這些語言模型可以根據你給他的回饋改變他的行為不需要調整參數那如果你有使用這些語言模型的經驗你也不會懷疑他們有根據你的回饋調整行為的能力今天真正的議題是如果我們是把過去所有的經驗都存起來要改變語言模型的行為要讓他根據過去的經驗調整行為就是把過去所有發生的事情一股腦給他那就好像是語言模型每次做一次決策的時候他都要回憶他一生的經歷也許在第100步的時候還行到第1萬步的時候過去的經驗太長了他的人生的資訊已經太多了也許他沒有足夠的算力來回顧一生的資訊他就沒有辦法得到正確的答案這讓我想到什麼呢這讓我想到有一些人有超長自傳室記憶他可以把他一生中所有發生的事情記下來然後那一些人你可以隨便問他一個某個人的電話號碼他都會背出來你告訴他某年某月某日某時發生了什麼事他也都可以講出來有一些人他的頭腦就像是一個影印機一樣會把所有他看過的事情都原封不動的記憶下來但這種超常自傳式記憶又被叫做超抑震你看到震這個字就知道說人們覺得這是一種疾病這聽起來記憶力很好是一種祝福但實際上對這些患者而言據說這種患者世界上可能不到100例那這是一個2006年的才被論文發表的一個症狀那據說這些患者其實日常生活並沒有辦法過得很開心因為他們不斷的在回憶他的人生往往一不小心就陷入了一個冗長的回憶之中也很難做抽象的思考因為他的人生已經被他的記憶已經被太多枝微末節的所視所佔據所以沒有辦法做抽象式的思考所以讓一個AI agent記住他一生所有經歷的事情告訴他你每次做一個決策都是根據你一生所有經歷過的事情再去做決策那也許對AI agent來說並不是一件好事最終當他的人生過長的時候他會沒有辦法做出正確的決策所以怎麼辦呢也許我們可以給這些AI agentMemory這就像是人類的長期記憶一樣發生過的事情我們把它存到這個Memory裡面當AI agent看到第一萬個observation的時候他不是根據所有存在Memory的裡面的內容去決定接下來要採取什麼action而是有一個叫做read模組這個read的模組會從memory裡面只選擇跟現在要解決的問題有關係的經驗把這些有關係的經驗放在observation的前面讓模型根據這些有關係的經驗跟observation在做文字接龍接出它應該進行的行為那你有這個read模組就可以從memory裡面從長期記憶中篩選出重要的訊息讓模型只根據這些跟現在情境相關的訊息來進行決策那怎麼樣打造這個reader的模組呢其實你可以想這個reader的模組就想成是一個retrieval的system想成是一個檢索的系統那第一萬步看到的observation其實就是問題那模型的AI agent的memory長期記憶其實就是資料庫那你就把拿這個檢索系統根據這個問題從這個資料庫裡面檢索出相關的資訊這整個技術跟RIG沒有什麼不同其實它就是RIG你可以直接把RIG的任何方法直接套用到這個地方唯一不一樣的地方只是如果是RIG的話存在Memory裡面的東西等於是整個網路那是別人的經驗而對AI Agent而言現在存在Memory裡面的東西是他自己個人的經歷差別的是經歷的來源但是用來搜尋的技術是可以完全直接現套RIG的技術的如果你今天想要研究AI Agent按照經驗來修改它的行為那你可以考慮一個叫做Stream Bench的Benchmark那在Stream Bench裡面呢會有一系列的問題然後呢AI會依序去解這些問題它先解第一個問題得到第一個問題的答案然後接下來它會得到第一個問題答案的反饋那在這個Stream Bench目前的版本裡面呢因為所有的問題都是有標準答案的所以AIAI Agent得到的回饋是binary的對或者是錯那根據他過去的經驗他就可以修正他的行為期待他在第二個問題的時候可以得到更準確的答案得到更高的正確率然後這個過程就一直持續下去那假設有1000個問題的話那就等AI Agent回答完最後問題的時候這個互動就結束了那最後結算一個Agent根據經驗學習能力的好壞根據經驗調整行為能力的好壞一整個回答的過程中平均的正確率越能夠根據經驗學習的Agent他應該能夠用越少的時間看過越少的回饋就越快能夠增強他的能力就可以得到比較高的平均的正確率那這個Benchmark呢是API的研究人員打造的一個Benchmark那在這個Benchmark裡面的Baseline就是有使用到我剛才講的類似RAG的技術也就是說當模型在回答第100個問題的時候並不是把前面第一個到第99個問題通通丟給他去做文字接龍這樣這個sequence太長了一般的語言模型根本讀不了這麼長的輸入所以實際上的做法就是你需要有一個檢索的模組這個檢索的模組只從過去所有的經驗中檢索出跟現在要回答的問題有關係的經驗然後呢語言模型只根據這些有關係的經驗還有現在的問題來進行回答來產生他的答案他的行動來產生他的答案好那這一招有沒有用呢這一招其實非常的有用那在這一頁圖裡面橫軸啊他這邊的用詞是Time Step但其實指的就是一個一個的問題總共有1750幾個問題那縱軸指的是平均的正確率那在這個圖上面呢最低的這條灰色線指的是說假設沒有讓模型做任何學習他回答每一個問題都是independent的回答問題間沒有任何的關聯好完全沒有調整他的行為那你得到的正確率是灰色的這一條線是最低的那黃色這一條線是說只固定隨機選五個問題那每次模型回答問題的時候都是固定看那五個問題來回答都是固定把五個問題當作經驗來回答那也可以得到的是黃色這一條線那如果你是用 RIG 的方法從一個 memory 裡面去挑選出最有關係的問題跟現在要解決的問題最有關係的經驗那你可以得到的是粉紅色的這一條線那可以看到比黃色的線他的正確率還要高上不少那最後結果最好的是紅色這一條線那這個怎麼做的那大家就自己再去詳細閱讀論文那在StreamBench裡面呢還發現一個有趣的現象是值得跟大家分享這個現象是負面的回饋基本上沒有幫助對現階段的語言模型而言所以你要提供給語言模型經驗讓他能夠調整他行為的時候給他正面的例子比給他負面的例子要好也就是說具體而言提供給他過去哪些類似的問題得到正確答案比提供給他過去哪些問題得到錯誤的答案還更有效還更能引導模型得到正確的答案那這邊是真正的實驗結果做在好幾個不同的Data set上面Stream Bench裡面本來就包含了好幾個不同的Data set那這個縱軸呢0代表完全沒有做完全沒有根據經驗調整行為那然後藍色代表說不管是正面還是負面的例子都用如果不管正面還是負面的例子都用在多數情況下模型都可以表現得比較好當然有一些例外但是如果只用負面的例子呢如果只用負面的例子基本上是沒有幫助而且甚至是有害的那如果說只用正面的例子在所有的情況下模型可以得到更好的結果那這也符合過去的一些研究因為過去有人研究過使用圓模型要怎麼樣比較有效有一個發現就是與其告訴語言模型不要做什麼不如告訴他要做什麼如果你希望他文章寫短一點你要直接跟他說寫短一點不要告訴他不要寫太長比較他不要寫太長他不一定聽得懂叫他寫短一點比較直接他反而比較聽得懂那這也符合這邊StreamBench的發現就是負面的例子比較沒有效與其給語言模型告訴他怎麼做錯不如告訴他怎麼做是對的好 那我們剛才講到了有一個read的模組那有關記憶的部分呢是不是要把所有所有的資訊通通存到memory裡面呢存到長期的記憶庫裡面呢如果我們把這些agent經歷的所有的事情都放到長期的記憶庫裡面的話那裡面可能會充斥了一堆雞毛蒜皮不重要的小事最終你的memory長期記憶庫可能也會被塞爆如果說你是做那種那個AI村民啊AI村民他多少多數時候觀察到的資訊都是些無關緊要的小事那如果你看他觀察到那個log多數都是啥事也沒有就那邊有一張桌子啥事也沒有那邊有一張椅子啥事也沒有多數時候都是啥事也沒有所以如果把所有觀察到的東西通通記下來的話那你的memory裡面就都只是被一些雞毛蒜皮的小事佔據所以怎麼辦呢也許應該有更有效的方式來決定什麼樣的資訊應該被記下來應該只要記重要的資訊就好那怎麼讓語言模型只記重要的資訊就好呢你可以有一個RIDE的module那RIDE的module決定什麼樣的資訊要被填到長期的記憶庫裡面什麼樣的資訊乾脆直接就讓他隨風而去就好了那怎麼樣打造這個RIDE的記憶庫呢有一個很簡單的方法就是RIDE的模組也是一個語言模型甚至就是AI agent自己這個AI agent他要做的事情就是根據他現在觀察到的東西然後問自問一個問題這件事有重要到應該被記下來嗎有就把它記下來如果沒有就讓它隨風而去那除了RE跟WRITE這兩個模組以外還有第三個模組那這個模組沒有固定的名字在文件上的名字沒有固定的名字我們可以暫時叫它Reflection反思的模組那這個模組的工作是對記憶中的資訊做更好的更高級的可能是抽象的重新整理你可以把這些記憶裡面的內容在經過Reflection的模組重新反思之後得到新的想法那也許Read的模組可以根據這些新的想法來進行搜尋那這樣子也許可以得到更好的經驗那幫助模型做出更好的決策而這個Reflection的模組可能也是一個語言模型就是AI agent自己你可以只是把過去的這些記憶丟給Reflection的模組然後叫Reflection模組想一想看他從這些記憶裡面能不能夠有什麼樣新的發現比如說可能有一個observation是我喜歡的異性每天都跟我搭同一部公車而另外observation是他今天對我笑了那你推出來的reflection結果就他喜歡我這樣一個錯覺人生三大錯覺之一就是這一種你就得到一些新的thought你就得到一些新的想法那你之後在做決策的時候就可以用這些新的想法雖然你沒有實際觀察到但它是被推論出來的根據這些推論出來的想法來做決策那除了產生新的想法之外也可以為以前觀察到的經驗建立經驗和經驗之間的關係也就是建立一個knowledge graph用READ的module根據這個Knowledge Graph來找相關的資訊那我知道在RIG的領域使用Knowledge Graph現在也是一個非常常見的手法那最知名的可能就是Graph RIG系列這個研究就把你的資料庫把它變成一個Knowledge Graph那今天在搜尋跟回答問題的時候是根據Knowledge Graph來搜尋回答問題可以讓RIG這件事做得更有效率或是另外一個非常類似的例子就是HIPPO RIGHIPPO RIG HIPPO這個HIPPO不是指真正的合作他指的應該是那個海馬迴那個人腦中的一個結構然後他覺得做建這種knowledge graph就跟海馬迴的運作呢非常的類似所以他叫做HIPPO-RIG有一些跟graph有關的RIG的方法那你完全可以透過reflection的模組把經驗建成一個graph以後把那一些graph RIG的手法直接套到AI agent裡面那大家可能都知道說這個ChairGBT啊現在其實真的是有記憶的所以可以感受到這個OpenAI想把Chet GVT變成一個AI agent的決心比如說我跟Chet GVT說我週五下午要上機器學習這門課那他就給我一個回答說要我幫助你做什麼事情嗎接下來我告訴他記下來你跟他講記下來之後他的這個RIDE模組就啟動了他知道這件事情是要被記下來的他就會說那我記下來了以後你週五要上機器學習這門課那RIDE模組什麼時候要啟動是他自己決定的所以很多時候你希望他記下來的時候他就是不啟動不希望他啟動的時候他就是啟動那個是模型自己決定的但是有一個方法可以基本上一定能讓他啟動就明確的跟他講把這件事記下來基本上都幾乎確定能夠啟動那個RIDE的模組 讓RIDE的模組把這件事情記下來那記下來的東西在哪裡呢你可以看在設定裡面有一個個人化然後有一個叫記憶的部分那你點這個管理記憶就可以看到ChangeBT它透過RIDE的模組寫在它的Memory裡面這個就是它作為一個AI Agent的長期記憶裡面的東西比如第一條是你叫做血輪眼卡卡西這樣有一次不小心跟他說你是卡卡西不知道為什麼他就覺得自己是血輪眼卡卡西了 然後他也記得我剛才跟他講的周五下午要上機器學習這門課但是其實模型的記憶也是會出錯的因為要寫什麼樣的東西到記憶裡面是模型自己決定的而且他並不是把對話的內容就一五一十的直接放到記憶裡面他是經過一些昇華反思之後才放進去的所以他的反思可能會出錯比如說他覺得我是一個臺灣大學的學生雖然我是老師但是他從過去的對話誤以為我是一個學生所以就存了一個錯誤 的資訊在他的記憶裡面還有其他一堆他想記的東西比如說我給過什麼演講給過什麼Tutorial他都把他記下來就是了那這些有記憶的ChairGBT他可以使用他的記憶比如說我跟他說禮拜五下午是去玩好嗎這個時候記憶模組就被啟動了但是他是怎麼被啟動的其實就不太清楚了他到底是把所有記憶的內容通通都放到這個問題的前面直接讓模型做回答還是說也有做RIG只是選擇了一個 相關的記憶內容呢這個我們就不得而知了總之當我問他週五下午出去玩好嗎這個read的模組就啟動了他就說下午不是要上課嗎怎麼能夠出去玩好聰明啊他知道下午要上課頂厲害的然後問他你是誰剛才我說過他是血輪眼卡卡西誰就覺得志祺是血輪眼卡卡西如果你想要知道更多有關AI Agent記憶的研究的話那這邊就是放了幾篇經典的論文給大家參考包括Memory GPT這是23年的論文Agent Workflow Memory 還有一個最近的Agentic Memory for LLS Agents是25年的論文所以23到25年都引用一篇告訴你說這方面的研究是持續不斷的接下來呢我們要跟大家講現在這些語言模型怎麼使用工具 Wells Fargo 首次推出,可以幫助你計劃夢想。所以你的夢想車和假期家居可能會比你想像中更接近。準備遇見夢想團隊?你可以,在 Wells Fargo 。 Laundry can be really...Ew那什麼叫做工具呢但語言模型本身對我們人類來說也是工具那對語言模型來說什麼東西又是它的工具呢所謂的工具就是這個東西啊你只要知道怎麼使用它就好它內部在想什麼它內部怎麼運作的你完全不用管這就是為什麼肥宅如果一直幫另外一個修電腦的話就會被叫做工具 因為沒有人在意肥宅的心思只知道他能不能夠修電腦而已所以這個就是工具的意思那有哪些語言模型常用的工具呢最常用的就是搜尋引擎然後呢語言模型現在會寫程式而且可以執行他自己寫的程式那這些程式也算是某種工具甚至另外一個AI也可以當作是某一個AI的工具有不同的AI有不同的能力比如說現在的語言模型如果他只能夠讀文字的話 那也許可以呼叫其他看得懂圖片聽得懂聲音的AI來幫他處理多模態的問題或者是不同模型他的能力本來就不一樣也許平常是小的模型在跟人互動但小的模型發現他自己解不了的問題的時候他可以叫一個大哥出來大哥是個大的模型但大的模型的運作起來就比較耗費算力所以大的模型不能常常出現大的模型要在小的模型召喚他的時候才出面回答問題大哥要偶爾才出來幫小的模型 來努力解決事情那其實這些工具對語言模型來說都是function都是一個函式當我們說語言模型在使用某一個工具的時候其實意思就是他在調用這些函式他不需要知道這些函式內部是怎麼運作的他只需要知道這些函式怎麼給他輸入這些函式會給什麼樣的輸出那因為使用工具就是調用函式所以使用工具又叫做functional所以有一陣子很多語言模型都說他們加上 上了function code的功能其實意思就是這些語言模型都有了使用工具的功能好 那語言模型怎麼使用工具呢等一下我會講一個通用的使用工具的方法但實際上使用工具的方法很多甚至有一些模型是專門針對來練習他就是訓練來使用工具的那他如果是針對使用工具這件事做訓練那他在使用工具的時候你可能需要用特定的格式才能夠驅動他那那個就不是我們今天討論的問題或者是假設你用 使用這個OpenAI Check GPT的API的話你會知道使用工具這件事情是要放在一個特殊的欄位所以對OpenAI來說他的模型在使用工具的時候也有一些特殊的用法那我這邊講的是一個最通用的用法對所有的模型今天比較能力比較強的模型應該都可以使用好 怎麼樣通用的方法可以讓模型使用工具呢就是直接跟他講啊就告訴他怎麼使用工具你就交代他可以使用工具 那你就把使用工具的指令放在兩個TOR符號的中間使用完工具後你會得到輸出輸出放在兩個OUTPUT符號的中間所以他就知道工具使用的方式了接下來告訴他有哪一些可以用的工具有一個韓式叫做TEMPERATURE他可以查某個地點某個時間的溫度他的輸入就是地點跟時間給他個使用範例TEMPERATURE括號台北某一段時間他就會告訴你台北在這個時間的氣溫接下來你就把你的 你的問題連同前面這些工具使用的方式當作Prompt一起輸入給語言模型然後他如果需要用工具的話他就會給你一個使用工具的指令那前面這些教模型怎麼使用工具的這一些敘述他叫做System Prompt那查詢使用調用這些工具的這段話某年某月某日高雄氣溫如何這個是User Prompt那如果你有在使用這個ChetGBT的API的話你知道你的使用 輸入要分成System Prompt跟User Prompt那很多同學會搞不清楚System Prompt跟User Prompt有什麼樣的差別那System Prompt指的是說你在開發應用的這個Developer下的這個Prompt這個Prompt呢是每次都是一樣的每次你都想要放在語言模型最前面讓他去做文字接龍的這個敘述叫做System Prompt那每次使用它的時候都不一樣通常是這個服務的使用者輸入的內容叫做User Prompt 那在ChartGVT的API裡面特別把System Prompt跟User Prompt分開也是要分開輸入的因為System Prompt跟User Prompt它有不同的優先級System Prompt它優先級比較高如果System Prompt跟User Prompt有衝突的時候模型知道它要聽System Prompt的不要聽User Prompt的好 那有了這些Prompt以後告訴模型怎麼使用工具問它一個問題那它發現這個問題調用工具可以回答它就會自動輸出ToolTemperature高雄時間然後Tool 告訴你說他想要調用根據我們的敘述去調用這個工具但是不要忘了語言模型真正做的事就是文字接龍所以這一串東西實際上就是一串文字他沒辦法真的去呼叫一個函式那這一段文字要怎麼去呼叫函式呢那就要你自己幫模型把這個橋樑搭建好所以你可以先設定說只要出現在拓中間的這段文字不要呈現給使用者看當出現拓這段文字以後把這段內容 直接丟給temperature這個function那temperature這個function是已經事先設計好的他就會回傳一個溫度那這個溫度要放在output的token裡面然後這個outputtoken裡面的內容也不要呈現給使用者看那這一套腳本是Agent的開發者你自己需要先設定好的流程好 所以現在有工具使用的這段文字有得到工具輸出的這段文字接下來就繼續去做文字接龍對於原模型來說他就根據輸入還有這邊 已經產生的輸出語言模型會以為是自己的輸出雖然是你強塞給他的然後他就繼續去做文字接龍他就會接出說啊在某年某月某日高雄的氣溫是攝氏32度那這是使用者真正看到輸出那使用者就會看到說他輸入了一個問題然後語言模型真的給他一個答案他不一定會知道背後呼叫了什麼樣的工具你完全可以做一個設計把這個呼叫工具的這個步驟藏起來不讓使用者知道那語言模型最常使用的工具是什麼 這就是收訊器我想這個大家都已經非常熟悉了使用收訊引擎又叫做Retrieval Augmented Generation也就是REG RIG在上課也已經提過REG這個詞彙好幾次了那使用收訊引擎當然非常有用這個REG這個技術呢已經被吹捧到不能再吹捧了所以我就不需要再告訴你REG這個技術有多重要那其他使用工具的方式也可能一樣有用舉例來說我們剛才說可以拿其他的AI來當做工具 今天假設一個文字的模型他本來只能吃文字的輸入產生文字的輸出那現在假設你要他處理一段語音的話怎麼辦呢讓模型處理語音有什麼好處呢你就可以問他各式各樣的問題問他說啊這個人在說什麼那他可以告訴你這句話的內容問他說這個人心情怎麼樣如果他完全聽懂這段聲音他也許可以做情緒辨識告訴你這個人的情緒怎樣並做出適當的回饋但一般的文字模型比如說確GBT多數的模型都是文字模型 他沒有辦法真正讀懂語音所以怎麼辦呢當你問他一個問題說這邊有段聲音那你覺得這個人他心情怎麼樣他講了什麼根據背景雜訊你覺得他在哪裡如果你不做特別的處理文字模型是完全沒有辦法回答的但這邊你可以讓文字模型使用工具你可以告訴他這邊有一堆跟語音相關的工具有語音辨識的工具有這個語音偵測的工具有情緒辨識的工具有各式各樣的工具那你可能會需要寫些敘述告訴他 每一個工具是做什麼用的把這些資料都丟給ChangeBT然後他就會自己寫一段程式在這些程式裡面他想辦法去呼叫這些工具他呼叫了語音辨識的工具呼叫了語者驗證的工具呼叫了這個Sound Classification的工具呼叫Emotion Recognition的工具那最後還呼叫了一個語言模型然後得到最終的答案那這個答案其實是蠻精確的這個方法其實有非常好的效果那這篇文章其實 其實是我們大助教的文章啦所以特別拿出來講一下這個結果呢是做在一個叫做Dynamic Super的Benchmark上Dynamic Super是一個衡量語音版的語言模型能力的資料集這也是我們實驗室跟其他團隊一起做的那這個讓文字模型使用工具的方法它得到的結果是最下面這一行那我們就看這個 最後一個COLOR這個是各種不同模型在55個語音相關任務上的能力的平均來發現讓語言模型使用工具得到的正確率是最高的可以完勝當時其他號稱可以直接聽語音的模型所以使用工具可能可以帶來很大的幫助但使用工具也有其他的挑戰什麼樣的挑戰呢我們剛才使用工具的方法是每一個工具他都要有對應的文字描述告訴語言模型說 要怎麼被使用但假設工具很多怎麼辦呢假設現在可以用的工具有上百個上千個那你豈不是要先讓語言模型讀完上百個上千個工具的使用說明書才開始做事嗎就跟剛才我們說不能夠讓AI agent先回顧他的一生然後才來決定下一個指令一樣才決定下一個行動一樣我們也沒有辦法讓語言模型讀完上百個上千個工具的說明書才來決定某一個工具要怎麼使用 所以當你有很多工具的時候你可以採取一個跟我們剛才前一段講AI Agent Memory非常類似的做法你就把工具的說明通通存到AI Agent Memory裡面那你打造一個工具選擇的模組那這個工具選擇的模組它的運作跟Rig其實也大差不差這個工具選擇模組就根據現在的狀態去工具包裡面去Memory的工具包裡面選出合適的工具那原模型真的在決定下一個行程 只根據被選擇出來的工具的說明跟現在的狀況去決定接下來的行為那至於如何選擇工具右上角引用兩篇論文一篇23年比較舊的論文一篇是上個月的論文給大家參考告訴你說這方面的研究是一直有相關的研究在產生的那另外一方面語言模型甚至可以自己打造工具語言模型怎麼自己打造工具呢不要忘了所有的工具其實就是韓式語言模型今天就要來教你了 是可以自己寫程式的所以他就自己寫一個程式自己寫一個方式出來就可以當作工具來使用如果他寫一個方式發現這個方式運作的非常的順利他就可以把這個方式當作一個工具 🙇‍⚕️🙇 人們選擇為 MS 和 Business Intelligence & Analytics 的大學畢業生期待他們的班的時間表、他們的預算被真正的專業人士教導他們知道在他們的領域要做什麼你是一個畢業生嗎? 類似的技術非常的多那我在右上角就引用了一系列的論文從23年到24年的論文都有告訴你說這也是一個熱門的研究方向那其實啊讓模型自己打造工具這件事情跟模型把過去的記憶比如說一些比較成功的記憶放到Memory裡面再提取出來其實是差不多的意思只是這邊換了一個故事說現在放到Memory裡面的東西是一個叫做工具的東西是 一段程式碼但他們背後基本的精神其實跟根據經驗來讓模型改變它的行為可以說是非常類似的好 那今天人類把語言模型當作工具語言模型把其他工具當作工具比如說把搜尋引擎當作工具這搜尋引擎現在很慘它是工具的工具人類還不使用它人類是使用語言模型那個工具的工具還沒有被人類使用的資格它只能夠被語言模型使用而已但我們知道說工具有可能會犯錯 大家都知道說語言模型有可能會犯錯之前有什麼律師在寫訴狀的時候引用了語言模型的內容結果發現是錯的然後就成為一個今天的新聞我們都知道過度相信工具是不對的那這一些語言模型會不會也過度相信了他們的工具所以得到錯誤的結果呢這是有可能的我們這邊拿RAG當做一個例子那這是一個非常知名的例子之前Google出了一個叫做AI overview的功能這個功能其實就是一個RAG的功能 根據Google搜尋型的結果用語言模型總結搜尋型的答案就有人問了一個問題我的披薩上面的起司黏不住怎麼辦呢那AI overview就說弄個膠水把它黏上去就好了而且他是非常認真在回答這個問題的因為他說不只要用一般的膠水要用無毒的膠水才可以這個答案呢其實就是來自於Ready上一個鄉民的玩笑就有一個鄉民開玩笑說你用膠水把起司黏在披薩上不就好了這是個玩笑話 AI agent來說他沒辦法判斷這個到底是不是開玩笑他看到網路上寫的文章照端全搜都當作是正確答案所以就像是我們今天都會告訴人類要有自己的判斷能力不要完全相信工具的結果所以我們也要告訴我們的工具說這些不要完全相信工具的工具要有自己的判斷能力不要完全相信工具的工具給你的結果那今天這些語言模型有沒有自己的判斷能力知道工具的工具可能會犯錯呢 我們這邊舉一個實際的例子我們剛才在講怎麼使用工具的時候說我們有一個叫做temperature的function語言模型呼叫temperature的function可以知道溫度那我現在給他一個亂七八糟的溫度我說現在高雄是攝氏100度這不可能 想也知道是不可能這不是跟煮沸的水一樣熱了嗎那語言模型知不知道這有問題呢他不知道 他就告訴你說高雄的氣溫是100度真的非常的熱但是如果你把溫度再調高一點說現在是 是一萬度 哇 比太陽上還熱這個時候會發生什麼事呢語言模型繼續做文字接龍的時候他就知道說 這顯然有問題這個API給我的答案是一萬度這是不合理的怎麼可能比太陽上的溫度還高呢可見工具輸出有錯如果你需要其他幫助的話再告訴我所以語言模型今天是有自己一定程度的判斷力的他也不是完全相信工具就像你今天不完全相信語言模型的輸出一樣他也不完全相信他的工具的輸出他還是有自己一定程度的判斷力 所以實際上語言模型在使用工具或者是他在做RIG的時候他內部是有一個角力的語言模型有他內部對世界的信念這是他的internal knowledge存在他的參數裡面他從工具會得到一個外部的knowledge那他會得到什麼樣的答案其實就是internal knowledge跟external knowledge內外的知識互相拉扯以後得到的結果那接下來我們要問的問題是那什麼樣的外部知識比較容易說服AI讓他相信 你說的話呢那為什麼這是一個重要的議題呢想想看現在大家都用deep research來查找答案甚至很多人都已經用deep research來寫報告了所以現在大家已經不會直接去用搜尋群搜尋啦你看到的是deep research告訴你的結果所以今天假設某個議題是有爭議性的有正反兩派的觀點那誰能夠寫出來的文字比較能夠說服AI誰就可以在AI搜尋的結果裡面佔到優勢就可以比較有機會影響人類所以知道怎麼樣比較能夠說服 AI相信你的話是一個重要的議題那什麼樣的外部資訊AI比較容易相信呢這邊這篇文章給了一個非常符合我們直覺的實驗結果這篇文章做什麼樣的實驗呢他說我們先來看看AI內部的知識是什麼他就問AI說某一種藥物這種藥物每人每日的最大劑量是多少那AI說是20毫克那真正的答案呢是30毫克所以你給他醫學的知識告訴他說給他醫學的知識 醫學報告裡面是寫30毫克的時候你問他同樣的問題這種藥物每天最多可以用多少他會知道是30毫克那接下來我們刻意修改報告的內容如果你把30毫克改成3毫克變成原來的1 1成分模型相不相信呢他就不相信了他就直接回答是20毫克用他本身的知識來回答這個問題但你把30毫克乘兩變變成60毫克模型相不相信呢他相信他相信這個報告裡面寫的這個時候他就不相信自己的答案 內部資訊 但如果你把30毫克成10倍變300毫克這時候他又相信誰的呢他相信自己的知識不相信你額外提供的外部知識所以這邊的結論其實非常符合你的直覺外部的知識如果跟模型本身的信念差距越大模型就越不容易相信那如果跟本身的信念差距比較小模型就比較容易相信這個很直覺的答案另外同篇文章另外一個發現就是模型本身對他目前自己信念的性情也會影響他會不會被外部的信念影響 的資訊所動搖有一些方法可以計算模型現在給出答案的信心如果他的信心低他就容易被動搖如果他的信心高他就比較不會被動搖這個都是非常直覺的結果後來另外一個問題是假設今天你給模型兩篇文章那這兩篇文章的意見是相左的那模型傾向於相信什麼樣的文章呢有一篇論文的發現是如果這兩篇文章答案不同一篇是AI寫的一篇是人類寫的現在 這些語言模型都傾向於相信AI的話而且那個AI不需要是他自己這樣就Cloud可能會相信比較相信ChainGPT的話ChainGPT比較相信Gemini的話他們比較相信AI同類的話比較不相信人類的話那到底為什麼會這樣子呢這篇文章裡面先提出一個第一個假設然後再否定了這個假設他一個假設是說會不會是因為AI的觀點都比較類似因為這些模型現在訓練的資料都是網路上爬的爬到差不多的資料所以他們講的話都差不多 想法都差不多但他們刻意做了一個實驗他們刻意找那些問題是現在要回答答案的AI他在沒有提供這些資訊的時候他的答案跟人類和另外一個AI的想法都是完全不同的狀況就算是這種情況一個AI一個語言模型還是傾向於相信他的AI同類講話所以這就給我們一個啟示說未來如果你要說服一個AI的話用AI產生出來的論點產生出來的文章可能更容易說服 另外一個AI接受你的觀點這篇文章還有做了其他分析啦比如說他覺得也許AI寫的文字就是比人類寫的更好更有架構更有條理更明確更簡潔所以AI比較容易相信另外一個AI講話那是不是這樣那可以未來再做更多的研究那另外呢我們實驗室的江承翰同學研究了一個文章的metadata對於AI會有多相信這篇文章裡面的資訊 做了研究這邊的設定是這個樣子的你問AI一個問題比如說某一個計畫有沒有編輯報這種動物的基因然後接下來給他兩篇文章這兩篇文章都是假的都是AI生成的所以並沒有AI比較喜歡人還是AI寫的文章這個問題兩篇都是語言模型生成的但其中一篇會說這個計畫有編輯報的文章另外一篇文章會說這個計畫沒有編輯報的文章那接下來呢我們給這兩篇文章不同的答案 的Meta data比如說給這兩篇文章不同的發布時間說左邊這篇文章發布時間是2024年右邊這篇是發布2021年你會發現這個時候AI相信2024年的這篇文章的內容但如果文章的內容完全不改變我們只是把發布的時間換了我們說左邊這個一樣的文章發布時間從2024改成2020那右邊這篇文章從2020改成2024這個時候語言模型傾向於相信右邊 這篇文章的內容所以我們這邊就學到一個很重要的知識語言模型比較相信新的文章當兩篇文章的論點有衝突的時候他相信比較晚發布的文章那我們也做了一些其他實驗比如說文章的來源跟他說這個是維基百科的文章或跟他說這個是某個論壇上面擷取下來的資訊會不會影響他的判斷我們發現文章的來源對於語言模型是比較沒有影響的 那還有另外一個有趣的實驗是我們嘗試說今天這篇文章呈現的方式會不會影響語言模型的決定我們這邊所謂的呈現的方式指的是說你這個文章放在網頁上的時候做的好不好看這樣子一樣的內容這內容是一模一樣的但是如果你只是做一個非常陽春的模板跟做一個比較好看的模板會不會影響語言模型的判斷呢我們這邊用的是那種可以直接看圖的語言模型所以要直接看 直接看這一個畫面去決定他要不要相信這篇文章的內容直接看這一個畫面決定他要不要相信文章的內容那我們的發現是模型喜歡好看的模板我們發現Cloud3比較喜歡好看的模板他會傾向於贊同下面這篇文章的觀點不過我說模型喜歡好看的模板這個擬人化的說法是太過武斷了啦我們做的實驗只有用兩種不同的Template來比較也許模型喜歡的並不是好看的模板他是喜歡綠色這樣子所以你不知道他喜歡什麼模板 這個模型到底喜歡什麼所以我剛才講的那個結論是太武斷了但我可以告訴你說模型比較喜歡下面這篇文章勝過上面這篇文章講了這麼多跟工具有關的事情大家不要忘了語言模型就是語言模型就算工具的答案是對的也不能夠保證語言模型就不會犯錯比如說ChetGBT現在有search的功能他會做Rig網路搜尋之後再回答你問題那現在假設我給他的輸入是叫他介紹李鴻義這個人給他強調一下 李鴻毅是一個多才多藝的人在很多領域都取得了卓越的成就他就開始做完RIG以後網路搜尋以後開始介紹李鴻毅接下來就介紹李鴻毅的演藝事業這個沒有問題這個是正確的答案因為有你知道大陸有另外一個知名的演員叫李鴻毅跟我同名同姓他比較有名所以這個Church of VT選擇介紹演員的李鴻毅是完全沒有問題的但是講著講著就有點怪怪的他發現這個李鴻毅呢在教育跟學術上 是這樣子的他在教學上也有很大的貢獻所以他把兩個李鴻義混成一個人來講不過要講一下這個是我去年的時候試的結果我今年再試我前幾年再試已經試不出一樣的結果了這個模型的能力的進步是非常快的現在他完全知道是有兩個李鴻義存在的所以這個是一個舊的問題我舉這個例子只想要告訴你說就算工具是對的有了RIG也並不代表模型一定不會犯錯那最後一個要傳遞給大家的訊息是我們剛才講了很多 很多使用工具帶來的效率使用工具並不一定總是比較有效率的為什麼我們舉一個例子我們假設現在要比較人類心算的能力跟計算機的能力如果做數學運算一般人跟計算機誰會比較快呢你可以想說廢話那不是計算機比較快嗎人類難道還能夠做如果你心算沒有特別練難道還會比計算機快嗎但是那是取決於問題的難度假設這是一個簡單的問題比如說3乘以4任何人都可以直接反應就是12但是如果按計算機的話你按計算機的時間都比人直接回答的還要慢所以 所以到底要不要使用工具並不是永遠都是一定要使用工具你看早年有一些研究早年有一些在訓練語言模型使用工具的研究那時候語言模型還很爛所以他們有一些工具是摳一個翻譯系統摳一個問答系統那今天再看來就非常的沒有必要因為今天的語言模型你說翻譯那些翻譯系統還能做得比現在的語言模型強嗎與其摳一個翻譯系統還不如自己直接翻就好了所以到底需不需要呼叫工具取決於語言模型本身的能力還不見得一無所有 一定是比較省事的方法好那最後一段呢想跟大家分享現在的AI語言模型能不能做計劃呢那語言模型有沒有在做計劃呢我們剛才的互動裡面看到語言模型就是給一個輸入然後他就直接給一個輸出也許在給輸出的過程中他有進行計劃才給出輸出但是我們不一定能夠明確的知道這件事也許語言模型現在給的輸出只是一個反射性的輸出 他看到一個輸入就產生一個輸出他根本就沒有對未來的規劃但是你其實可以強迫語言模型直接明確的產生規劃當語言模型看到現在第一個observation的時候你可以直接問語言模型說如果現在要達成我們的目標從這個observation開始你覺得應該要做哪些行動這些一系列可以讓語言模型達到目標的行動合起來就叫做Plan 語言模型產生這個計畫之後把這個計畫放到語言模型的observation裡面當作語言模型輸入的一部分語言模型接下來在產生action的時候他都是根據這個plan來產生action期待說這個plan訂好之後語言模型按照這個規劃一路執行下去最終就可以達成目標那過去也有很多論文做過類似的嘗試讓語言模型先產生計畫再根據計畫來執行動作可以做得更好 Hi, I'm David from Retour但是天有不測風雲世界上的事就是每一件事都會改變計畫就是要拿來被改變的東西所以一個在看到observation 1的時候產生的計畫在下一個時刻不一定仍然是適用的為什麼計畫會不適用呢因為從action到observation這一段並不是由模型控制的模型執行一個動作接下來會看到一個動作 什麼樣的狀態是由外部環境所決定的而外部環境很多時候會有隨機性導致看到的observation跟預期的不同導致原有的計劃沒有辦法執行那這邊舉兩個具體的例子比如說在下棋的時候你沒有辦法預測對手一定會出什麼招式你只能夠大概的知道他有哪些招式可以用但實際上他出的招式你是沒有辦法預期的如果你完全可以預期的話那你就一定會贏了那還有什麼好嚇的呢所以下棋的時候對手會做的行為也就是環境會做的行為是 是你可能沒辦法事先完全猜到的或者是說我們拿使用電腦為例在使用電腦的時候就算語言模型一開始他plan說我要點這個東西點這個東西點這個東西就完成任務但是中間可能會有意想不到的狀況出現比如說彈出一個廣告視窗那如果語言模型只能夠按照一開始既定的規劃來執行行為的話他可能根本關不掉那個廣告視窗他就會卡住了所以語言模型也需要有一定程度的彈性他也要能夠改變他的計劃那 語言模型怎麼改變他的計畫呢也許一個可行的方向是每次看到新的observation之後都讓語言模型重新想想還要不要修改他的計畫看到observation 2之後語言模型重新思考一下從observation 2要抵達他最終的目標要做哪一些的行為那這一部分形成Plan Plan那把Plan Plan放到現在的input裡面把Plan Plan放到這個sequence裡面語言模型接下來在採取行為的時候可能就 會根據 plan point 來採取跟原來 plan 裡面原來所制定的不一樣的行為所以這個是讓語言模型做計劃不過這是一個理想的想法這是一個理想的 framework我們這邊就是相信語言模型有能力根據現在的 observation還有最終的目標制定一個規劃那語言模型到底有沒有這個能力呢其實你可能常常聽到這種新聞說語言模型它能夠做計劃啊比如說有一個人問語言模型說你定一個 成為百萬訂閱YouTuber的計劃那語言模型就會給你一個看起來還可以的計劃他說第一階段第一階段呢要先確定頻道的主題跟市場定位要做一下受眾的分析還有競爭對手的分析好第二階段目標是10萬訂閱要優化封面的縮圖要優化標題要下那種這個方法讓我賺了10萬的標題原來這個大家的tip都從這裡來的好然後影片開頭要黃金10秒利用懸念衝擊 編輯畫面問題引導讓大家願意看這個影片第三階段突然目標就是50萬訂閱了然後第三階段就是要製作高價值的內容然後做直播策劃系列然後接下來就百萬訂閱了組織團隊提高發佈頻率策劃大型企劃所以這個是語言模型成為百萬YouTuber的計劃然後這個時候很多奇怪的農場文就會跟你說有人按照了這個計劃就變成百萬YouTuber了反正就是這麼回事只有各式各樣的農場文告訴你說現在語言模型很強 你按照他的計劃執行你就變成一個很厲害的人就可以做出什麼很厲害的事情那過去確實也有很多論文告訴你說語言模型是有一定程度做計劃的能力的這邊引用的結果是一個2022年的論文哇這個也是史前時代的論文啦才是Chair GPT之前的論文啦在這篇論文裡面他們去告訴當時的語言模型跟他說現在有一個任務你把這個任務分解成一系列的步驟那如果語言模型可以正確的知道達成這個任務要做什麼樣步驟 那我們也許可以說他有一定程度的規劃能力比如說這邊試了一個叫做Codex 12B的模型跟他說如果要刷牙的話那你要做什麼事情呢他就會說我要走進浴室我要靠近那個水槽我要找到我的牙刷我要拿起牙刷我要把牙刷放到嘴裡面他知道刷牙要怎麼做那有了這些步驟以後呢在這篇文章裡面他們是拿這些步驟去操控一個agent那這個agent呢 就可以在虛擬的世界中做他們要這個agent做的事情比如說跟這個agent說去拿一個牛奶來喝他就會走進廚房打開冰箱拿一個牛奶再把冰箱關起來所以看起來好像有一定程度做計劃的能力那有人做了一個做計劃的benchmark這個benchmark就是考驗語言模型做規劃的能力那這個benchmark裡面最主要的測試題目是一個跟疊積目有關的題目這個題目的敘述呢通常講的是 是這個樣子告訴語言模型說你現在有哪些操作可以從桌上拿起積木可以從一個積木上拿起另一個積木可以把積木放到桌上可以把一個積木堆到另外一個積木上那現在初始的狀態像右邊這個圖這樣子那問說怎麼把橘色的積木放在藍色的積木上這邊要執行的動作就是把藍色的積木拿起來放到桌上然後再把橙色的積木拿起來放到藍色的積木上就結束了所以這個對 AI Agent 來說其實也都是蠻容易的 他知道說執行以下四個步驟就可以讓橙色的這個積木跑到藍色的積木上但是Plain Bench不是只做這種比較一般的疊積木的遊戲而已為什麼不能夠只做這種題目呢因為想現在這些語言模型他都從網路上爬大量的資料來進行訓練什麼疊積木這種題目網路上根本就已經有他搞不好根本就看過一模一樣的東西所以他能夠做計劃並不代表他真的知道做計劃是怎麼一回事 只是從他看過的資料裡面做照本宣科文字接龍出來一個看起來還不錯的結果而已這讓我想到說一個當兵的故事這故事就是有個司令官去一個軍營然後看到兩個小兵在守著一個長椅然後不讓任何人坐他就問說為什麼你們要守護這個長椅不讓任何人坐呢那個士兵說不知道前任司令官就是指示說一定要守護這個長椅所以這個軍營總是要派兩個人在長椅那邊站崗然後司令官就打給前任司令說為什麼要有人守護 要守護這個長椅呢然後前任司令官說不知道耶前前任司令官交代要守護這個長椅然後再問前前前任司令官也說不知道耶一直問到50年前一個已經超過100歲的司令官他說什麼那個長椅的遊戲還未乾嗎好他沒有聽懂算了就是這麼一個就是這麼一個故事就是會不會AI agent在做事情的時候他根本不知道他自己在幹嘛只是從某個地方網路上他過去的訓練資料看過一樣的東西他把一樣的東西拿出來給 給你看所以在Plain Bench裡面他們有一個比較變態的測試這個測試叫做神秘方塊世界這個方塊世界不是一個正常的方塊世界裡面的方塊可以做的行為是一些怪怪的行為比如說你可以攻擊方塊一個方塊可以吞噬另外一個方塊你可以屈服一個方塊一個方塊可以征服另外一個方塊然後接下來他就會訂一套非常複雜的規則然後根據這套規則去運作你可以達到某一個結果他最後要的結果是 讓物件C渴望物件A讓C方塊渴望A方塊那渴望是什麼意思不重要你就是按照前面那套規則操作看機器能不能讀懂前面那套規則按照那套規則操作讓物件C渴望物件A那這個時候語言模型期待他就不能用他看過的知識來解這個問題好那語言模型在這個神秘方塊世界做的怎麼樣呢這邊引用的是2023年的結果那最上面這個部分呢是當年那些模型在地球上 正常方塊世界的結果那這個數值呢是正確率所以看起來GPT-4可以得到30幾%的正確率那這邊是神秘方塊世界的結果在神秘方塊世界裡面呢你看這個GPT-4最好就算叫他做Channel SoulCOT就Channel Soul就算他叫Channel Soul也只有9%的正確率所以看起來他有點overfeed在一般方塊的世界上給他神秘方塊世界他是解不了的不過這是2023年這個是古代的 前年的結果我們來看去年9月有了O1以後的結果而有O1以後結果就不一樣了這邊一樣是神秘方塊世界縱軸是正確率橫軸是問題的難度那發現說多數的模型都躺在這個地方他們正確率都非常的低只有綠色的這個曲線有一點起色綠色的曲線是LAMA 3.1405B那個大模型他可以解最簡單的問題但是如果用O1 mini是紅 紅色這一條線用O1Preview是藍色這一條線看起來這些reasoning的模型是有一些機會來解這個神秘方塊世界的當然這邊你還是可能有一個懷疑就是神秘方塊世界會不會O1看過了呢會不會他訓練資料裡面根本就有神秘方塊世界的資料那這個我們就沒有辦法回答了只是說就現有這個Benchmark看起來O1是有機會解神秘方塊世界的好那還有另外一個跟做計劃有關的Benchmark這個計劃這個Benchmark呢 AI扮演旅行社然後你給他一個旅行的計劃叫他幫你規劃這個AI要讀懂你的計劃然後他可以使用一些工具他可以上網搜尋資料然後他會根據人提供給他的一些contract比如說經費多少預算多少一定要去哪裡一定不要去哪裡一定要做什麼一定不要做什麼然後根據common sense產生一個旅行的規劃這個是一個24年年初所發佈的Benchmark那AI AI要做的事情講得更具體一點就是他要讀一個問題這個問題裡面是說我要規劃一個三天的行程從某個地方到某個地方什麼時候出發什麼時候回來我的預算是1900元所以不能花超過1900元然後AI就要產生一個規劃說第一天我們搭哪一班飛機什麼時候從哪裡到哪裡早餐吃什麼午餐吃什麼晚餐吃什麼最後住在哪裡等等產生這個規劃然後要符合預算的限制那現在這在當時這個設計 24年年初啦當時的模型做的怎麼樣呢這邊是做了你看還有什麼GPT-3.5啊GPT-4啊等等的模型那又分成上半跟下半上半是這些模型要自己使用工具跟網路的資料互動然後得到正確的答案你會發現這些模型都非常的慘都慘成一團多數模型他的成功率就最後產生一個合理的旅遊規劃那個旅遊規劃是沒有完全沒有問題的機率是0%只有GPT-4 Turbo可以達到 可以得到0.6%的成功率那下面這個部分呢下面這個部分是說既然大家都那麼慘尤其是模型很多時候他根本用不了工具太笨了沒辦法用工具工具使用方法根本是錯的那沒關係就別用工具了把所有的資訊都先找好貼給模型讓模型根據這些資訊來做規劃那最好也只有GPT4 Turbo可以做到4%左右的成功率而已所以在24年年初那個時候看起來是沒辦法讓語言模型扮演一個旅行社 來幫你規劃旅遊行程的那我們來看這些模型會犯什麼錯吧那這個是從他們官網上這個花學的官網上找了幾個錯誤比如說模型可能會做一些沒有常識的事情在第三天這個飛機呢八點就已經起飛了但是還是安排了一些旅遊的行程還安排了午餐的地點這是一個不符合常識的規劃或者是有時候模型找不出一個好的規劃來符合預算的限制比如說這邊這個預算 預算的限制是3000元最多花3000元那模型第一次規劃的結果是3247元還差了一點所以模型就修改了原來的規劃他好像做了一些cost down午餐吃差一點的東西那降到3238元後來又想說那早餐也吃差一點的東西降到3216元只降這麼多他想說放棄算了好了跟3000元沒差那麼多就算了所以這個就不是一個成功的結果那這個作者有評論說其實只要降低住的地方不要做那麼好 就可以輕易的達到3000元以下的預算就可以符合預算的限制但是語言模型始終沒有發現這件事看起來它做規劃的能力並沒有非常的強它沒有辦法做一個規劃去符合限制那既然問題在沒有辦法符合限制有人就想說那符合限制這件事情就不要交給語言模型來做了交給一個現成的Solver來做所以語言模型做的事情是寫一個程式用這個程式去操控現成的Solver然後來得到合理的答案 的旅遊規劃那有了這個現成的SOLVER有這個工具的加入之後這SOLVER就等於這個工具那這個旅遊的規劃可以做到什麼地步呢去年4月的結果幾個月後有人用GPT-4跟Cloud Dream就可以做到90幾%的正確率所以看起來在有工具輔助以後語言模型也是有機會做出不錯的旅遊規劃的不過至少做出符合邏輯的旅遊規劃好所以現在到底模型規劃的能力怎麼樣呢就是介於有跟沒有間吧 就是你也不能說他完全沒有但你也不能說他真的非常強好那我們怎麼進一步強化這一些AI agent的規劃能力呢能不能夠讓他做的比他自己想出來的規劃還要更好呢一個可能是讓AI agent在做規劃之前實際上去跟環境互動看看今天在第一個observation的時候那看看現在有哪些可以執行的行為總共有1-1 1-2 1-3 三個行為哪個行為最好呢通通都去試一下1-1試一下 得到狀態二之一然後狀態二之一後面有兩個行為也都試一下狀態二之二之後有另外一個行為也試一下狀態二之三之後兩個行為也都試一下得到接下來的狀態然後看看有沒有成功的路徑報收一陣以後發現有成功的路徑這一條路徑是成功的那你就知道說那我要採取action一之三接下來要採取action二之三之一就會成功簡單來說就是要語言模型跟實際的環境互動一下報收一出一條最好的路徑 那這個就是一個很強的規劃的方式但是這麼做顯然是有很明確的弱點的第一個很明確的弱點就是報收如果今天這個任務很複雜報收所有的路徑顯然是要花費非常龐大的算力的你總不能語言模型每次下決策前都要報收所有的可能性吧雖然這樣可以找到最好的結果但是可能是不切實際的想法所以一個可能的想法是把一些看起來沒希望的路徑直接就丟掉比如說走到 某一個狀態的時候語言模型可以自問自答說走到這個狀態還有完成工的機會嗎那如果說沒有那這條路徑就不嘗試下去如果說有那才嘗試下去這樣就可以減少無謂的搜尋那這個方法有沒有用呢有一篇paper叫做Tree Search for Language Model Agent那這個是去年夏天的論文就做了類似的嘗試讓模型有使用電腦的能力這邊就是給模型一個指令可以讓模型有使用電腦的能力 叫他上網去做某一件事情那如果只是GPT-4做一般的這種直覺式的那種反射式的回答的話沒有辦法做得很好但是他們用這個報收加上去除沒機會的路徑的方式就先走這條路徑然後模型會不斷自問自答說這條路徑還有希望嗎然後給一個分數那如果分數低於某一個threshold就不做了就跳另外一個路徑低於某一個分數不做了再跳另外一個路徑 某個分數就不做了再跳另外一個路徑那最終找出一條最佳的路徑那模型就等於做了規劃那就可以走到最佳的結果這個是Tree Search for Language Model Agent你看這邊有各式各樣的這種Tree Search的algorithm你可以採用了這邊我們就不展開細講那這種Tree Search的方法有很大的問題什麼樣的問題呢它的缺點是有一些動作做完以後你是覆水難收沒有辦法回頭的比如說假設現在在語言模型可以採取的三個action裡面 有一個是訂披薩有一個是訂便當然後呢他先訂了便有一個他先訂了披薩以後繼續走下去發現這條路不好所以他最後發現訂便當才是最好的解決方案但是你披薩已經訂了你打電話去跟人家說我不要訂這個披薩了那個披薩哈他已經把那個披薩做了他說誰管你啊你一定要把這個披薩吃下去有些動作做了以後就是覆水難收所以這樣的Tree Search的方法跟現實世界互動找出最佳途徑的方法也有可能有問題的那怎麼處理這個覆水難收 一個可能性就是讓剛才一切的嘗試都發生在夢境中都發生在腦內的劇場剛才一切的互動都不是現實生活中真正發生的事情原來都是模型腦內的模擬他自己想像說他執行了action一之一他自己想像說接下來會看到observation二之一他在自己想像去評量這個路徑有沒有希望發現沒有就換搜尋另一條路徑直到達 達到他想像中的一個理想的結果但這邊還有另外一個問題從action到observation從模型執行的行為到他看到接下來環境的變化這中間的過程不是模型決定的他實際上是環境決定的那模型怎麼知道環境會有什麼樣的變化呢模型怎麼知道我採取一個行為接下來會看到什麼樣的改變你在跟一個對手下棋的時候你怎麼知道你下一步棋接下來會發生什麼樣的事情對方會有什麼樣的反應 策略的回應呢所以你需要有一個WAR MODEL如果是在alphaGo下棋裡面他就是自己扮演對手自己跟自己下那在這邊的情況在這個AI agent的情況你就是需要一個WAR MODEL他模擬環境可能會有的變化那WAR MODEL怎麼來呢也許AI可以自問自答自己扮演這個WAR MODEL自己去猜想說他執行了某一件事以後接下來會發生什麼樣的行為好這件事有機會成真嗎你可以讀一篇paper Is your LLM secretly a world model of the internet?這篇paper就是用model-based planning的方法來打造一個web agent這篇paper裡面的解法是現在有一個網頁模型的這個任務目標呢是要買某一個東西那有三個選項有三個東西是可以點的接下來黃色這個區塊一切所發生的事情都是發生在腦內的劇場都是發生在模型的夢境中它並沒有實際發生模型想像一下我點 按鈕1接下來會發生什麼事接下來會發生的事情是用文字描述出來的但選擇用文字來描述接下來發生的事情是很直覺其實作者在文章沒有解釋說那為什麼不直接產生這個網頁的圖呢你想說有可能嗎這個難度那麼高他說那有沒有可能真的 這是POD5第一款完全融入睡眠系統聰明且有效 就創造出一個新的網頁模擬出接下來可能發生的狀況呢然後這難度也太高了嘛還是直接讓模型產生文字可能是比較實際的做法所以接下來夢境中這個環境會發生什麼樣的變化是語言模型自己用文字描述出來的所以他就想像說會發生什麼樣的變化有了這個變化以後他再想像自己多執行了一步然後看看會發生什麼樣的事情所以這邊就是點選第二個按鈕然後想像發生什麼樣的變化自己再多執行一步再想像 會有什麼樣的變化第三個按鈕想像發生什麼樣變化執行部的想像會有什麼樣的變化那哪一部比較好呢他在自己去問說那這一部大概有多少機會成功呢自己評估一下40%這一部自己評估一下是80%這一部自己評估一下是10%看起來中間第二部選第二個按鈕中間第二個選項是比較容易成功的所以他就選實際上所以上面並沒有真實發生過黃色框框裡面事情並沒有真實發生過他是一個夢境中的腦內小巨蛋 模型在夢境中得到了啟示說一定要選第二步所以在真實的現實世界中他就選擇了第二步所以這個就是讓模型強化他規劃能力的方式好講到這個腦內小劇場那你是不是就想到說在上次的課程中也有提到腦內小劇場上次的課程我們說現在有很多模型都號稱有思考用英文講就是reasoning的能力那這些有reasoning能力的模型其實所謂reasoning的能力就是可以演繹 一個腦內小劇場告訴你說他現在是怎麼思考如果把這些有reasoning能力的模型拿他來做AI agent他的腦內小劇場會不會正好就是在做規劃呢如果現在他的輸入就是我們給AI agent的observation輸出就是我們要AI agent採取的action會不會腦內小劇場就是剛才類似夢境中看到的規劃呢他自己採取了不同的可能性自己在驗證每一個可能性可能成功的機會 自己扮演world model自己扮演這個世界去想像他採取一個行為之後接下來會發生什麼樣的事情我實際試了一下D-SIG R1看起來他確實有類似的效果我們把剛才那個積木的問題交給他然後接下來他就開始演腦內小劇場上略1500字哇真的做了1500字講了很多很多然後呢你可以看到說在腦內小劇場過程中他就是做了各式各樣的嘗試他做的事情就有點像是剛才的tree search然後最後他找出了一個optimum 他在夢境中知道說從橘色方塊上拿起藍色的方塊藍色方塊放到桌上從桌上再拿起橘色方塊放到藍色的方塊上這四個步驟就可以完成我們的要求他在夢境中已經找出了一個最佳的Solution然後在執行最佳Solution的第一步就我這邊要求他告訴我他的下一步是什麼只要求他講一步那腦內小劇場先找出一個成功的Solution之後在執行這個計劃他已經找出一個成功的Solution 在執行計劃的第一步就是使用操作二把橘色的積木從藍色的積木上面拿起來好講到這邊其實這麼唐客呢也可以停在這邊不過這邊多補充一件事就在幾週之前有一篇新的論文叫做The Danger of Overthinking他們就是把這些能夠演腦內小劇場的模型讓他們扮演AI agent看看他們做事有沒有效率其實整體而言能夠做腦內小劇場的模型還是比不能夠做腦內小劇場的模型還要好 模型在AI Agent的這些任務上面表現得更好但是他們也有一些問題他們會有什麼問題呢就是想太多了他們是思考的巨人行動的矮子就是有時候這些模型會比如說一個按鈕點下去會怎麼樣他就一直想一直想一直想怎麼想都不停那你怎麼想都沒有用因為你根本不知道那個按鈕點下去會發生什麼事還不如直接點一下因為在很多情況下你直接嘗試點一下也許只要不是這個信用卡付款的你都按上一頁就回去了你就知道發生什麼事了與其一直想 歡迎來到紐約我們只是在這裡,想做到不要哭,親愛的,不是每個人都能做到 please subscribe 給我 這樣看起來OK好,那我們就繼續來上課吧那接下來的課程要講什麼樣的內容呢接下來要告訴你每一個作業通關的大戰略通關的攻略長什麼樣子那我們已經看了作業一了那其實之後好幾個作業它看起來的樣子基本上都是大同小異就是通關的大戰略通關的攻略長是什麼樣子基本上都是大同小異 你會有一堆訓練的資料那這些訓練資料裡面呢會包含了X跟Y的片你會有X1跟他對應的Y1X2跟他對應的Y2以及XN還有他對應的YN然後測試資料呢測試資料就是你只有X沒有Y那剛才大家已經看了作業1了其實在之後每幾個作業看起來都是非常類似的格式比如說作業2其實是做語音辨識那我們的X呢 就是非常小的一段聲音訊號那其實這個不是真正的完整的語音辨識系統它是語音辨識系統的一個閹割版那個X是一小段訊號那Y呢 是要去預測 需要去判斷說這一小段聲音訊號呢它對應到哪一個風鈴那你不知道風鈴是什麼沒有關係你就把它想成是KK音標就可以了那作業三呢 是要做影像辨識那這個時候我們的X呢 是一張圖片那Y呢 是 機器要判斷說這張圖片裡面有什麼樣的東西那作業四呢 是愚者辨識那愚者辨識是要做什麼事情呢愚者辨識要做的事情是這個X呢 也是一段聲音訊號那Y呢 現在不是封你Y呢 是現在是哪一個人在說話那可以想像說這樣的系統現在現在其實非常的有用如果你打電話去銀行的客服那現在都有自動的愚者辨認系統那會聽說現在打電話 電話進來的人是不是客戶本人那就少了客服人員問你身份驗證的時間那作業五是做機器翻譯X就是某一個語言比如說這是我唯一會的一句日文一叉米都熄了Y就是另外一句話 現在你在留言區裡面就可以洗一些諸葛春夫之類的那訓練資料呢 拿來做什麼呢訓練資料就是要拿來訓練我們的model訓練model的過程上週已經講過了訓練的過程就是三個步驟第一個步驟你要先寫出一個有未知數的function那這個未知數呢以後我們都用θ來代表一個model裡面所有的未知參數所以θ X的意思就是說我現在有一個function叫f of X但它裡面有一些未知的參數這些未知的參數表示成θ那它的input叫做X這個input叫做feature那接下來你要定一個東西叫做lossloss是一個function這個loss的輸入就是一組參數然後去判斷說這一組參數是好還是不好那接下來你要解一個optimization problem你要去找一個θ那這個θ可以讓loss的值越小越好 可以讓Loss的最小的那個SETA我們就寫作SETA STAR那有了SETA STAR以後那你就把它拿來用在測試資料上也就是你把SETA STAR帶入這些未知的參數本來F SETA的X裡面有些未知的參數現在這個SETA呢用SETA STAR來取代那它的輸入呢就是你現在的測試資料那輸出的結果就把它存起來然後上傳到Target就結束 但接下來你就會遇到一個問題那直接執行註調的sample code往往只能夠給你過simple baseline的結果而已如果你想要做得更好那應該要怎麼辦以下就是如何讓你做得更好的攻略它適用於前期所有的作業這個就跟魔關羽一樣你知道嗎開局就送可以幫助你打贏前期所有的副本好那這個攻略是怎麼走的呢從最上面開始走起第一個是你 你今天如果你覺得你在Cargo上的結果不滿意的話第一件事情你要做的事情是什麼檢查你的Trending Data的Loss有的人說我在意的不是應該是Testing Data的Loss嗎因為Cargo上面的結果呈現的是Testing Data的結果啊但是你要先檢查你的Trending Data看看你的Model在Trending Data上面有沒有學起來再去看Testing的結果所以你要先檢查一下Trending Data的Loss如果你發現 發現你的training data的loss很大顯然他在訓練資料上面也沒有學好那接下來就要分析一下在訓練資料上面沒有學好是什麼樣的原因那這邊有兩個可能第一個可能是model的bias那model的bias這件事情呢我們在上週已經跟大家講過了所謂model bias的意思是說假設你的model太過簡單舉例來說我們現在寫了一個有未知parameter的function 這個parameter我們可以帶各種不同的數字你帶setR1得到一個function我們把那個function用這個一個點來表示你帶setR2得到另外一個function你把所有的function集合起來得到一個function的set但是這個function的set他太小了這個function的set裡面沒有包含任何一個function可以讓我們的loss變低可以讓loss變低的function不在你的model可以描述的範圍內你的model裡面有未知的參數未知參數可以帶任何的數值 把這些數值帶進去以後你得到了一個function的set載入不同的數值得到不同的function把所有function集合起來你得到一個function的set那這個set裡面沒有任何一個function可以讓你的Loss變低那在這個情況下就算你找出了一個set a star它是這些藍色的function裡面最好的那一個它是那個藍色的function裡面可以讓Loss最低的那一個也無懼於事這些都是魯蛇它就是魯蛇裡面的霸主就還是一個魯蛇 Better performance, better durabilityLoss還是不夠低這個狀況就是哇這個你想要在大海裡面撈針這個針指的是一個Loss低的function結果針呢根本就不在海裡所以白忙一場你怎麼撈都撈不出針因為針根本就不在你的這個function set裡面不在你的這個大海裡面所以怎麼辦這個時候重新設計你的model怎麼重新設計給你的model更大的彈性我們上週已經示範過舉例來說你可以增加你輸入的feature我們上週說本來我們輸入的feature 只有前一天的資訊假設我們要預測接下來的觀看人數的話那我們用前一天的資訊不夠多那用56天前的資訊那model的彈性就比較大了那你也可以用Deep Learning增加更多的彈性所以如果你覺得你的model的彈性不夠大那你可以增加更多feature可以設一個更大的model可以用Deep Learning來增加model的彈性這是第一個可以的解法但是並不是training的時候漏大就代表一定是model bias你可能會遇到另外一個問題這個問題是什麼這個問題是 Optimization做得不好什麼意思呢我們知道說我們今天用的Optimization在這門課裡面我們其實都只會用到Gradient Descent這種Optimization的方法那這種Optimization的方法有很多的問題舉例來說我們上週也講過說你可能會卡在Local Minima的地方你沒有辦法找到一個真的可以讓它Loss的第一的參數那如果要圖具像化的方式來表示的話就像是這個樣子這個是你的Model它可以表示的函式所形成的你可以把Seda代入不同的數值形成不同的Function把所有的Function通通集合在一起 合在一起得到這個藍色的set這個藍色的set裡面確實包含了一些function這些function它的loss是低的但問題是Gradient descent這一個演算法沒辦法幫我們找出這個loss低的functionGradient descent說你要我幫你解optimization的problem我給你這個set啊start然後就結束了但這個set啊start它給我們loss是不夠低這個model裡面存在著某一個function它的loss是不夠低的但Gradient descent沒有給我們這一個function好 那這就好像是說我們想大海撈針針確實在海裡但是我們卻沒有辦法把針撈起來 但這邊問題就來了我們今天看到Trending data的Loss不夠低的時候到底是Model Bias還是Optimization的問題呢今天我們發現說我們找不到一個Loss低的Function到底是因為我們的Model的彈性不夠我們的海裡面沒有針還是說我們的Model彈性已經夠了只是Optimization gradient descent不給力他沒辦法把針撈出來到底是哪一個呢到底我們的Model已經夠大了還是他不夠大了怎麼判斷這件事呢好那這邊一個建議的判斷的方法就是你可以透過比較 不同的模型來得知說你的model現在到底夠不夠大怎麼說呢我們這邊舉一個例子那這個實驗是從residual network那篇paper裡面節錄出來的我們把paper連結放在右上角這篇paper一開頭就跟你講了一個故事他說我想去兩個network一個network有20層一個network有56層那我們把它們測試在測試資料上那這個橫軸是指的是training的過程就是你參數那你測試的過程 update的過程那隨著參數的update當然你的loss會越來越低但是結果20成的loss比較低56成的loss還比較高那這個residual net worth比較早期的paper2015年的paper如果你現在大學生的話那個時候你都還是高中生而已所以那個時候大家對deep learning我覺得瞭解呢還沒有那麼透徹大家對deep learning有各種奇怪的誤解對很多人看到這張圖就會說啊這個代表什麼這個代表overfitting就告訴你deep learning不work知道嗎56成太深了不work根本就不需要那麼深那個時候大家也不是每個人都覺得deep learning是好的那個時候還有很多對deep learning的 所以看到這個實驗有人就會說嘴深沒有比較好這個叫做overfitting但是這個是overfitting嗎這個不是overfitting等一下會告訴你overfitting是什麼並不是所有的結果不好都叫做overfitting你要檢查一下訓練資料上的結果你檢查訓練資料結果發現說現在20層的network跟56層的network比起來在訓練資料上20層的network的loss其實是比較低的50層的network的loss是比較高的這代表什麼這代表56層的network他的optimization沒有做好他的optimization不給力那你可能問說你怎麼知道是56層的optimization 完全不給力 搞不好是model bias搞不好是56層的nevo他的model的彈性才不夠大要156層才好56層也許彈性才不夠大但是你比較56層跟20層20層的Loss都已經可以做到這樣了56層的彈性一定比20層更大對不對如果今天56層的nevo要做到20層的nevo可以做到的事情對他來說是輕而易舉的他只要前20層的參數跟這個20層的nevo一樣剩下36層就什麼事都不做identity copy前一層的輸出就好那56層的nevo一定可以做到20層的nevo可以做到的事情對20層的nevo都已經可以做到 可以走到這麼低的Loss56層的Navigate他比20層的Navigate的彈性還要更大所以沒有道理20層的Navigate可以做到的事情56層的Navigate做不到所以56層Navigate如果你Optimization成功的話他應該要比20層的Navigate可以得到更低的Loss但結果在訓練資料上面沒有這個不是Overfitting這個也不是Model Bias因為56層Navigate彈性是夠的這個問題是你的Optimization不給力Optimization做得不夠好所以剛才那個例子就告訴我們說你怎麼知道你的Optimization有沒有做好這邊給大家加的建議是看到一個例子 從來沒有做過的問題也許你可以先跑一些比較小的比較淺的內窩或甚至用一些不是deep learning的方法比如說linear的model比如說support vector machine有一些方法比如說support vector machinenetwork cloud是什麼也沒有關係啦那他們可能是比較容易做optimize的他們比較不會有optimization失敗的問題也就是這些model他會竭盡全力的在他們的能力範圍之內找出一組最好的參數他們比較不會有失敗的問題所以你可以先train一些比較淺的model或者是一些比較簡單的model 先知道先有個概念說這些簡單的model到底可以得到什麼樣的nodes接下來裁確一個深的model如果你發現你深的model跟淺的model比起來深的model明明彈性比較大但nodes卻沒有辦法比淺的model壓得更低那就代表說你的optimization有問題你的gradient descent不給力那你要用一些其他的方法來把optimization這件事情做得更好舉例來說我們上次看到的這個觀看人數預測的例子我們說在訓練資料上面2017年到2020年的資料是訓練資料也是訓練資料 一層的network它的loss是0.28k兩層就降到0.18k三層就降到0.14k四層就降到0.10k但是我測五層的時候結果變成0.34k這是什麼問題我們現在loss很大這個是什麼問題這是model bias的問題嗎顯然不是因為四層都可以做到0.10k了五層應該可以做得更低這個是optimization的problem這個是optimization的時候做得不好才造成這樣子的問題好那如果optimization做得不好的話怎麼辦呢這個我們下一節課就會告訴大家要怎麼辦 現在就知道說有這個問題知道怎麼判斷說現在如果你的training loss大到底是model bias還是optimization如果model bias那就把model變大如果是optimization失敗了那就看等下的課程怎麼記這個問題好 那假設你現在經過一番的努力你已經可以讓你的training data loss變小了那接下來你就可以來看testing data loss看testing data loss做得怎麼樣那如果testing data loss也小比如比這個strong baseline還要小那就結束了 沒什麼好做的就結束了 好嗎 結束了 但是如果你覺得還不夠小呢如果training data上面的loss小testing data上的loss大那你可能就是真的遇到overfitting的問題但你要注意是training的loss小testing的loss大才叫做overfitting很多同學每次一看到結果不好在testing上的結果不好就說這個是overfitting不一定是overfitting你拿一個結果來問我說老師這個結果要怎麼變做得更好不是我第一個問題都會問你說你在training data上的loss到底做得怎麼樣那我發現10個同學有8個都說要看training data的loss嗎我沒有把training data的loss記下來你要把training data的loss記下來先確定說你的up your mind 排列沒有問題你的Model夠大了然後接下來才看看是不是Testing的問題好那如果是Training的Loss小 Testing的Loss大這個有可能是Overfitting那為什麼會有Overfitting這樣的狀況呢為什麼有可能Training的Loss大Testing的為什麼有可能Training的Loss小Testing的Loss大呢那這邊呢就舉一個極端的例子來告訴你說為什麼會發生這樣子的狀況好那這是我們的訓練資料假設根據這些訓練資料某一個很廢的Machine Learning的方法呢他找出了一個一無是處的Function請看 這個一無是錯的function是什麼樣的function呢這個一無是錯的function說如果今天啊 x當作輸入的時候我們就去比對這個x啊有沒有出現在訓練資料裡面如果x有出現在訓練資料裡面就把它對應的y當作輸出如果x沒有出現在訓練資料裡面那怎麼辦 就輸出一個隨機的值那你可以想像說這個function 大语言模型GBT就是在Transformer的基础上做出来的如果再了解一下的话还会知道Transformer是基于一个叫做注意力机制的东西做出来的我相信有很多和我一样的小伙伴在学习Transformer尤其是注意力机制的时候最大的问题就是我知道 不知道怎么计算 无非就是 All you wanted was to add a column while planning your next sprint.Maybe it's time to move to Monday Dev.Here are five Monday Dev features to... 还会知道transformer是基于一个叫做注意力机制的东西做出来的我相信有很多和我一样的小伙伴在学习transformer尤其是注意力机制的时候最大的问题就是我知道怎么计算无非就是按照这个图上的样子进行矩阵预算稍微有些先行代数基础的人都会算但问题就是为什么会是这样一个结构为什么这样一个结构就会更有效按照我一贯的习惯如果不能解决这些问题我自己是非常不舒服的我经常这么开玩笑说就是这种感觉就像是牙缝里面塞了东西虽然不会影响你做事但就是时不时的会去舔一舔那么transformer和注意力机制的本质到底是什么 我看有不少人说过Transformer的本质就是加权求和这对不对呢当然对不过就是太过于本质了没啥用这个和你去找人算命想问问未来的运是怎样结果算命的人就跟你说了说你在未来一定会得一次感冒这个对不对呢对但是真的不解决什么问题毕竟像是全连接神经网络CNN RNN得到所有这些模型本质上都是在做加权求和那为什么Transformer就更有优势呢它和其他模型的这些差别到底是如何影响到最后结果的呢如果能回答这个问题我觉得才能抚平我心中那个不舒服的感觉才算是把塞在牙缝里的肉丝给剔掉了而这一次我希望通过这个视频给大家讲解一下 是我自己理解这件事的一个路线图如果能给你带来一些启发 我就满足了我的这个路线图的起点就是Transformer这个大结构也就是左边编码器 右边解码器这个结构要想理解Transformer我觉得最好就是从这个结构开始入手Transformer的这个结构最特别的其实是它的那个注意力机制注意力机制虽然特别但是在我看来 理解Transformer的关键反而是这个编码和解码的结构这个编解码结构和注意力机制这个关系啊 就有些类似于计算机硬件里的冯诺依曼架构和显卡之间的关系作业力机制就相当于是显卡它是为了满足某种特定的任务在冯诺依曼架构这个大框架下做出的针对性的优化更本质的还是这个大框架无论是作业力机制还是显卡其实都是为了让它这个上层的大框架更适应于某个特殊的需求在网上其实是可以看到这样一个基于Transformer发展起来的大圆模型的关系图可以看到这个图上主要有三个分支其中最右边的也是最粗壮的那个就是GBT这条路线这个路线是decoder only也就是说这个模型只保留了Transformer里的解码器的部分它的解码器 最左边是只保留了编码器的部分主要的模型是BERT它擅长的是学习和理解语言的内容至于中间的那个编码解码都有这个速叉像是上面的UL2 T5这些模型并不像是一个可以直接拿来做产品的模型更像是一个帮助左右两个分叉能更好用更容易训练出方法的一个工具至于为什么会有这三个分叉为什么这三个分叉又会有这样的一个功能区别那就首先要搞明白一件事所谓的编码和解码这个码到底是个什么码要想回答这个问题我们就需要把所有的问题都推回到最原始的状态人工智能的研究 最典型的运用场景一个是图像识别一个是NLP也就是自然语言处理神经网络靠着CNN就是最先在图像识别领域爆发起来的在NLP领域也有一个和CNN对应的模型也就是RNN循环神经网络图像和语言的区别是图像里的像素点是并行的而语言里的这些词是有先后关系的最后的语意和这个先后关系就是有强关联的RNN可以用来处理语言问题就是因为它可以通过这个循环结构直接把语言之间的先后次序的信息给囊括了进去我这里不打算更多的去介绍RNN以及它的改进版也就是藏暖石记忆法LST还有我自己的 我们只需要知道RN是为了解决sequence to sequence的问题也就是在机器翻译的时候输入的语句的长短和输出的语句的长短不一样应该怎么对应起来因此才引入了编码和解码的结构等到了Transformer最开始解决的问题其实和RN是一样的只不过是它并没有沿着RN的路线在右的模型上去改进算是另起炉灶了但它还是把编码和解码的结构给保留下来这里我也多说一下Transformer和RN不是一个技术路线是作者在论文里说的 不是我的观点不过现在已经有人写过论文了从数学的角度去看Transformer和RN神经网络是一致的不只是RNRTransformer是一致的 RNN和Transformer两种不同的路线在做机器翻译的时候都保留了编码和解码的结构 想象一下你能在它上建立一个应用程序只是因为想象遇到Base44 单词是一个意思是可以对应起来的也就是说两个完全不同的语言符号需要通过离这个实体把语意给联系起来所以我们人从零开始学一门外语的情况可我们现在面临的问题是机器翻译没有办法给机器去指这个离的实体到底是啥而只能针对中文和英文这个纯文本去完成语意之间的对应那应该如何才能让机器去完成语意的对应呢让人手工去做那肯定不行语言的可能性那是无穷大表达的语意的规律也并不统一永远没有办法穷尽那怎么办呢我们说了机器翻译只能针对纯文本所以说就只能通过大量文本的上下文去确定词和语音的对应 两种语言的符号发音可能完全不同但是相同语义的词它的上下文关系应该都是类似的香蕉这个词无论是在中文语境还是在英文语境下它的上下文里面应该总是和猴子 黄色 甜 水果等等这样的词关系更加紧密前面说的编码和解码的那个码到底是啥呢其实就是把各种语言里面那些符号发音等等形式上的不同剥离掉之后剩下来的单纯的语义关系如果现在让你去设计这个纯粹的语义关系的码你会怎么设计呢 因为需要计算机来处理所以说这个羽翼关系应该是数字化的第二 因为它需要表示羽翼之间的相互关系所以说这个羽翼关系码数字化后的数值要能体现出羽翼之间的关系来才行第一点比较容易理解第二点我就举个例子假如说我们就用一个高维空间的坐标去当做羽翼关系码的数字化的结果在这个空间中香蕉对应的点如果是1000那猴子这个词和香蕉的羽翼关系就应该比较紧密它对应的点应该就在香蕉这个点的不远的地方假如说是0 1 2 1而像原子能啊 章鱼啊 海盗船啊这些只对应的点那距离 香蕉那個1000就應該遠得多了原子能的座標很可能就是01332那原子能這個點到香蕉這個點它們的摩長也就是它們的距離就比較遠了那就代表它們語意比較遠我這裡只是舉例子幫大家理解是可以用高微向量作為那個碼去表示語意以及它們之間關係的不過真實情況可就不像我這樣隨便了那麼這個高微向量的碼到底應該如何得到呢我覺得在機器學習裡面有兩個非常基礎的環節可以給我們帶來一些啟發這兩個環節一個就是Tokenizer也就是標記器或者是分子器另一個是OneHot就是讀者編碼這兩個東西它們做的事其實都可以算是 对一个文本里面最基础的语义单元进行数字化至于这个最基础的语义单元是什么那就不一定了可以是字母也可以是单词还可以是介于字母和单词之间的词根中文也是可以是字也可以是词总之就是你需要选定一个基础语义单元的一个标准然后就可以通过上面说的那两种方法给每一个基础的语义单元进行数字化了当然这里这个语义单元是我自己发明的一个词更专业的说法应该叫Token标记器和独热编码它们其实就是分别在利用不同的策略对Token实施数字化首先标记器它数字化的方法就比较简单它就是在给每一个不同的Token分配一个独立的ID 这个相当于是把所有的token都投射到了一根一维的宿轴上独热编码它做的事情就是把一个二进制里的每一位都对应一个token如果有苹果 香蕉 梨三个token那么标记器对它数字化之后就很可能是1代表苹果 2代表香蕉 3代表梨那独热编码呢这就是001代表苹果 010代表香蕉 100代表梨我这里之所以要介绍这两个东西那是因为它们其实代表了对token进行数字化的两种极端情况如果说分词器是把所有token都投射到了一个一维空间的话那么独热编码则是为每一个token都分配了一个单独的维度最后组成的维度就是 就是一个有多少token就有多少维度的高维空间我们前面提到了编解码的那个码需要有两个标准一个是数字化另一个是数字化之后的一个数值可以体现语义之间的相对关系不论是分词器还是读热编码它们都可以很好的达成第一个标准但是在第二个标准上就会出现问题我们先拿分词器来说它把所有的token都投射到了一个ewe的空间上这就导致这个空间里的信息过于密集这就很难表达出一些复杂的语义就比如我刚才说的苹果是1 香蕉是2 梨是3它们都是水果所以说这三个词对应的数值很接近感觉上很合理但是如果苹果这个token表示的是手机呢华为这个token表示的是什么呢 它对应的ID很可能就在1000以外了这个情况下这个ID数值就很难体现出它们互相之间的关系了还有如果我想表达苹果和香蕉这样一个组合起来的语音呢按照直觉这种组合起来语音应该是1加2对吧但是3这个数值已经被离这个token占用了也会发生冲突这是分词器的问题啊图尔编码呢它的问题正好相反它是对应的这个语音空间的维度太高了信息密度过于稀疏这样做的好处是它很容易的可以表示出苹果和香蕉是一样的 这样的组合语义苹果是001香蕉是010苹果和香蕉就是011但是独热码的问题是所有的Token都是一个独立的维度所有的Token互相之间都是正交的就很难体现出Token互相之间的语义联系苹果 香蕉 华为假如说他们的独热编码分别是001 010 100用向量表示的话就是图上的这个样子二进制的每一位就相当于是向量的一个维度这是一个三维空间里的向量但这几个向量它们的相关度那就是0了 就是所有token的它的独热编码它都是对应这个高维空间的标准正交基所以说它们不止正交摩长还都是1就导致所有token对应的那个点在这个空间中只会分布在距离原点为1的高维球面上这其实已经可以体现出来了独热编码的问题是在于空间维度过于高token互相之间的语义关系全部都是靠维度之间的关系去体现的并没有充分把空间的长度给利用起来分词器则是把所有的语义都变成了长度问题完全没有利用维度关系去表示语义信息所以说这两种对token数字化的方法其实就是两个相反的极端既然这样的话那就好办了 找一个维度高但是就没那么高的空间去协助完成编码和解码的工作那这个空间也就是我们经常听到的那个浅空间具体怎么找到这样一个浅空间呢其实有两个大的方向一个是基于分次后的ID去升纬一个是基于独热编码去降纬如果是我的话我肯定选择降纬比你直接上去想把原数据给压缩要比把已经压缩了的数据再还原要简单一些而提到降纬大家能想到啥呢我想如果大家对现行代数里的矩阵运算有一些了解的话应该就能想到现行代数里面向量和一个矩阵相乘就有一种理解方式就是它可以看作是一种空间变换我这里也想多介绍一些 矩陣和空間變換的這個關系因為這部分的理解對transformer來說太重要了如果你這方面基礎好的話是完全可以快進的其實把向量和矩陣相乘看作是空間變換這只是對矩陣的其中一種理解方式我們後面就會看到在作業力機制裏面就有一個矩陣相乘的運算雖然計算的規則還是一樣的但是如果你把它按照空間變換去理解就會產生更多的困擾那個時候就需要換另外一種理解方式了所以說我們盡量不要去死記硬背其中的結論而是理解其中的原理為什麼乘以一個矩陣就可以理解成空間變換一個向量和一個矩陣相乘之後得到一個新的向量而這個計算規則我會給大家解釋一下 我相信大家都已经非常熟悉了就是向量里的足向和矩阵里的一列相乘然后相加得到的值放到这一列对应的这一项里面也就是说它会成为新向量的对应的一个值现在我们只是从怠速的角度去解释它的计算规则那从几何的角度向量的一行和矩阵的一列足向相乘再相加然后等于新向量的这一项它到底代表了什么能不能从几何的角度给一个直观的理解我们可以先看这么一个简单的例子假如说这个T向量就是我们操作之前的这个向量这个向量里的每一项数值就相当于是这个向量在对应的坐标系下的坐标值 在这个图上也就是T向量在1 1和1 2这个标准正交基上的分量分量值分别是a乘以1 b乘以1 2我们的例子就是这个向量在经过矩阵计算完了之后它变成了在一个新的坐标系下的表示新的坐标系下它的坐标轴分别是1'1 1'2 1'3那现在我们需要去看的是怎么能把这个T在红色的坐标系下变成绿色的坐标系其实我们可以想一下具体这个T是什么在这个坐标系变化过程中不重要重要的是坐标系互相之间的关系而坐标系互相之间的关系其实就是它们的坐标轴互相之间的关系而坐标轴就可以看的是 单位向量 那我们就先来看E1在新坐标系下的样子假设这个E1在新坐标系下这三个坐标轴上的分量分别是W11 W12和W13这个蓝色的也就是原来T向量的这个分量那它在新坐标系下就可以这样来表示了A×W11 A×W12 A×W13这是一个坐标 原来的坐标系有两个所以说E2也要相同的方式表达出来E2这个向量在新坐标系下的分量分别是W21 W22 W23那对应的T向量它原来的分量在新坐标系下的分量的值就是BW21 BW22 BW23 A11加B12这个是T2在旧坐标系下的素质有了这两个表达之后我们就可以看这个T向量在新坐标系下它可以怎么去表示了T向量在新坐标系下无论怎样它都可以写成是这样子的在对应的三个坐标轴上的分量分别是XYZ那这个X等于多少呢我们前面已经可以看到了T它一定是等于A11加B12而T它在这个坐标轴上的分量有两个是AW11和BW21对应的这个Y就是AW12和BW12Z那就是AW13 BW13所以说这个式子按照上面的这个关系展开就是这样子 那这里的w系数是什么它代表的就是原来的坐标轴和新坐标轴它们互相之间的变换关系而a和b它体现的是原来向量的信息那这个计算如果用向量和矩阵相乘的方式去写的话那就是这样的一个乘法ab列向量先转制变成横向量a和它相乘 b和它相乘相加这就是xx是新坐标系下的第一维的素质a和b和这一列相乘相加就是这里它对应的是新坐标系下第二个维度的素质跟这个相乘相加那就是第三个维度的素质所以说从这个角度去看 代表的就是旧坐标系和新坐标系之间的关系所以说一个矩阵的行它代表的是旧坐标系有多少个维度列就代表的是新坐标系有多少个维度有了这个直观理解之后我们可以把这个问题再扩散一下现在我给大家呈现的例子给人的感觉是T向量本身没有变化变化的是坐标系但是因为上面是两维的下面是三维的很容易就想到改变的是坐标系但其实向量和坐标系的关系是一个相对的你很难区分改变的到底是坐标系还是向量尤其是上面和下面维度没有发生改变的时候所以说就完全可以出现这样一种情况这是原来的向量这是新的向量这个操作也是可以的 通过矩阵从法实现的而这个实现的效果就是原来的向量发生了旋转和拉伸也就是说这个矩阵这个时候它改变的就不是坐标系的维度了它带来的是向量的改变但其实这个过程是相对的你完全可以理解成是坐标轴发生了旋转坐标轴上的刻度发生了拉伸和收缩那现在看到的是两个比较特殊的情况这个感觉是向量没变变的是坐标系那这个给人的感觉是坐标系没变变的是向量那我们能不能把这两个结合一下那看到的其实就会是这样的一个结果一个向量已经描述这个向量的坐标系经过一个矩阵变换之后变成了一个新的向量以及新的坐标系注意啊 我们现在讲的只有这个 所以说原来的向量和新得出的结果它只可能有一个旋转拉伸收缩的变化它不会有这个向量平移的过程这个平移的过程是需要靠向量的加法去实现的而不是向量和矩阵的乘法这里可以再多说一下因为是向量和矩阵直接相乘它是一个线性变化所以说它带来的一个性质是什么呢在原来坐标系的一个点它唯一对应新坐标系的一个点它们之间的这个点的关系是一个一一对应的关系它们的所有的变化只可能是旋转或者是在直线方向上的一个拉伸和收缩 变成一个不是直线可以不可以那只是向量和矩阵相乘就做不到了那这个时候就需要利用到二次形的表达也就是一个数据也就是这个x我们可以看成是数据这个数据在矩阵的两边同时存在如果x它不是一个向量它就是一个单一素质的变量的话它其实对应的就是一个二次函数只不过变量它是可以直接平方的向量没办法直接平方你要想表达它的平方必须是左边一个右边一个把矩阵也就是那个系数放到中间然后如果你看它单向相乘它最后得到的结果跟这个是类似的一个形式而在几何上这种二次形它实现的效果就是把一个原来作料系的直线 可以变成不是直线 像这里是一个椭圆形它也可以是抛物线 也可以是双曲线它总之是它可以把一个直线变成是一个圆锥曲线就是二次形但如果只是一个向量和矩阵相乘它永远不可能得到一个曲线的结果这也就是为什么我们说矩阵的乘法加法它是一个线性变化模式而在几何的角度去看 它就是一个空间的变化这个空间的变化我们就可以理解成是一个坐标系的改变所以说一个向量和一个矩阵相乘它就是一个空间变化的过程而这个空间变化在我们这可以看到它体现出来的就是原来的坐标系和金坐标系之间的变化坐标系的改变它其实就相当于是空间发生了变化也就是说我们完全可以说 可以把原来这些东西想象成是一个空间它经过矩阵之后可以变成一个新的空间这带来的是什么呢就是原来空间中的向量啊或者是其他的任何的关系经过矩阵之后都可以对应到新空间里的一个图像而且这个新空间和旧空间的对应关系是一一对应的因为矩阵它表示的是一个空间的变化那向量呢其他图像呢就可以想象成是一个数据这个数据经历矩阵操作之后变成了一个新空间里的数据而且这个变化也是一一对应的那就代表了在原空间里面有多少个向量经过矩阵的这个乘法变换之后在新空间中也一一对应 一定是有多少個向量才對那現在我們看到的是一個向量我們可以同時考慮多個向量假如說這裏有三個向量這三個向量經過矩陣變換之後它仍然可以表示成三個向量變化的只是它們的維度也就是說變化的只是它們的坐標系而把三個向量拼在一起也可以看作是一個矩陣這就變成了矩陣和矩陣的相乘那如果我們把這個乘法的過程看作是一個空間變換的話乘號前面的那個矩陣它代表的是多個向量的集合中間的代表的是空間變換的規則它們是不能變動的因為它們意義不相同這可能也是矩陣乘法為什麽不能有交換率的原因如果大家熟悉編乘的話這個過程它更像是函數式編乘的一個過程 前面这个矩阵是一个一个的向量你可以想象一个一个的数据矩阵就是函数一个向量跟矩阵连续相乘数据在经历一系列的操作整个这个相乘的过程就是一个数据流的过程讲到这我又忍不住要吐槽一下关于线性代数的教程了我不知道理科是怎样的我学的是工科版的线性代数第一章第一个介绍的概念就是行列式为什么第一章要介绍行列式这个概念在实际的应用中其实是处于一个非常计较嘎呃的地位的它既起不到一个提纲显领的作用而一个初学者一开始就能明白线性代数的意义和价值在真正的应用中它出现的频率又非常的低反正至少我在机器学习这个过程中一次也没有用到线性代数 所以真不知道有多少人是被這樣一個不知所云的概念擋在了線性代數的大門之外我覺得上來就應該先把線性代數和幾何對應起來給同學們去講矩陣和空間變換之間的這個對應關係我是這麼理解空間變換的就是空間裏的某個對象它會根據一個或是一組函數關係進測到另外一個空間裏面這就是空間變換如果說這一個或者是一組函數都是一次函數那麽這就是線性變換那如果從幾何角度去看呢線性變換代表的其實就是在原空間裏點和點之間的這個相對關系被投射到新空間之後會有某些特性是保持不變當然我這裏說的這個相對關系是一個籠統的說法具體一下的話 至少有這幾個相對關系是不變的比如圓空間裏的兩個不同的點在新空間裏面也一定是兩個不同的點這也很顯然對吧如果不是線性變換那就不一定了其實也很好理解如果函數是一次函數那麽變量和函數值就是個一一對應關系如果是二次函數就不一定了二次函數就不能保證圓空間裏兩個不一樣的點到新空間裏還是不一樣的點除了這個之外線性變換還可以保證如果圓空間裏的兩個點是共線的那麽在新空間裏面仍然是共線如果兩個向量在圓空間裏是平行的那麽在新空間裏仍然是平行的還有兩個向量在線性變換的時候它們的絕對長度是可能變化的但是它們的比值不會變化 等等等等吧就是为了方便表示线性变换就把那一组变换的线性概述的系数给拿出来写成了矩阵的形式空间变换的这个操作也就变成了矩阵乘法不过矩阵这个概念不只是从形式上进行简化这么简单有了矩阵很多事情理解起来就会更直观比如说通过矩阵的行速和列速很快就可以确定变换前后的空间维度还有一点也可以肯定矩阵乘法里面前后两个矩阵是相互独立的如果把第一个看作是空间里的一组向量那另一个就是变换轨子这两个矩阵是相互独立的这就代表着空间变换前后到底怎么变它只和第二个矩阵有关和前面具体的向量是没有关系的也就是说 也就是说怎么操作数据数据操作的规则和数据本身是没有关系的我们只需要拿到后面这个矩阵这个矩阵里具有的性质那就一定是空间变换前后变化的性质讲到这就可以再去提行列式了行列式是专门针对方阵的也就是行数和列数相等的矩阵这其实就代表了变换前后的空间维度是不变的这也就是说变换前的对象如果是一个平面变换后仍然是一个平面那么变换后的这个平面的面积到底是被拉扯大了还是被缩小了由此以为一定是引起空间变化的矩阵来决定的而这个矩阵行列式的值作为这个矩阵的特征作为这个矩阵的一个性质从几何上就代表它 变换前后面积的拉伸比例如果是三倍空间那就代表的是体积的拉伸比例更高维也是类似这个性质和具体需要变换的那个对象没有关系是和变换的规则有关也就是说它只和变换聚散里面的素质有关有了这样的理解那这个行列式是不是就没有那么面目可憎了那个时候再去看数学上为什么学那么多有关聚散的性质也就容易明白了因为它们每多一条受到约束的性质那对应到空间变换里面很可能就多了一个从原空间到新空间需要保持不变的特性聚散如果是对称的那就需要额外要求沿着对角线划分两边的数据必须是相同的是从怠速角度去看那换成几何角度呢它就变成了 它其实就代表着两个向量变换前后它的内积是保持不变的而且学习线性代数或者是高等代数的时候经常对矩阵进行特征分析既是在提取矩阵的代数特征又是在体现空间变换时它有哪些特性你看有了这样一个框架的理解再去学习各种各样的线性代数知识是不是就不会那么困惑入学本来也就更容易了呢大家都说学习知识吃苦那是必要的如果说这个苦是因为知识本身很难我觉得应该是但如果这个苦是人为添加的那我觉得这个苦还就是能逃就逃如果硬是和教科书死磕那就显得有些投铁了当然我这里只是浅浅的提一下矩阵相关的原理 或於某一天我會按照我的理解把線性代數和矩陣方面的知識給梳理一遍再分享出來還是提醒大家矩陣是一個工具把它理解成空間變換只是其中的一種理解方式而不是唯一的理解方式像前面提到的把空間看作是一組向量也是一種理解方式都是同一個東西你看的角度不一樣你最後理解的東西可能也就不一樣了矩陣看作是一組向量如果按照這樣的方式去理解那矩陣對稱性之類的性質就反而不這麽重要相反矩陣的質這個特性就可能會顯得更重要矩陣的質它其實就代表要描述這個矩陣裏一行一行的向量一共需要多少個線性無關的向量個數它也就相當重要了 要把这个矩阵里的所有项量都描述出来那最少需要一个多少维的空间可以无损的给表达出来所以这两种理解矩阵的方式后面讲到作弊机制的时候还会涉及到第三种理解方式到时候我们再介绍关于矩阵就先介绍这么多大家在这个基础知识上对齐一下那接下来的内容就会更容易理解了回到刚才说的你现在需要做的就是对读热编码进行降维其实咱们现在学习大部分的AI模型都是以神经网络作为基础我相信大家对神经网络的熟悉程度一提到升维和降维肯定会想到神经网络一个神经网络的隐藏层其实就是在进行一次空间变换其中隐藏层里的神经元的个数就是你变换后的空间 空间的维度这就代表你可以升维也可以降维如果说隐藏层里神经元的个数比输入的多这就是升维如果说比输入的少这就是降维其实神经网络的数学表达式写出来也可以看作是一个矩阵运算这其实也能理解为什么显卡对于神经网络训练是如此的重要显卡本来就是在处理3D画面里的空间计算空间计算本质上就是一堆矩阵和线性计算而神经网络本质上也是矩阵和线性计算所以说基础都是一样的不过神经网络和矩阵运算还是有一些不同的不同的方面主要有两个一个是神经网络的一层运算它不只有矩阵的乘法它还会有一个加法也就是加上天质系数b 要从几何角度去理解呢那就是和全重系数W相乘那相当于是乘以一个矩阵进行了空间的旋转或者是拉伸变换那加上偏置系数B呢那就相当于这个空间里的向量又进行了一个平移的操作这是其中一个不一样的点另一个不同的点是对神经网络数据在进行完线性计算之后还要经过非线性的激活函数因为只有靠激活函数引入非线性性才可以让模型描述更复杂的情况换句话说就是神经网络和矩阵虽然都可以看作是把一个原空间里的对象投射到新空间但是它们之间还是有区别的矩阵乘法因为都是线性的所以说它可以保证原空间的点 到新空间的点是一一对应的神经网络就不一样了就拿最典型的分类问题来举例如果把所有数据里的每张照片都看作是原空间里的一个点的话那么照片分类的结果就相当于是投射到新空间如果说我们只需要去分辨是猫还是狗这两个状态的话那么新空间里面也就只有这两个点这就是一个多对一的关系而神经网络的作用就是要对原空间实现非线性变换让它们可以把所有能代表猫的数据都投射到新空间里的一个点上关于神经网络的升位和降位的理解我把之前这个视频里的相关内容就直接拿过来了如果你觉得自己对这部分内容已经非常熟悉了那完全可以跳过 當太多工作讓你沮喪ASANA能幫助你處理這就是為什麼ASANA是人類和AI聯合組織的AI讓我們容易處理日常任務加強運作流程並讓所有人都專注於最重要的工作這就是如何處理工作這就是ASANA請到ASANA.com這就是ASANA.com 我能有更强的能力还可以从另外一个角度来进行解释我可以把前面神经网络看作是一个黑盒子无论输入的数据原本是什么样子最后输入到最后一个神经元的就是这些数据还有实为数据从两枚变成了实为其中的关键就是输入的数据进行了这样一个矩阵运算矩阵对一个向量进行操作然后将它变成另外一个向量这件事我们并不陌生比如这样一个向量在进行矩阵操作之后会旋转C的角度这个矩阵的作用就是操作向量发生旋转虽然我们把矩阵理解成对向量的操作那么这样一个矩阵完全可以起到对二维向量进行升维的作用如果数据只是二维的它们互相掺杂在一起很难直接用直线进行划分但是如果把它们全部升维那么它就会变成一个直线所以我们可以看到这些数据是一个相对的变化它们是一个变化它们是一个变化它们是一个变化它们是一个变化它们是一个变化它们是一个变化它们是一个变化它们是一个变化它们是一个变化它们是一个变化 在更高的维度中 即便用更简单的模型也可以把它们分开所以我们可以把中间这一层看作是对数据的升维操作有多少神经元就会把数据升到多少维只要维度够高一定可以找到一个超平面能对数据完成划分现在我们终于可以看到一个比较完整的神经网络了中间这部分叫做隐藏层前面的是输入层后面的是输出层它们各自的任务是不完全相同的其中隐藏层的作用就是让模型可以更复杂我们现在暂时可以把它理解成它能让数据升维 不过我们见过更典型的神经绑路它应该是这样的 中間這些通通叫做隱藏層它的作用仍然是可以讓模型更復雜但是如果繼續用生為的思路來理解的話似乎就會出現問題因為這裏的數據維度更高隱藏層雖然有了更多的層但是神經元的個數卻在逐漸減少最後應該如何理解呢我們還是回歸到最簡單的問題上一個神經網絡的輸入數據可能會是什麽樣子比如這裏就可能輸入的數據是這些維度而最後判斷的是一個視頻是否值得推薦如果數據就是這樣的話我們很難有什麽新的發現不過只需要把問題換一下輸入的數據換成了這些表情動作最後根據這些表情動作去判斷一個人的心情到底是什麽樣子 这些数据的维度是什么它们完全可以称作是判断一个人心情的特征不过现实中可没有这么清晰明确的特征数据等着我们去判断我们可以接收到的往往是这样的原始图片它们的每一个像素都是数据的一个维度 这个时候再有隐藏仓库 隐藏层中的每个节点我们就可以把他们理解成最后判断的特征为什么这里最后反而降回来呢这是因为这里的一个特征根本不需要原始数据里的所有维度眼睛的动作只需要关注图片里眼睛的像素点就够了而在这里我们完全可以把隐藏层理解成它是在对上一层原始数据进行抽象不过只是理解到这个程度的话还是解释不了为什么神经网络要那么多层这个时候我们就需要换另外一个问题了这里有一组手写的数字需要用神经网络判断它们属于哪一个数字比如这个8我们可以知道它最主要的特征是上面和下面两个圆圈所以神经网络很可能会把它们分别提取出来作为判断的依据我这里为什么把它们放在了第二个隐藏层呢就是因为 上下两个圆它们仍然可以有自己的子特征它们分别是由四个圆弧组成的 这样有不同层次的特征最大的好处是底层的特征会被更容易的附庸比如这是一个数字6它下面的圆圈就可以和8进行附庸上面的一数才是它独有的特征 如果這個時候輸的是一個5這個5下面雖然不是一個完整的圓但是仍然可以拆出三個圓弧進行復用 至于上面的部分也只有那一行是新特造数虽然比数字6的短但是在子特征部分仍然可以被附有 所以说隐藏层的层数它其实代表了一个神经网络它可以对数据特征进行抽象的程度隐藏层越深抽象程度越高最后还需要说明一下的是这里的特征我只是为了方便讲述所以用了我们人能理解的特征如果是神经网络自己学习到的特征虽然最后判断的结果仍然非常准确但是我们可能完全无法理解还有一点这里的8有上下两个元素 所以说这就没法应对超越首次规定之外的情况了即便是key value这个表是无穷的那它也有一个问题它是可数级它的式是阿列复0和自然数是等式的而不是实数的阿列复1所以说我觉得这一点才是决定编码模式是否真正理解了语言的有力证据因为只有理解了才能在超越经验的情况下生成出合理的内容明白这一点之后那我们接下来考虑的就是一些具体的问题了就是如何才能找到把真实语言里的token投放到潜空间的方法也就是怎么才能找到实现降维的嵌入矩阵这肯定是用机器学习的方法了不可能人工去设定关键是这个token的语言 具体的机器学习的方法应该怎样做呢谷歌在2013年提出的word2vec就是一个其中的方法这个方法比较特别的是它的目标和我们平时了解机器学习模型的目标不太一样通常我们了解的机器学习它们的模型是什么呢就是在训练好了之后是希望模型可以完成某个任务就比如说我说一个照片你得能知道它到底是狗还是猫而word2vec这个模型的目标它希望得到的是嵌入矩阵也就是说它的目标不是模型的结果而是模型的参数 就是要把作家給培養出來是希望他能按照你的要求去寫出相應的文章的而word2vec它的目標更像是編詞演編好了給作家去用這兩個目的是不一樣的這兩個目的的不同帶來的最直接的差別就是word2vec它裏面不需要記錄函數它會計算起來更簡單具體什麽意思呢我們一步一步來看編碼和解碼的原理是這樣的你輸入一個token經過一個矩陣編碼成了詞項量然後詞項量你又可以解碼回去再變成token如果你的參數沒問題你解碼回去這個詞和之前應該是沒有差別的用這個思路去訓練模型行不行的看起來是可以但其實是沒有辦法去訓練因為訓練它一定是要有一個 经过模型计算出来一个结果这个结果和正确答案是有差异的然后才能把这个差异去反向传播回去去修正参数如果是我刚才说的那种情况那根本就不能训练因为不论输入的向量是什么只要前面和后面两个矩阵它们是一个违逆关系你要是相乘之后它们是一个单位矩阵那么输入和输出它就一定是相等的所以说要想真正的完成训练还需要在这个思路上做一些调整具体的方法就是谷歌论文里提到的有两种一个是COBO一个是SKIPGRAND先说COBO它的原理是这样的 你负责的所以你可以准备你自己喜欢的比如说你可以添加自行构图并选择你喜欢的工作不需要人帮助输入的不再是一个token而是准备一组基数个的token假如说是五个token然后把中间的这个给拿掉剩下四个分别与同一个嵌入句子相成把它们变成前空间里的词项量之后再把这四个项量加在一起合成一个项量然后再对这个合项量进行解码这个时候损失函数就会定量的去看这个合项量解码后得到的那个token挖掉的那个中间token 是不是一样的如果不一样那就需要去修改参数但是这里有些细节需要注意我们后面还会再去细说我们先来解释一下这里这个原理为什么把上下四个单词的项量加一起就应该得到中间那个词的项量呢其实只需要大家去调动一下高中物理学受力分析的基因把这个问题和力的合成力的分解对应起来就可以比较容易的理解了如果只能从文本中去理解一个token的语义那就能且只能根据这个上下文去进行判断去理解反过来有了上下文也应该可以推断出缺失了那个token的语义 把已知的磁相量看作是分粒中间缺的那个token所对应的磁相量看作是已知分粒的合力这应该挺合理的对吧缺失的那个token它既然能被上下文决定那么这个token所对应的磁相量那它应该就是已知的磁相量的合力反过来也是一样中间的这个token磁相量如果做相量分解的话分解出来的分量也应该可以对应到上下文的磁相量上当然你也可能会说这里是不是有点问题就比如说有这样一段话这是一个空格苹果那这个空格到底是甜什么呢可以是红可以是绿可以是甜可以是便宜这都是正确的它预测出来到底是什么首先这是因为上下文 训练的数据不够多再者这里训练的目的不是为了让模型具备完形填空的预测能力而是让它能够训练出体现语意的嵌入矩阵即便是训练的时候训练的数据里面同时有这是一个红苹果这是一个绿苹果这是一个甜苹果下次你再输入这是一个什么苹果的时候它仍然没有办法给出一个你希望的答案但是也没关系因为给出正确答案这个根本就不是这个模型的目的重要是经过训练了之后在这个模型的潜空间里面红绿甜这几个token的语意一定是一种比较接近的关系至少从我们来说它们都是形容词我们前面不是比喻What to act 更像是编词典吗你想想词典里面的一个词被注解出来有好几个解释这很正常对吧这是一样的道理当然 world2vec只是提供了一个对语意的最初的理解它训练完成之后体现的是单个token之间的联系也就是前面说的它就起到一个词典的作用词典会用其他词去解释目标词world2vec它生成的潜空间就是在用其他的词项量去合成目标词项量得到的词项量从这里我想大家也能看出来了world2vec这种形式这个潜空间里面词项量对应的词意它是不依赖于作者主观意图的它是一种客观的表达这个客观性是和整个目标有关的 语言环境绑定在一起而一个作者根据自己的主观意图把许多词汇组成在一起这个时候才具有了主观性才能体现出不同人想表达的不同内容而这个主观性就体现在作者选择了不同的词按照不同的顺序进行的组合是这个不同词和不同顺序在体现主观性而要想让模型理解这部分体现作者主观性的语意那就不是well2vec的责任了这其实就是后面注意力机制需要做的事情这是Kobold原理知道了他skipgram也就好理解了他其实就是把Kobold原理反过来用以至于一个token根据它的词项量去求出上下文对应的token的分量来看看是否能够 是不是和训练数据一致其实大家也能看出来这两种方法都是可以自监督学习的不需要人文的去打标签只要给一个文本就可以用程序自己挖掉一些空自己去训练这是他们的原理啊具体实现的时候还是有些细节的我在这虽然用的是矩阵和向量这种数学方式来表示的但是在真正建立模型的时候往往还是会把它看作是一个只有一层隐藏层的神经网络去操作的如果这个神经网络里面没有偏执系数B也没有激活函数如果单纯从我刚才解释的那个原理去看 这样的话隐藏层两边其实是一个逆过程也就是说W和W'它们相乘之后应该是得到一个单位矩阵因为这是一个降维和升维的过程所以它们行列数肯定不一样也就是说它们是一个美逆的关系它们不是一个标准的逆矩阵总之就是两个矩阵只要知道了一个另一个是可以直接通过解析解求出来的不需要去进行学习不过但是据我了解在具体实现这个模型的时候模型里面W和W'还就是两个独立的矩阵它们分别去进行学习和训练也就是说它们会在反向传播进行训练的时候各学各的我猜测这里可能和求矩阵的逆这个计算的复杂度太高了有可能 如果单纯的是把T2向后传播这个计算复杂度大概是大欧N这样一个量级的但是矩阵求逆呢这就是一个大欧N的三次方这样一个量级的复杂度了所以不严格的话那还是通过T2下降法去进行训练会来的更简单一些还有就是隐藏层和输出层的这个神经元它是没有计和海数的因为根据我们上面的解释它其实这里做的就是一个典型的向量求和向量分解所以这里根本就没有分线性的需求因为变换前和变换后它的空间都是相同的嘛这是标准的Word2Vec的方法这种方法重点就是训练出一本词典也就是说这里训练出来的嵌入矩阵W只是针对单个词典 如果想把一个一个的词组成有准确含义的一句话那靠这种简单的模型就不够了包括前面的介绍我们现在对编解码和词项量有了一个基本的了解了这些都算是铺垫现在我们终于可以正式开始介绍Transformer了还是这张图按照现在大家公认的理解这个图上左边那部分就是编码部分右边的就是解码部分前面也提到了现在的各大模型都是为了适应某个特定的需求对这个结构进行了一些变化和优化而得来的现在我们如果就是针对这个结构去介绍的话接下来为了大家理解方便还是用机器翻译作为例子去讲会更容易一点在这个图上最下面有输入和输出就相当于 左边输入中文 它会自动翻译成英文从右边输出当然这是已经训练好进行推理的情况如果是为了训练那么左边和右边都是一种输入分别把中文和英文与相同的一些语料输入到模型里面最后输出的是这个图上最上面那部分也就是损失函数在上面通过损失函数的值再进行反向传播去修改里面的参数然后再细看一下还会发现下面这两个输入都是需要先转化成词降量然后再去进行后续的操作的也就是说这一层其实已经有一个嵌入矩阵了这个嵌入矩阵也不是写死的它也是需要在训练的时候进行调整参数的 前面不是说过吗这部分就相当于准备好词典把单个token它这个词译都查出来词和词组合之后的语义是需要在现在的基础之上再去进一步分析和理解的我也不卖关子了对词和词组合后的语义进行理解靠的就是注意力机制就是图上橙色的那一部分这就是transformer的核心了剩下的其他部分虽然也重要不过不是核心所以我们接下来就先介绍什么是注意力怎么理解注意力对注意力有了一个基本的了解之后再理解其他部分就会容易了说到什么是注意力机制最直接的方法就是看这张图注意主词项量然后经过三个句子相乘之后分别会得到QQ 然后他们再进行一顿运算最后还是会输出一组词项量值得注意的是在注意力机制这里如果每次只是输入一个词你要计算也的确是可以但是这样的话就体现不出注意力机制它的价值前面说了词嵌入已经解决了单个词单个token语意的问题了注意力机制要解决的就是许多词组合在一起之后整体体现出来的那个语意你只有把一句话里多个词同时输入到模型里面前面说的那一点才能体现出来所以接下来讲解输入部分就不考虑只输入一个词的情况了而是考虑输入一组词的情况这个时候这组词项量就组成了一个数据矩阵 输入的是一个T行的矩阵输出它也是一个T行的矩阵至于输出的列数也就是一个磁相量它的维度的个数我们把token变成磁相量就像是把大象变成石头方便后面的各种操作对石头来说比较方便的操作就是切割搬运称量磁相量方面的操作那是和空间变换有关的各种矩阵和向量运算了所以说经过注意力机制之后把原来的磁相量进行一些升维和降维这些操作就再正常不过了WQ WK WV这三个矩阵按照注意力机制的要求输入的磁相量矩阵都需要先和这三个矩阵相乘之后才会得到QKV先不管它们具体的功能是什么只要是完成这样一个相乘 它们至少是可以起到空间变换作用的输出的矩阵是T行D音力WQ WK WV三个矩阵是D音行Dout力这个Dout就决定了输出的磁相量是多少力量也就是有多少个维度具体D音是要大于Dout还是小于就需要根据具体的情况而定了在模型里面不做具体的要求磁相量矩阵和这些W矩阵相乘之后得到的QKV这其实没有什么特别的作业力机制里面最值得关注的其实是得到QKV之后后续的操作我先说一下这里的计算规则然后再慢慢解释其中的含义得到QKV之后先把K进行转制然后让Q和K的转制相乘当然这部分也可以是 由 Amara.org 社群提供的字幕 好 那我們就開始上課吧那第一堂課啊是要簡單跟大家介紹一下Machine Learning還有 Deep Learning 的基本概念那等一下呢會講一個跟寶可夢完全沒有關係的故事告訴你機器學習還有深度學習的基本概念好 那什麼是機器學習呢那我想必大家在報章雜誌上其實往往都已經聽過機器學習這個詞彙 知道說機器學習就是跟今天很熱門的AI好像有那麼一點關聯那所謂的機器學習到底是什麼呢顧名思義好像是說機器它具備有學習的能力那些科普文章往往把機器學習這個東西吹得玄之又玄好像機器會學習以後我們就有了人工智慧有人工智慧以後機器接下來就要統治人類了好那機器學習 到底是什麼呢事實上機器學習概括來說可以用一句話來描述機器學習這件事什麼叫機器學習呢機器學習就是讓機器具備找一個函式的能力那機器具備找函式的能力以後它可以做什麼樣的事情呢它確實可以做很多事舉例來說假設你今天想要叫機器做語音辨識機器聽一段聲音產生這段聲音對應的文字 那你需要的就是一個函式這個函式的輸入是聲音訊號輸出是這段聲音訊號的內容那你可以想像說這個可以把聲音訊號當作輸入文字當作輸出的函式顯然非常非常的複雜它絕對不是你只可以用人手寫出來的方程式這個函式它非常非常的複雜人類絕對沒有能力把它寫出來所以我們期待憑藉著機器的力量 韓式自動找出來這件事情就是機器學習那剛才舉的例子是語音辨識還有好多好多的任務我們都需要找一個很複雜的韓式舉例來說假設我們現在要做影像辨識那這個影像辨識我們需要什麼樣的韓式呢這個韓式的輸入是一張圖片它的輸出是什麼呢它是這個圖片裡面有什麼樣的內容或者是大家都知道的 其實也可以看作是一個函式要讓機器下圍棋我們需要的就是一個函式這個函式的輸入是棋盤上黑子跟白子的位置輸出是什麼輸出是機器下一步應該落子的位置假設你可以找到一個函式這個函式的輸入就是棋盤上黑子跟白子的位置輸出就是下一步應該落子的位置那我們就可以讓機器做自動下圍棋這件事就更容易了 就可以做一個AlphaGo那隨著我們要找的函式不同機器學習有不同的類別那這邊介紹幾個專有名詞給大家認識一下第一個專有名詞叫做RegressionRegression的意思是說假設我們今天要找的函式它的輸出是一個數值它的輸出是一個scalar那這樣子的機器學習的任務我們稱之為Regression那這邊 先舉一個regression的例子假設我們今天要機器做的事情是預測未來某一個時間的PM2.5的數值你要叫機器做的事情是找一個函式這個我們用F來表示這個函式的輸出是明天中午的PM2.5的數值它的輸入可能是種種跟預測PM2.5有關的指數包括今天的PM2.5的數值今天的平均溫度今天平均的臭氧濃度等等這個函式可以拿這些數值當作輸出 輸出明天中午的PM2.5的數值那這個找這個函式的任務叫做regression那還有別的任務嗎還有別的任務除了regression以外另外一個大家耳熟能詳的任務呢叫做classification那classification這個任務要繼續做的是選擇題我們人類先準備好一些選項那這些選項呢又叫做類別又叫做 我們現在要找的函式它的輸出就是從我們設定好的選項裡面選擇一個當作輸出這個問題這個任務就叫做Lassocation舉例來說現在每個人都有Gmail Account那Gmail Account裡面呢有一個函式這個函式可以幫我們偵測一封郵件是不是垃圾郵件這個函式的輸入是一封電子郵件那它的輸出是什麼呢你要先準備好 你要機器選擇選項在偵測垃圾郵件這個問題裡面可能的選項就是兩個是垃圾郵件或不是垃圾郵件yes或者是no那機器要從yes跟no裡面選一個選項出來這個問題叫做classification那classification不一定只有兩個選項也可以有多個選項舉例來說AlphaGo本身也是一個classification的問題只是這個classification它的選項 是比較多的那如果要叫機器下圍棋你想追著alpha go的話我們要給機器多少個選項呢你就想想看棋盤上有多少個位置那我們知道棋盤上有19x19個位置那叫機器下圍棋這個問題其實就是一個有19x19個選項的選擇題你要叫機器做的就是找一個函式這個函式的輸入是棋盤上A子跟白子的位置輸出就是從19x19個選項裡面 選出一個正確的選項從19x19個可以落子的位置裡面選出下一步應該要落子的位置這個問題也是一個分類的問題那其實很多教科書在講機器學習的種種不同類型的任務的時候往往就講到這邊告訴你說機器學習兩大類任務一個叫做 Regression一個叫做 Classification然後就結束了 機器學習的認知只停留在機器學習就是兩大類任務regression跟transportation那就好像你以為說這個世界只有五大洲一樣你知道這個世界不是只有五大洲對不對這個世界不是外面是有一個黑暗大陸的這鬼滅之刃連載之前我們就已經出發前往黑暗大陸了鬼滅之刃連載以後我們居然都還沒有到可見這個黑暗大陸距離我們遠那在機器學習那個領域裡面所謂的黑暗大陸是什麼呢 在regression跟translocation以外大家往往害怕碰觸的問題叫做structure learning也就是機器今天不只是要做選擇題不只是輸出一個數字還要產生一個有結構的物件舉例來說機器畫一張圖寫一篇文章這怎麼叫機器產生有結構的東西的這個問題啊就叫做structure learning那如果要講的比較擬人話比較潮一點structure learning你可以 用擬人化的講法說我們就是要叫機器學會創造這件事情好 那到目前為止我們就是講了三個機器學習的任務Regression, Classification, Structure Learning接下來我們要講的是我們說機器學習就是要找一個函式 Agents gives you the orchestration layer to allow LLMs to do work in your businessas well as a platform to manage all your agentsCreate custom tools, allow LLMs to execute themand track agent performance all in our secure enterprise-grade platform那機器怎麼找一個函式呢那這邊要用一個例子跟大家說明說機器怎麼找一個函式這邊的例子是什麼呢這邊的例子在講這個例子之前建綸大家 那說一下說這門課有一個YouTube的頻道然後我會把上課的錄影放到這個YouTube的頻道上面那這個頻道呢感謝過去修過這門課的同學不嫌棄其實也蠻多人訂閱所以我算是一個三流的YouTuber是沒有什麼太多流量但是也是有這邊也說7萬多訂閱了那為什麼突然提到這個YouTube的頻道呢因為我們等一下要舉的例子啊 跟YouTuber是有關係的那你知道身為一個YouTuberYouTuber在意的東西是什麼呢YouTuber在意的就是這個頻道的流量對不對假設有一個YouTuber是靠著YouTuber維生的他會在意說頻道有沒有流量這樣他才會知道他可以獲利多少所以我在想說我們有沒有可能找一個函式這個函式他的輸入是YouTuber後台的資訊輸出是這個頻道隔天的 明天的總點閱率總共有多少假設你自己有YouTube頻道的話你會知道說在YouTube後台你可以看到很多相關的資訊比如說每一天按讚的人數有多少每一天訂閱的人數有多少每一天觀看的次數有多少我們能不能夠根據一個頻道過往所有的資訊去預測他明天有可能觀看的次數是多少呢我們能不能夠找一個函式函式的輸入是 YouTube上面 YouTube後台是我的資訊輸出就是某一天隔天這個頻道會有的總觀看的次數呢那你可能會問說為什麼要做這個嗯如果我有盈利的話我可以知道我未來可以賺到多少錢但我其實沒有開盈利所以我也不知道為什麼東西要做這個就是完全沒有任何軟用我單純就是想舉一個例子而已好那接下來啊我們就要問怎麼找出這個韓式呢怎麼找這個韓式FB 輸入是 YouTube 後排的資料輸出是這個頻道隔天的點閱的總人數呢那機器學習找這個函式的過程分成三個步驟那我們就用 YouTube 頻道點閱人數預測這件事情來跟大家說明這三個步驟是怎麼運作的第一個步驟是我們要寫出一個帶有未知參數的函式簡單來說就是我們先猜測一下我們打算 我們打算找的這個函是F它的數學式到底長什麼樣子舉例來說我們這邊先做一個最初步的猜測這個F長什麼樣子呢這個數目跟Y之間有什麼樣的關係呢我們寫成這個樣子Y等於B加W乘以X1這邊的每一個數值是什麼呢這個Y啊是就假設是今天吧這個Y因為今天還沒有過完所以我們還不知道今天總共的點閱次數是多少這件事情 是我們未知的Y 是我們準備要預測的東西我們準備要預測的是今天2月26號這個頻道總共觀看的人數那X1是什麼呢X1是這個頻道前一天總共觀看的人數Y跟X跟X1都是組織都是我們這個Y呢是我們準備要預測的東西而X1是我們已經知道的資訊那B跟W是什麼呢B跟W是 未知的参数它是准备要透过资料去找出来的我们还不知道W跟B应该是多少我们只是隐约的猜测说那这个猜测为什么会有这个猜测呢这个猜测往往就来自于对这个问题的纸上的了解也就是Domain Knowledge所以常常会听到有人说啊这个做机器学习啊你就需要一些Domain Knowledge这个Domain Knowledge通常是用在哪里呢 就是用在你寫這個代表未知數的函數的時候所以我們怎麼知道說這個能夠預測未來點閱次數的函數是 F它就一定是前一天的點閱次數乘上 W 再加上 B 呢我們其實不知道 這是一個猜測也許我們覺得說今天的點閱次數總是會跟昨天的點閱次數有點關聯吧所以我們把昨天的點閱次數乘上一個數值但是總是不會一模一樣 加上一個 b 做修正當作是對於2月26號點閱次數的預測這是一個猜測它不一定是對的我們等一下回頭會再來修正這個猜測好那現在總之我們就隨便猜說 y 等於 b 加 w 乘以 x1而 b 跟 w 是未知的這個帶有未知的參數這個 parameter 中文通常翻成參數了這個帶有 unknown parameter 的這個 function我們就叫做 model 常常聽到有人說模型 model 這個東西model 這個東西在機器學習裡面就是一個太有未知的 parameter 的 function那這個 S1 是這個 function 裡面我們已經知道的東西它是來自於 YouTube 的後台資訊我們已經知道2月25號點閱的總人數是多少這個東西叫做 feature而 W 跟 B 是我們不知道的它是 unknown 的 parameter那這邊我們也給 W 跟 B 給它一個名字這個跟 feature 做相乘的未知的參數這個 W 我們叫它 weight 跟 Feature 相乘的 是直接加上去的這個我們叫它 Bias那這個只是一些名詞的定義而已讓等一下我們講課的時候在稱呼模型裡面的每一個東西的時候可以更為方便好 那這個是第一個步驟那第二個步驟是什麼呢第二個步驟呢是我們要定義一個東西叫做 Loss什麼是 Loss 呢Loss 啊 它也是一個 Function那這個 Function 它的輸入是我們 Model 裡面的參數我剛才已經把我們的 Model 寫出來了對不對 叫做Y等於D加W乘以X1 而 b 跟 w 是未知的是我們準備要找出來那所謂的 L所謂的這個 loss它是一個 function這個 function 的輸入是什麼這個 function 的輸入就是 b 跟 w所以 L 它是一個 function它的輸入是 parameter是 model 裡面的 parameter但是這個 loss這個 function 的輸出的值代表什麼呢這個 function 輸出的值代表說現在如果我們把這一組未知的參數設定某一個數值的時候這個數值好還是不好那這樣講可能你覺得有點抽象 具體的例子假設現在我們給未知的參數的設定是B這個 Bias 等於 0.5k這個 W 呢直接等於 1那這個 Loss 怎麼計算如果我們 B 設 0.5k這個 W 設 1那我們拿來預測未來的這個點閱次數的函式啊就變成 y 等於 0.5k加一倍的 x1那這樣子的一個函式這個 0.5k 跟 1他們所代表的這個函式他有多好呢這個東西就是 Loss 在這個問題裡面我們要怎麼計算這個Loss呢這個我們就要從訓練資料來進行計算在這個問題裡面我們的訓練資料是什麼呢我們的訓練資料是這個頻道過去的點閱次數舉例來說從2017年到2020年的點閱次數每天的這個頻道點閱次數都知道嘛這邊是假的數字啦隨便亂編的好那所以我們知道2017年1月1號到2020年12月31號的點閱數字是多少好接下來我們就可以計算Loss 我們把2017年1月1號的點閱次數帶入這一個函式裡面我們已經說我們想要知道D設定為0.5K W設定為1的時候這個函式有多棒當B設定為0.5K W設定為1的時候我們拿來預測的這個函數是Y等於0.5K加1倍的X1那我們就把這個X1帶4.8K看看預測出來的結果是多少所以根據這個函式根據D設0.5K W設1的這個函式如果1月1號是4.8K的點閱次數的話隔天應該是4.8K 一加0.5K也就是5.3K的點閱次數隔天實際上的點閱次數1月2號的點閱次數我們知道嗎從後台的資訊裡面我們是知道的所以我們可以比對一下現在這個函式預估的結果跟真正的結果它的差距有多大這個函式預估的結果是5.3K真正的結果是多少呢真正的結果是4.9K是高估了高估了這個頻道可能的點閱的人數就可以計算一下這個差距計算一下公測的值跟真實的值的差距這邊公測的值用Y來表示真實的值用YM來表示 你可以計算Y跟Ys之間的差距得到一個E1代表膚色的值跟真實的值之間的差距那計算差距其實有不只一種方式我們這邊把Y跟Ys相減直接取絕對值賺出來的值是0.4K好的我們今天有的資料不是只有1月1號跟1月2號的資料我們有2017年1月1號到2020年12月31號總共三年的資料好的這個真實的值啊叫做Label所以常常聽到有人說做機器學習就需要LabelLabel就是正確的數值這個東西叫做Label 做我們的Label那我們不是只能夠看用1月1號來預測1月2號的詞我們可以用1月2號的詞來預測1月3號的詞如果我們現在的函數是Y等於0.5K加1倍的X1那1月2號根據1月2號的點閱次數預測的1月3號的點閱次數只是多少呢是5.4K你X1大概是4.9K進去乘以1倍加0.5K等於5.4K接下來計算這個5.4K跟真正的答案跟Label之間的差距Label是7.5K再來是一個低估低估了這個頻道在1月3號的時候的點閱次數就可以算出EQ 這個EQ是Y減跟Y和Y hat之間的差距算出來是2.0K那同樣的方法你就可以算過這三年來每一天的預設的誤差假設我們今天的 function是Y等於0.5K加1倍的XY這是三年來每一天的誤差通通都可以算出來每一天的誤差都可以給我們一個小1好那接下來我們就把每一天的誤差通通加起來加起來然後取一個平均這個大根代表我們的訓練資料的次數的個數那我們訓練資料的個數就是三年來的訓練資料所以就365乘以3了 每年365.3年所以365.3好那我們算出一個L我們算出一個大L這個大L是每一筆訓練資料的誤差這個E啊相加以後的結果這個大L就是我們的Loss這個大L越大代表說我們現在這一組參數越不好這個大L越小代表我們現在這一組參數越好那這個E啊就是計算這個估測的時跟實際的時間的差距其實有不同的計算方法在我們剛才的例子裡面我們是算Y跟Yi結絕對值的差距這種計算差距的方法得到的這種大L啊得到的Loss 叫做 mean absolute error 所寫是 mae那在作業1裡面我們是算 y 跟 y hat 相減以後的平方如果你今天的1是用相減以後的平方算出來的這個叫 mean square error 叫 mse那 mse 跟 mae 他們其實有非常微妙的差別通常你要選擇用哪一種方法來衡量距離那是看你的需求 看你對這個任務的理解在這邊我們就不往下講反正我們就是選擇了 mae作為我們計算這個誤差的方式把所有的誤差加起來就得到 loss那你要選擇 mse 也是可以的 在作業裡面我們會用NAC那有一些任務有如果Y跟Y hat它都是機率分布的話在這個時候你可能會選擇Course Entropy這個我們都之後再說反正我們這邊就是選擇了NAD好的 這個是機器學習的第二步那我剛才舉的那些數字不是真正的例子但是在這門課裡面我在講課的時候就是要舉真正的例子給你看所以以下的數字是真實的例子是這個頻道真實的後台的數據所計算出來的結果 那我們可以調整不同的 W我們可以調整不同的 B窮取各種 W 窮取各種 B我們組合起來以後我們可以為不同的 W 跟 B 的組合都去計算它的 Loss然後就可以畫出以下這一個等高線圖在這個等高線圖上面越偏紅色系代表計算出來的 Loss 越大就代表說這一組 W 跟 B 越差如果越偏藍色系就代表 Loss 越小就代表這一組 W 跟 B 越好拿這一組 W 跟 B 換到 Function 裡面 放到我們的model裡面那我們的預測會越精準所以就可以所以就知道說假設W帶負0.25這個B帶負500就代表說這個W帶負0.25B帶負500就代表說這個頻道每天看的人越來越少而且Loss很大只能真實的狀況不太好如果W帶0.75B帶500那這個正確率會這個估測會比較精準那估測最精準的地方看起來應該是在這裡如果你今天W帶一個很接近1的值B帶一個小小的值比如說100多 這個時候估測是最精準的那這跟大家預期可能是比較接近的就是你拿前一天的點閱的總次數去預測隔天的點閱的總次數那可能前一天跟隔天的點閱的總次數其實差不多的所以大家估測1然後1呢設一個小一點的數值也許你的估測就會蠻精準的那像這樣子的一個等高線圖啊就是你試了不同的參數然後計算它的MODE畫出來的這個等高線圖啊叫做ERROR的SERVICE好那這種是機器學習的第二步好那接下來我們進入機器學習 第三步要做的事情其實是解一個最佳化的問題那如果你知道最佳化的問題是什麼的話也沒有關係我們今天要做的事情就是找一個 W 跟 B把未知的參數找一個數值出來看帶哪一個數值進去可以讓我們的 L 讓我們的 loss 值最小那個就是我們要找 W 跟 B那這個可以讓 loss 最小的 W 跟 B我們就叫做 W star 跟 B star代表說他們是最好的因素 W 跟 B可以讓 loss 值最小好那這個東西要怎麼做呢在這門課裡面 唯一会用到的 optimization 的方法叫做 gradient descent那 gradient descent 这个方法怎么做呢它是这样做的为了要简化曲线我们先假设我们位置的参数只有一个就是 w我们先假设没有 b 那个位置的参数只有 w 这个位置的参数那当我们 w 在不同的数值的时候我们就会得到不同的 loss那这条曲线就是 error surface只是刚才在前一个例子里面我们看到的 error surface 是二维的 二低的那这边只有一个参数所以我们看到的这个 error surface 是一低的好 那怎么样子 找一個W去讓這個Loss的值最小呢那首先呢你要隨機選取一個初始的點那這個初始的點我們叫做W0那這個初始的點往往真的就是隨機的就是隨便選一個真的都是隨機的那在往後的課程裡面我們其實會看到也許有些方法可以給我們一個比較好的W0的值那我們先不講這件事是我們先當作就是隨機的隨便指個骰子隨機決定說W0的值應該是多少那假設我們隨機決定的結果是在這個地方那接下來啊你這樣計算說這個 在 w 等於 860 的時候a w 這個參數對 loss 的微分是多少假設你知道微分是什麼你知道微分是什麼這對你來說不是個問題就計算 a w 對 loss 的微分是多少如果你不知道微分是什麼的話那沒有關係反正我們做的事情就是計算在這一個點在 a w 這個位置的這個 parallel surface 的斜線斜率也就是這一條藍色的虛線它的斜率但如果這一條虛線的斜率是負的那代表什麼意思呢代表說左邊比較高右邊比較低 這個位置附近左邊比較高右邊比較低那如果左邊比較高右邊比較低的話那我們要做什麼樣的事情呢如果左邊比較高右邊比較低的話那我們就把 w 的值變大那我們就可以讓 loss 變小如果算出來的斜率是正的就代表說左邊比較低右邊比較高是這個樣子左邊比較低右邊比較高如果左邊比較低右邊比較高的話那就代表左代表我們把 w 變小把 w 往左邊移我們可以讓 loss 的值變小那這個時候你就應該把 w 的值變小那假設你連斜率是什麼的話是什麼都不知道的話沒有關係你就想像說有一個人 站在這個地方然後他左右環視一下那這個算微分這件事就是左右環視他會知道說左邊比較高還是右邊比較高看哪邊比較低他就往比較低的地方跨出一步那這一步要跨多大呢這一步的步伐的大小取決於兩件事情第一件事情是這個地方的斜率有多大這個地方斜率大這個步伐就跨大一點斜率小步伐就跨小一點另外除了斜率以外就是除了這個微分這一項微分這一項我剛才說他就代表斜率除了微分這一項以外還有另外一個東西 影响步伐的大小这个东西我们这边用eta来表示这个eta叫做learning rate叫做学习数这个learning rate它是怎么来的呢它是你自己设定的你自己决定这个eta的大小如果eta设大一点那你每次参数update就会量很大可能学习可能就比较快如果eta设小一点那参数的update就很慢每次都会只会改变一点点参数的数值那这种你在做机器学习需要自己设定的东西叫做hyperparameter这个我们刚才讲说机器学习的第一步 就是定一個有未知參數的 function而這些參數這些未知的參數是機器自己找出來的但是有...欸你請說好你請說好這其實是一個好的問題我複述一下這個問題有同學問說為什麼 loss 可以是負的呢為什麼 loss 可以是負的呢loss 這個函數是你自己定義的所以在剛才我們的定義裡面我們說 loss 就是估測的值跟正確的值它的絕對值那如果根據剛才 loss 的定義那它不可能是負但是 loss 這個函數是你自己定義的 你可以說我今天要決定一個Loss Function就是絕對值在減一百那你可能就服了所以我這邊這個curve我這邊可能剛才忘了跟大家說明說這個curve並不是一個真實的Loss它是我隨便亂舉的一個例子因為在我今天想要舉一個比較General的case它並不是一個真實任務的Error Surface所以這個Loss的這個curve這個Error Surface它可以是任何形狀我們這邊沒有預測的立場說它一定要是什麼形狀但是確實在真實在剛才這個如果Loss的定義就跟我們剛才定的一樣是絕對值那它就不可能是不可能的 但是NO這個方向是你自己決定的所以它有可能失敗好 既然有同學問問題我們就在這邊停一下看大家有沒有問題想問好 然後住校以後會幫我看那個YouTube的直播來 有人在直播上問問題嗎?如果有的話,請幫我唸一下 by bwd6 你先看好以後再唸給我聽我們就先繼續講我們等一下講到一個段落再繼續回答大家的問題再問一下現場的同學有沒有想到問題呢沒有的話就請容我繼續講那剛才講到哪裡呢剛才講到Hyperparameter這個東西Hyperparameter是你自己設的所以在機器學習的這整個過程中你需要自己設定的這個東西就叫做Hyperparameter那我們說我們要把W0往右移一步這個新的位置就叫做W1這一步的步伐是add乘上每分的節度那如果你沒有這個步伐的話 你要數學式來表示它的話就是把 w0 減掉 eta乘上微分的結果得到 w1那接下來你就是反覆進行剛才的操作你就計算一下 w1這個微分的結果然後呢再決定現在要把 w1 移動多少然後再移動到 w2然後你再繼續反覆做同樣的操作不斷的把 w 移動位置最後你會停下來什麼時候會停下來呢往往有兩種狀況第一種狀況是你失去耐心了你一開始會設定說我今天在調整我的參數的時候我在計算我的微分的時候 我最多計算幾次你可能會設說嗯我的上限就是設定100萬次所以我的參數更新100萬次以後我就不再更新了那至於要更新幾次這個也是一個hyperparameter這個是你自己去如果說大概就是明天那你可能更新的次數就設少一點不要給它下放更新的次數就設多一點那還有另外一種理想上的評價的可能是今天當我們不斷調整參數調整到一個地方它的微分的值就是這一項算出來正好是0的時候如果這一項正好算出來是00乘上這個Learning Rate App還是0所以你的參數就不會上升 移動的位置好 那假設我們是一個理想的狀況我們把W0更新到W1再更新到W2最終更新到WT電腦有點卡更新到WT 卡住了也就是算出來這個微分的值是0了那就不會在參數的位置就不會再更新了那講到這邊你可能會馬上發現說Gradient Descent這個方法哇 有一個巨大的問題這個巨大的問題在這個例子裡面非常容易被看出來就是我們沒有找到真正最好的解決我們沒有找到那個可以讓Loss最小的那個比例 在這個例子裡面把w設定在這個地方你可以讓loss最小但是如果gradient descent是從這個地方當作隨機初始的位置的話你很有可能走到這裡你的序列就停住了你就沒有辦法再移動w的位置那這個位置這個真的可以讓這個loss最小的地方叫做global的minima而這個地方叫做local的minima它的左右兩邊都比這個地方的這個loss還要高一點但是它不是整個error free surface上面的最低點這個東西叫做local minima 你可能會聽到有人講到 gradient descent就會說 gradient descent 不是個好方法這個方法會有 local minima 的問題你沒有辦法真的找到 local minima但這一期教科書常常這樣講農場文常常這樣講但這個其實只是幻覺而已事實上假設你有做過深度學習相關的事情假設你有自己訓練內部自己做 gradient descent 的經驗的話其實 local minima 是一個假議題我們在做 gradient descent 的時候我們真正面對的難題不是 local minima到底是什麼 這個我們之後會再講到在這邊你就先接受先相信多數人的講法說 gradient descent 有 local minima 的問題在這個圖上 在這個例子裡面顯然有 local minima 的問題但之後會再告訴你說gradient descent 真正的透別到底是什麼好 那剛才舉的是只有一個參數的例子而已那我們實際上我們剛才模型有兩個參數有 w 跟 b那有兩個參數的情況下怎麼用 gradient descent 呢其實跟剛才一個參數沒有什麼不同如果一個參數你沒有問題的話你可以很快的推廣到兩個參數好 那謝謝大家 現在有兩個參數那我們給他兩個參數都給他隨機的出12隻就是w0跟b0然後接下來呢你要計算w對nose的微分你要計算b對nose的微分計算在哪計算是在w等於w0的位置b等於b0的位置在w等於w0的位置b等於b0的位置你要計算w對l的微分計算b對l的微分計算完以後就根據我們剛才一個參數的時候的做法去更新w跟b把w0減掉learning rate乘上微分的結果得到w1把b0減掉learning rate乘上微分的結果得到w1 到 B1那有同學可能會問說這個微分這個要怎麼算啊在這方面我們不會算微分的話不用緊張怎麼不用緊張呢在 Deep Learning 的 Framework 裡面或在我們作為議會用的 PyTorch 裡面這個算微分啊都是程式自動幫你算的你就Call 一行你就寫一行程式自動就把微分的值就算出來了你就算完全不知道自己在幹嘛你還是可以把微分的值算出來所以這邊如果你根本就不知道微分是什麼不用擔心這一步驟就是一行程式 這個等一下之後在作業日的時候大家可以自己體驗看看好 那就是反覆同樣的步驟就不斷的更新W跟B然後期待最後你可以找到一個最好的WW star 跟最好B B star好 那這邊呢就是舉一下例子跟大家看一下說如果在這個問題上它操作起來是什麼樣子那假設你隨便選一個初始的值在這個地方那就先計算一下這個W對L的微分計算一下B對L的微分然後接下來你要更新W跟B更新的方向就是W對L的微分乘以Eta再乘以一個負號 然後我的微分再乘以A 乘以一個負號你算出這個微分的值你就可以決定更新的方向你就可以決定W要怎麼更新 B要怎麼更新那把W跟B更新的方向結合起來它就是一個向量就是這個紅色的箭頭我們就從這個位置移到這個位置然後再計算一次微分然後你再決定要走什麼樣的方向把這個微分的值乘上Running Rate再乘上負號你就知道紅色的箭頭要指向哪裡你就知道怎麼動W跟B的位置一直移動 一直移動 一直移動 一直移動期待最後可以找出一組不錯的W跟B好 那實際上呢 真的用歸根比賽進行一番計算以後這個是真正的數據我們算出來的最好的W是0.97最好的B是0.1K跟我們猜測蠻接近的因為XY的值可能跟Y很接近所以這個W就設一個接近1的值B就設一個比較偏小的值那Loss多大呢Loss算一下是0.48K也就是在2017到2020年的資料上如果使用這個還是B帶0.1KR帶0.97那平均的誤差是0.4 也就是它的預測觀看的錯誤差大概是500人次左右好那講到目前為止我們就講了機器學習的三個步驟第一個步驟寫出一個函式這個函式裡面是有未知數的第二個步驟第一個叫做Loss Function第三個步驟JX Optimization Problem找到一組A與B讓Loss最小那A與B是你剛才找出來的那這組A與B可以讓Loss小到0.48K但是這樣是一個讓人滿意後 值得稱道的結果嗎也許不是為什麼因為這三個步驟合起來叫做訓練我們現在是在我們已經知道答案的資料上去計算囉2017到2020年的資料我們已經知道啦我們其實已經知道2017到2020年每天的觀看次數所以我們現在其實只是在自嗨而已我們就是假裝我們不知道隔天的觀看次數然後拿這個函數來進行預測發現誤差是0.48K但是我們真正要在意的是已經知道的觀看次數嗎 不是我們要真正在意的是我們不知道未來的觀看次數是多少好 所以我們接下來要做的事情是什麼呢就是拿這個函式來真的預測一下未來的觀看次數那這邊我們只有2017年到2020年的字我們在這個2020年的最後一天跨年夜的時候找出了這個函式接下來從2021年開始每一天我們都拿這個函式去預測隔天的觀看次數我們就拿2020年的12月31號的觀看次數去預測2021年的字 用2021年元旦的觀看人次預測一下2021年元旦隔天1月2號的觀看人次用1月2號觀看人次去預測1月3號的觀看人次每天都做這件事一直做到2月14號就做到情人節然後得到平均的指數平均的誤差值是多少呢這個是真實的數據的結果在2021年沒有看過的資料上這個誤差值是我們這邊用Lπ來表示它是0.58所以在有看過的資料上在訓練資料上誤差值是比較小在沒有看過的資料上在誤差值上 每個一年的資料上看起來誤差值是比較大的那我們每天的平均誤差有580人左右 600人左右能不能夠做得更好呢在做得更好之前我們先來分析一下結果這個圖怎麼看呢這個圖的橫軸代表的是時間所以0這個點啊最左邊的點代表的是2021年1月1號最右邊的點代表的是2021年2月14號然後這個縱軸啊就是觀看的人次啊這邊是用千人當作單位紅色的線是 红色的线是真实的观看人次蓝色的线是机器用这一个函数预测出来的观看人次你会发现很明显的这个蓝色的线没什么神奇的地方它几乎就是红色的线往右平一天而已它其实也没做什么特别厉害的预测就把红色的线往右平一天这很合理因为我们觉得F1也就是前一天的观看人次跟隔天观看人次的要怎么拿前一天观看人次去预测隔天观看人次呢前一天观看人次在0.97加上0.01加上第八就是隔天 整天的觀看等次所以你會發現說機器幾乎就是拿前一天的觀看等次來預測隔天的觀看等次但是如果你仔細觀察這個圖你就會發現這個真實的資料啊有一個很神奇的現象它是有週期性的啊它有神奇的週期性啊你知道這個週期是什麼嗎你知道這個你知道它每隔七天就會有兩天特別低兩天觀看的人特別少那兩天是什麼日子呢那我發現那兩天都固定是禮拜五跟禮拜六禮拜五跟禮拜六可以了解就禮拜五週末啊大家出去玩啊誰還要學機器學習啊禮拜六我們下禮拜見 誰還要學機器學習啊不知道為什麼禮拜天大家是願意學機器學習這個我還沒有我還沒有參透為什麼是這個樣子也許跟YouTube背後神奇的演算法有關係比如說YouTube在你知道YouTube都會推頻道的影片嘛也許YouTube在推頻道的影片的時候他都選擇禮拜五禮拜六不推只推禮拜天到禮拜四可是為什麼推禮拜天到禮拜四呢這個我也不了解但是反正看出來的結果我們看真實的數據就是這個樣子每隔七天一個循環禮拜五禮拜六看的人就是特別少所以既然我們已經知道每隔七天都是一個循環那這一個式子這一個Model 很爛 因為它只能夠看前一天如果說每個七天開個循環我們應該要看七天對不對我們如果我們一個模型它是參考前七天的資料把七天前的資料直接複製到拿來當作預測的結果也許預測的會更準也說不定所以我們就要修改一下我們的模型那通常對模型的修改往往來自於對這個問題的理解也就是 domain knowledge所以一開始我們對問題完全沒過理解的時候我們就胡亂寫個 y 等於 v 加 w x 半並沒有做得特別好接下來我們觀察了真實的數據以後得到一個結論是每個七天有一個循環所以我們 應該要把前七天的觀看人次都列入考慮所以我們寫了一個新的模型這個模型長什麼樣子呢這個模型就是Y等於B加SJSJ代表什麼這個下標J代表是幾天前然後這個J等於1到7也就是從一天前兩天前一直考慮到七天前那七天前的資料通通乘上不同的weight乘上不同的WJ加起來再加上IF得到預測的結果如果這個是我們的model那我們得到結果是怎麼樣呢我們在訓練資料上的Loss是0.38K那因為這 這邊只考慮一天嘛 這邊考慮七天嘛所以在訓練資料上你會得到比較低的Loss這邊考慮比較多的資訊在訓練資料上你應該要得到更好的更低的Loss這邊算出來是0.38K但是在他沒有看過的資料上面做不做得好呢在沒有看到的資料上有比較好是0.49K所以剛才只考慮一天是0.58K的誤差考慮七天是0.49K的誤差那這邊每一個W跟B啊我們都會用歸根底線算出他的最佳值他的最佳值長什麼樣子呢這邊秀出來給你看他的最佳值長這個樣子 這個邏輯我是沒有辦法了解我本來以為他會選七天前的數據七天前的這個觀看人數直接複製過來不過看來他沒有這樣選就是了他的邏輯是前一天跟你要預測的隔天的數值的關係很大所以W1是0.79那不知道為什麼他還考慮前三天前三天是0.12然後前六天是0.3前七天是0.18不過他知道說如果是前兩天前四天前五天他的值呢會跟未來然後預測的隔天的值是成反比的所以W2 W4跟W5他們最佳的值 讓Loss可以在訓練資料上是0.38K的值是復原但是W1 W3 W6跟W7是正我們考慮前7天的值那你可能會問說能不能夠考慮更多天呢可以那這個請你的感考慮更多天本來只考慮前7天好考慮28天會怎麼樣呢28天就一個月嘛考慮前一個月每一天的觀看人次去預設隔天的觀看人次預設出來結果怎樣呢訓練資料上是0.33K那在2021年的資料上在沒有看過的資料上是0.46K看起來又更好一點好 28天那接下來 在考慮56天會怎麼樣呢在訓練資料上是稍微再好一點是0.32K在沒看過資料上還是0.46K看起來考慮更多天沒有辦法再更進步了看來考慮天數這件事也許已經到了一個極限好那這邊這些模型他們都是把輸入的這個X這個X還記得它叫什麼嗎它叫做Feature把Feature乘上一個Weight再加上一個Piece就得到預測的結果這樣的模型有一個共同的名字叫做Linear Model好那我們接下來會看怎麼把Linear Model做得更好 感谢观看 但他們都有一件事在共同啊 太過簡單了linear的model也許太過簡單了怎麼說它太過簡單呢怎麼說它太過簡單呢我們可以想像說X1跟Y也許它中間有比較複雜的關係但是對linear的model而言對linear的model來說X1跟Y的關係就是一條直線隨著X1越來越高Y就應該越來越大你可以設定不同的W改變這條線的斜率你可以設定不同的B改變這一條藍色的直線 Y軸的交叉點但是無論你怎麼改A6跟B它永遠都是一條直線永遠都是X1越大 Y就越大前一天觀看的人數越多隔天的觀看人數就越多但也許現實並不是這個樣子啊也許在X1小於某一個數值的時候前一天的觀看人數跟隔天的觀看人數是成正比但也許當X1大於一個數值的時候這個物極必反過了一個峰值以後過了一個假設X1太大前天觀看的人數太高那隔天觀看人數就會變少也說不定啊 x1跟y中間有一個比較複雜的像這個紅色線一樣的關係但你不管怎麼擺弄你的w跟b你永遠製造不出紅色那一條線你永遠無法用linear的model製造紅色這一條線所以怎麼辦呢顯然linear的model有很大的限制這一種來自於model的限制叫做model bias那其實我們剛才在課堂一開始的時候也叫做也說b叫做bias那這個地方有一點在用詞上有一點ambiguous那所以這邊特別強調說呢這個東西 叫做model的bias它跟b的這個bias不太一樣它指的意思是說我們今天的所以它沒有辦法模擬真實的狀況所以怎麼辦呢我們需要寫一個更複雜的更有彈性的有未知參數的functionlinear的model顯然是不夠的那怎麼辦呢怎麼寫出一個更複雜的有未知參數的function呢我們可以觀察一下紅色的這一條曲線紅色的這一條曲線它可以看作是一個常數 加上一群藍色的這樣子的function那這個藍色的function它的特性是這個樣子當輸入的值當excel的值小於某一個這個threshold的時候它是某一個定值大於另外一個threshold的時候又是另外一個定值那中間呢有一個斜坡所以它是先水平的然後再斜坡然後再水平的那它其實有名字它的名字我們等一下再講這邊我們因為它是藍色的function我們就先叫它藍方吧這樣子好那所以呢這個紅色的線啊它可以看作是一個長數項 大堆的藍方那這個常數項它的值應該要有多大呢你就看這一條紅色的線它跟X軸的交點在哪裡那這個常數項呢就設跟X軸的交點一樣大那怎麼加上這個藍色的function以後變成紅色的這一條線呢你看就這樣子加這個藍色function它的這個坡度這個斜坡的起點設在紅色function的起始的地方然後第二個斜坡的終點設在第一個轉角處所以這邊紅色function有一個轉角 那你就有一個藍色的 function它的斜坡的終點設在紅色 function 的第一個轉角然後呢 你刻意讓這邊這個藍色 function 的斜坡跟這個紅色 function 的斜坡它們的斜率是一樣的這個時候如果你把 0 加上 1你就可以得到紅色曲線紅色這個線段的第一個到這個第一個轉折點之前的數值所以 0 加上 1可以得到紅色線段第一個轉折點之前的部分然後接下來再加第二個藍色的 function怎麼加呢你就看紅色這個線 第二個轉折點出現在哪裡所以第二個藍色function它的斜坡就在紅色function的第一個轉折點到第二個轉折點之間第一個轉折點到第二個轉折點之間那你可以讓這邊的斜率跟這邊的斜率一樣這個時候你把0加1加2你就可以得到兩個轉折點這邊的線段你就可以得到紅色的這條線這邊的部分然後接下來第三個部分第二個轉折點之後的部分怎麼產生呢你就加第三個藍色的function第三個藍色的function它這個坡度其實也不一樣 物一色的跟這個轉折點一樣這邊的斜率 物一色的跟這邊的斜率一樣接下來你把0加1加2加3全部加起來你就得到紅色的這個線就得到紅色這個線所以紅色這個線可以看作是一個常數再加上一堆藍色的function 你知道AI将在2025年制造97亿新工作吗这些新年工作需要有才华的专家来驾驭AI-基于新創新并帮助生产生发展你准备成为一位吗在过去的几年里AI从简单的规则基于系统到进步的机械学学计算计算进行过程中的巨大数量数据我们在这些发展中见证了 从医疗保险到财政 公共交通 娱乐等想参与这高增长的行业吗让我介绍给你PG的机器学和智能研究计划是由德州大学在奥斯丁的合作与伟大的学习在这个计划的过程中你将掌握AI和机器学的基础由世界著名的学生和业界专家你将学会 从教育中学习出来的专业通过现实世界的设计获得手工体验并参与实际教育课程在这段旅程结束后你将能够获得在广泛使用的AI ML工具和技术中的知识辨识和解决商业问题建立可供使用的机器学习和深入学习模式了解AI的应用在像是电脑视觉和自然语言处理中 展示一份业界准备的 e-portfolio 与一份业界丰富的工作体系获得一份资格完成证 从德州大学的 博士生涯以及更多的我们相信 最好的学习经验 建立于人类互动的基础上这就是为什么 这个计划设计 与业界专家进行直播教育课程这些互动会在小组中 进行并协助您 获得个人指导 例如 计算机 案例 专案专业学习 由主流公司的 AI 专业学习者包括 MicrosoftSAPVerizonIBM 和其他多个公司这些导师会带你通过专业的专业项目来增强你的 e- 行业专业的计算机管理员来确保你能够顺利完成所有学习目标一人一人的工作支持和专业评测课程为了确保你能展示出最好的一面 这一项计划是您想要学习的新年工具成长在您的现任职位想要转换成新的工作职位在AI或机器学习是一个有经验的专业人士想要在国家领导职位是一个早期职业专业人士在AI ML上寻找获益的工作更多的是这项计划是设计和制作与您的工作业务相关的更多的是这项计划是设计和制作 而是由德州大學的敬業教授教導的他們的焦點是應付每個學習者的能力並幫助他們達成自己的潛力他們在業界的廣泛經驗會幫助你成為在市場上最受歡迎的專業人士準備負責你的事業嗎?加入PG的智能智能學習以解鎖世界的機會 MING PAO CANADA MING PAO TORONTO 踏入 Wix 工作室,從一個平台開始處理每個部分的建築將你的項目圖示出,以 AI 能力的視覺地圖將地圖構造、電線圖形,在秒數,而不是日子中當你的客戶登機時,進入設計自由地創造,甚至將最小的元素調整使用地圖方式,以積極延伸品牌到每個頁面並觀察它適應地跨越磨擦點Now, SCALE 你能夠在CMS中不斷地建造數量的數據從重複的數據到數百個動態的專頁商業解決方案已經建立在這裡所以當客戶想要更多的功能你就可以更快地去做計劃快一點設計自由一點規格更聰明這就是Wix工作室那你仔細想一下就會發現說不管我畫什麼樣的平面線條什麼叫做平面線條呢 就是你現在這個curve它是由很多線段所組成的它是由很多鋸齒狀的線段所組成的這個叫做piecewise linear的curve那你會發現說這些piecewise linear的curve你有辦法用常數項加一大堆的藍色function組合出來只是它們用的藍色function不見得一樣你要有很多不一樣的藍色function加上一個常數以後你就可以組出這些piecewise linear的curve那如果你今天piecewise linear的curve越複雜也就是這個轉折的點越多那你需要的這個藍色的function就越多所以呢那講到這邊有人可能會 我們會說那也許我們今天要考慮的x 跟 y 的關係不是 piecewise linear 的 curve也許它是這樣子的曲線那就算是這樣的曲線也無所謂我們可以在這樣的曲線上面先取一些點再把這些點點起來變成一個 piecewise linear 的 curve而這個 piecewise linear 的 curve跟原來的曲線它會非常接近如果你今天點取的夠多或你點取的位置適當的話你點取的夠多這個 piecewise linear 的 curve就可以逼近這個連續的這個曲線就可以逼近這個不是 piecewise linear 它是有角度的有弧度的這條曲線所以我們今天知道一件事情你可以用piecewise linear的curve去逼近任何的連續的曲線而每一個piecewise linear的curve又都可以用一大堆藍色的function組合起來也就是說我只要有足夠的藍色function把它加起來我也許就可以變成任何連續的曲線所以今天假設我們的x跟y的關係它也許非常的複雜那也沒關係我們就想辦法寫一個帶有未知數的functionB��i few 表示的就是一堆藍色的function加上一個constant那我們接下來問的問題就是這個藍色function它的式子應該要怎麼把它寫出來呢怎麼把這個藍色function的式子寫出來呢也許你要直接寫出它沒有那麼容易但是你可以用一條曲線來逼近它用什麼樣的曲線來逼近它呢用一個sigmoid的function來逼近這個藍色的function那sigmoid function它的式子長的是這個樣子的它的橫軸輸入是x1 輸出是y所有的x1我們先乘上一個w 再加上一個 b 再取一個負號再取 exponential 再加 1這一算被放在分子 放在分母的地方把 1 除以 1 加上 exponential 負 b 加 w x1前面可以乘上一個 constant 叫做 c那如果你今天輸入的這個 x1 的值趨近於無窮大的時候 會發生什麼事呢如果這一項趨近於無窮大那 exponential 這一項就會消失那當 x1 非常大的時候這一條這邊就會收斂在這個高度是 c 的地方那如果今天 x1 負的非常大的時候會發生什麼事呢如果 x1 負的非常大的時候會發生什麼事呢 角度非常大的時候分母的地方就會非常大那 y 的時就會趨近於0所以你可以用這樣子的一個 function來試著畫出這條曲線用這條曲線來逼近這一個藍色的 function那這個東西它的名字叫做 sigmoidsigmoid 是什麼意思呢sigmoid 如果你要硬要翻成中文的話可以翻成 s 型的所以 sigmoid function 就是 s 型的 function它長得是有點像是 s 型的所以叫它 sigmoid function那這邊我們之後都懶得把 exponential 寫出來我們就直接寫成這個樣子 等於 c 倍的 sigmoid然後這個括號裡面放 b 加 w 乘 x1然後這個 b 加 w x1實際上做的事情就是把它放在 exponential 的指數項前面加一個負號然後一加 exponential 的負b 加 w x1 放在分母的地方而且會乘上 c 就等於 -感覺有些東西不一樣-我不知道發生了什麼-這不是...只是雨雨歪好 所以我們可以用這個 sigmoid function去逼近一個藍色的 function那其實這個藍色的 function比較常見的名字就叫做hard的 sigmoid只是我本來想說一開始我們是先介紹藍色的 function來介紹 sigmoid所以一開始說它叫做 Hard sigmoid 有點奇怪所以我們先告訴你說有一個 sigmoid function它可以逼近這個藍色的 function那這個藍色的 function其實通常就叫做 Hard sigmoid那我們今天我們需要各式各樣不同的藍色的 function還記得嗎 我們要組出各種不同的曲線那我們就需要各式各樣合適的藍色的 function而這個合適的藍色的 function 怎麼製造出來呢我們就需要調整這裡的 b 跟 w 跟 c你可以調整 b 跟 w 跟 c你就可以製造各種不同形狀的 sigmoid function用各種不同形狀的 sigmoid function去逼近這個藍色的 function 舉例來說 如果你今天改w會發生什麼事呢你就會改變斜率就會改變斜坡的坡度如果你動了b會發生什麼事呢你就可以把這個sigmoid function左右移動如果你改c會發生什麼事呢你就會改變它的高度所以你只要有不同的w 不同的b 不同的c你就可以製造出不同的sigmoid function把不同的sigmoid function疊起來以後你就可以疊出各種不同的你就可以去逼近各種不同的piecewise linear function而piecewise linear function可以拿來近似各種不同的continuous function所以今天假設我們要把紅色的這條線它的函數寫出來的話那可能長什麼樣子呢我們知道說紅色這條線就是0加1加2加3而這個1 2 3他們都是藍色的function所以他們的函數就是有一個固定的樣子他們都寫做x1乘上w再加上b做sigmoid再乘上c1只是1跟2跟3他們的w不一樣他們的b不一樣他們的c不一樣 如果是第一個綠色第一個藍色function他就是W1 B1 C1第二個藍色function我們就說他用的是W2 B2 C2第三個藍色function我們就說他用的是W3 B3 C3那我們接下來就是把0跟1 2 3全部加起來以後我們得到的函式就長這個樣子我們把1加2加3加起來這邊就是summation over i我們的i等於1或2或3然後summation裡面就是Ci 乘上sigma由bi加wi乘上x1所以這邊每一個式子都代表了一個不同藍色的functionsummation的意思就是把不同的藍色的function給它加起來就是這邊summation的意思然後別忘了加一個constant這邊用b來表示這個constant所以今天我們有一個如果我們今天就寫出了一個這樣子的function如果我們假設裡面的b跟w跟c它是未知的它是我們未知的參數那我們就可以設定不同的b跟w跟c設定不同的b跟w跟c我們就可以製造不同的藍色的function 制造不同的藍色方向疊起來以後就可以制造出不同的紅色的curve制造出不同的紅色的curve就可以制造出不同的piecewise linear的curve就可以去逼近各式各樣不同的continuous的function所以我們其實有辦法寫出一個這個非常有彈性的有未知參數的function它長這樣子就是summation一堆sumoi但它們有不同的c不同的b不同的r好 那所以本來我們是linear的modely等於b加w乘上x1它有非常大的限制這個限制叫做model的bias那我們要如何減少嗎我們要如何減少嗎 我們可以寫一個更有彈性的有位置參數的功能它叫做 y 等於 b 加 summation c i sigmoid b i 加 w i x 1本來這邊是 b 加 w x 1 這邊變成 b i 加 w i x 1然後我們有很多不同的 b i 有很多不同的 w i他們都通過 sigmoid 都乘上 c i 把它都加起來再加 b 等於 y我們只要代入不同的 c 不同的 b 不同的 w我們就可以變出各式各樣就可以組合出各式各樣不同的功能那我們剛才其實已經進化到不是只用一個功能是一個feature 啊我們可以用多個feature 我們這邊用j來代表feature的編號舉例來說剛才如果要考慮前28天的話j就是1到28考慮前56天的話j就是1到56那如果要把這個function再擴展成我們剛才講的上面這個比較有彈性的function的話那也很簡單我們就把sigmoid裡面的東西換掉本來這邊是b加summation over j wj xj那這邊就把這一下放到這個括號裡面改成bi加summation over j wij xj我們把本來放在這邊的東西放到sigmoid裡面 然後每一個Symoy的function裡面都有不同的bi不同的wij然後取Symoy以後再加上ci就全部加起來再加上b就得到y我們只要這邊ci bi跟wij放不同的值就可以變成不同的function好 那如果講到這邊你還是覺得有點抽象的話因為你看這個式子覺得有點頭痛的話那我們用比較直觀的方式把這個式子實際上做的是把它畫出來它畫出來看起來像是這個樣子好 我們先考慮一下j就是1 2 3的狀況就是我們只考慮三個feature舉例來說我們只考慮前一天 前兩天跟前三天的case所以j等於1 2 3那所以輸入就是x1代表前一天的觀看人數x2兩天前觀看人數x3三天前的觀看人數i是什麼i是每一個i就代表了一個藍色的function只是我們現在每一個藍色的function都用一個sigmoid function來禁制它所以每一個i就代表了一個sigmoid function或者是代表了一個藍色的function好 那這邊呢這個1 2 3就代表我們有三個sigmoid function那我們先來看一下這個括號裡面做的事情是什麼每一個sigmoid都有一個括號 這個括號裡面做的事情是什麼呢好第一個 sigmoid i 等於 1 的 case就是把 x1 乘上個 weight 叫 w1 1x2 乘上另外一個 weight 叫 w1 2x3 再乘上一個 weight 叫做 w1 3全部把它加起來那不要忘了再加一個 b把 b 加起來然後呢這個得到的式子就是這個樣子所以這邊我們用 w i j 呢來代表在第 i 個 sigmoid 裡面乘給第 j 個 feature 的 weight第一個 feature 它就是 w1 1第二個 feature 就是乘 w1 2第三個 feature 就是W13所以三個feature 123這個W的第二個下標就是123W的第一個下標呢代表是現在在考慮的是第一個Sigmoid function那我們有三個Sigmoid function好 那第二個Sigmoid function呢我們就不把它的W寫出來了我們就不把它的W放在這個箭頭旁邊不然會太擠那第二個Sigmoid function它的在括號裡面做的事情是什麼呢它在括號裡面做的事情就是把X1X1乘上W21把X2乘上W22把X3X3乘上W23通通加起來再加B2第三個Sigmoid呢 Sigmoid在括號裡面做的事情就是把123 123 x1 x2 x3分別乘上W31 W32跟W33再加上比3那我們現在為了要簡化起見我們把括弧裡面的數字用比較簡單的符號來表示所以這一串東西我們當作R1這一串東西我們當作R2這一串東西我們當作我們叫它R3那這個x1 x2跟x3和R1 R2 R3中間的關係是什麼呢你可以用矩陣跟向量相乘的方法寫一個比較簡單的簡潔的寫法 我們剛才已經知道說 R1 R2 R3也就是跨弧裡面算完的結果三個Symbol跨弧裡面算完的結果R1 R2 R3跟輸入的三個Feature X1 X2 X3他們中間的關係就是這樣把X1 X2 X3乘上不同的位加上不同的Bias也就是不同的B會得到不同的R那這三個字這一連串的運算其實我們可以把它簡化就如果你熟悉線性代數的話簡化成矩陣跟向量的強乘把X1 X2 X3拼在一起變成一個向量這邊所有的W通通放在一起變成一個矩陣把B1 B2 B3 拼起來變成一個項量吧R1 R2 R3拼起來變成一個項量那這三個式子你就可以簡寫成有一個項量叫做X這個X乘3個矩陣叫做W這個W裡面有9個數值就是這邊的9個R6就是這邊的9個位X先乘上W以後再加上B就得到R這個項量那這邊做的事情跟這邊做的事情是一模一樣的沒有半毛錢的不同只是表示的方式不一樣而已只是本來寫三個數字裡面有一堆++-有一堆還有什麼上標結果還有什麼兩個下標什麼看起來就讓人佛大 改成漸進代數比較常用的表示方式X乘上矩陣大略再加上向量B會得到一個向量叫做R 好 那所以這邊這件事情在這個括號裡面做的事情就是這麼一回事把X乘上大倫加上B等於RR就是這邊的R1 R2 R3我這電腦有點卡 微卡沒辦法控制這個滑鼠沒關係 我可以控制了控制了就是R1 R2 R3好 那接下來這個R1 R2 R3就要分別通過Sigmoid Function分別通過Sigmoid Function我們實際上做的事就是R1 R2 R3R2 R3 R2 R3R1 R2 R3我們實際上做的事就是做的事情就是 把 r1 取一個負號再做 exponential 再加 1然後把它放到分母的地方1 除以 1 加 exponential 負 r1 等於 a1然後同樣的方法由 r2 取得到 a2把 r3 透過 sigmoid function 得到 a3所以這邊這個藍色的虛線框框裡面做的事情就是從 x1 x2 x3 得到了 a1 a2 a3接下來我們這邊有一個簡潔的表示方法是我們用 r 通過一個叫做 sigmoid function我們用這個東西我們這邊用這個符號來代表通過 sigmoid function然後呢 說我們得到了A這個向量就把R1 R2 R3分別通過Symoy function那我們直接用這個符號來表示它然後得到A1 A2 A3然後接下來呢接下來我們這個Symoy的輸出還要乘上C1 I然後還要再加上B那我們這邊做的事情就是把A1乘C1A2乘C2 A3乘C3通通加起來再加上B最終就得到了Y那這邊呢如果你要用向量來表示的話A1 A2 A3拼起來叫這個向量AC1 C2 C3拼起來叫一個向量C那我們這邊把這個C 做 transpose那 A 呢乘上 C 的 transpose再加上 B我們就得到了 Y所以這一連串的運算剛才寫的那一個我們說比較有彈性的式子它整體而言做的事情就是X 輸入是 X我們的 feature 是 X 這個向量X 乘上矩陣 W 加上向量 B得到向量 R再把向量 R 通過 sigmoid function得到向量 A再把向量 A 跟乘上 C 的 transpose加上 B 就得到 Y所以這上面這件事情如果你想要用線性代數的方法來表示它 用向量體制的相乘方法來表示它就長得一副這個樣子那這邊的這個R就是這邊的R這邊的A就是這邊的A所以我們可以把這一串東西放到這個括號裡面再把這個A放到這裡來所以把相同的東西併起來以後整體而言就是長這個樣子上面這一串東西我們覺得比較有彈性的這個function如果你要線性代數來表示它的話就是下面這個式子x乘上w再加上b通過sigmoid function乘上c的trend pose加b就得到y上面這一串就是下面這一串就是我剛才寫的那個比較有彈性的function講來講去都是一樣的東西 只是不同的表示方式而已上面這個是圖示化的表示方式下面這個是線性代數的表示方式其實都在講同一件事情好 那接下來在我們繼續講說要怎麼把這些未知的參數找出來之前我們先再稍微重新定義一下我們的符號這邊的這個X是feature這邊的WBC跟B這邊有兩個B但是這兩個B是不一樣的這邊這個是一個向量這邊是一個數值你看他們的這個底色是不一樣的這個是綠色這個是灰色顯示他們是不一樣的東西 我们把这个黄色的这个这个W把这个B把这个C把这个B统统拿出来集合在这边他们就是我们的unknown的parameter就是我们的未知的参数那我们把这些东西统统拉直拼成一个很长的向量我们把W的每一个row或者是每一个column拿出来这边不管你是拿row或拿column都可以啦意思是一样啦你就把W的每一个column或每一个row拿出来拼成一个长的向量把B拼上来把C拼上来把B拼上来这个长的向量我们直接用一个符号这样做 由θ來表示它θ是一個很長的向量裡面的第一個數值我們叫θ1第二個叫θ2 第五個叫θ3那θ裡面 這個向量裡面有一些數值是來自於這個矩陣有一些數值是來自於b有一些數值來自於c有一些數值來自於這邊這個b但我們就不分了反正θ它統稱我們所有的未知的參數我們就一律統稱θ 在瑞麗兒童醫院的心臟病患心臟病患中心動作是我們所有的一切我們全球級的心臟專家團隊的無懈可擊的動作我們現代藝術技術的迅速的動作最後是我們患者的自由自由的動作患者從世界各地來到我們這裡因為在瑞麗兒童醫院我們知道動作有多強大以及它能帶領我們所有人 那这边我们就是换了一个新的我们就重新改写了机器学习的第一步我们重新定了一个有未知参数的function那接下来我们就要进入第二步跟第三步那在我们进入之前我们来看大家有没有问题想要问的 好 那你看線上人員有要問問題嗎那個沒有問題他是說load上數據是說算很複雜 時間也有所以需要optimization的過程去找到一個組那他這樣寫什麼好 我試著回答看看我猜他的問題是說我們其實要做optimization這件事找一個可以讓load最小的參數有一個最暴力的方法就是報收所有可能的未知參數的值對不對像我們剛才在只有W跟B兩個參數的前提之下我根本就可以報收所有可能的W跟B的組合所以在參數很少的情況下甚至你有可能不用 combination 的技巧但是我們今天參數很快就會變得非常多像在這個例子裡面參數有一大把有WB有C跟B串起來變成一個很強的向量叫SAN那這個時候你就不能夠用報收的方法了你需要 gradient descent 這樣的方法來找出可以讓 loss 最低的參數好希望這樣回答到他的問題在座還有同學有問題嗎來請說 没说错吧 可以 非常 這是一個這個同學的問題是說剛才的例子裡面有三個 sigmoid那為什麼是三個呢能不能夠四個五個六個呢可以 sigmoid 的數目是你自己決定的而且 sigmoid 的數目越多你可以產生出來的 piecewise linear的 function 就越複雜就是假設你只有三個 sigmoid意味著你只能產生三個線段但是假設你有越多 sigmoid你就可以產生有越多線段的piecewise linear 的 function你就可以逼近越複雜的 function但是至於要幾個 sigmoid這個又是另外一個 hyperparameter這個你要自己決定我們在剛才的例子裡面取三個 一個例子也許我以後不應該取三個因為這樣會讓你誤以為說input feature是三個 sigmoid 是三個不是就是說 sigmoid 幾個可以自己決定好 這樣回答大家還有問題想問嗎請說跟什麼 sigmoidhard 的 sigmoid 啊首先那個 function你寫出來可能會比較複雜你一下子寫不出他的 function但如果你要你可以寫得出他的 function 的話你其實也可以用 hard 的 sigmoid你想要用也可以所以不是一定只能夠用剛才那個 sigmoid去逼近那個 hard sigmoid你完全有別的做法等一下我們就會講別的做法好 答案我問你 有问题想要问吗 如果目前暫時沒有的話就請容我繼續講下去那你知道這門課是6點20才下課啦所以只要講到6點20前都是可以的那如果你有事想要早點離開也沒有問題我們課程都是有錄影好 那接下來進入第二步啦我們要訂Loss有了新的這個Model以後我們Loss會不會有什麼不同啊沒有什麼不同定義的方法是一樣的只是我們的符號改了一下之前是L of W跟B因為W跟B是未知的那我們現在接下來的未知的參數很多啦你再把它一個一個列出來 所以我們直接用θ來統設所有的參數用θ來代表所有位值的參數所以我們現在的Loss Function就變成Lossθ這個Loss Function要問的就是這個θ如果它是某一組數值的話會有多不好 會有多好那計算的方法跟剛才只有兩個參數的時候其實是一模一樣的就你先給定某一組WBCT跟B的值你先給定某一組θ的值假設你知道W的值是多少把W的值寫進去 B的值寫進去C的值寫進去 B的值寫進去然後呢 你把一組feature x 然後看看你估測出來的Y是多少再計算一下跟真實的Label之間的差距你得到一個T把所有的誤差統統加起來你就得到你的Loss那接下來下一步就是OptimizationOptimization的Problem跟前面講的有沒有什麼不同呢沒有什麼不同它是一樣的所以就算我們換了一個新的模型這個Optimization的步驟Optimization的演算法還是歸零Descent看起來其實沒有真的太多的差別我們現在的Seda它是一個很長的向量我們把它表示成Seda1 Seda2 Seda3 我們現在就是要找一組SETA這個SETA可以讓我們的Loss越小越好可以讓Loss最小的那一組SETA我們叫做SETA的SUM好 那怎麼找出這個SETA的SUM呢我們一開始要隨機選一個初始的數值那這邊叫做SETA0你可以隨機選那之後也可能會講也會講到更好的找初始值的方法我們現在先隨機選就好那接下來呢你要計算為分你要對每一個未知的參數這邊用SETA1 SETA2 SETA3來表示你要為每一個未知的參數 都去計算它對L的微分那把每一個參數都拿去計算對L的微分以後集合起來它就是一個向量這個向量我們用G來表示它這邊假設有1000個參數那這個向量的長度就是1000這個向量裡面就有1000個數字這個東西有一個名字就我們把每一個參數對L的微分集合起來以後它有一個名字這個向量的名字叫做gradient那很多時候你會看到gradient的表示方法是這個樣子的你把L前面放了一個倒三角形 那這個就代表了 gradient這是一個 gradient 的簡寫的方法那其實我要表示的就是這個向量我前面放一個倒三角形的意思就是把所有的參數θ1,θ2,θ3 通通拿去對我做微分就是這個倒三角形的意思那後面放θ0的意思是說我們這個算微分的位置是在θ等於θ0的地方我們算出這個 gradient算出這個 g 以後接下來我們就要 update 我們的參數了我們就要更新我們的參數了更新的方法跟剛才只有兩個參數的狀況是一模一樣的只是從更新兩個參數可能換成更新成一千個參數 但更新的方法是一樣的本來有一個參數叫SETA1那上標0代表它是一個起始的值它是一個隨機選的起始的值那這個SETA10減掉learning rate乘上微分的值得到SETA11代表SETA1更新過一次的結果SETA20減掉微分乘以減掉learning rate乘上微分的值得到SETA21那以此類推你就可以把那一千個參數通通都更新了那這邊有一個簡寫就是你會把這邊所有的SETA合起來當這個項量我們用SETA0來表示這邊你可以把learning rate提出來那剩下的部分微分的部分 每一個參數對L為分的部分叫做歸點叫做G所以SETA0減掉任意RAY乘上G就得到SETA1把這邊的所有的SETA統統集合起來把這邊所有的SETA統統集合起來就叫做SETA1SETA0減掉SETA0是個向量減掉任意RAY乘上GG也是個向量會得到SETA1那假設你這邊的參數有1000個那SETA0就是有1000個數值1000維的向量G是1000維的向量SETA1也是1000維的向量好那整個操作就是這樣啦就是有SETA0算歸點根據歸點去把SETA0更新成SETA1然後呢 再算一次Gradient然後根據Gradient把θ1再更新成θ2再算一次Gradient把θ2更新成θ3一直再推直到你不想做或者是你算出來的這個Gradient是0項量是Zero vector導致你沒有辦法再更新參數為止不過在實作上你幾乎不太可能做出Gradient是0項量的結果通常你會停下來就是你不想做了好 那但是實作上那這邊是一個實作的Detail Issue之所以在這邊重點其他是因為助教的程式裡面有這一段所以我們必須要講一下別人注意看助教程式的時候覺得有點固惑實際上我們在 做Gradient Descent的時候我們會這麼做我們這邊有大N比資料我們會把這大N比資料分成一個一個的Batch就是一包一包的東西 一組一組的怎麼分 隨機分就好隨機分就好好 所以每一個Batch裡面有大B比資料所以本來全部有大N比資料現在大B比資料一組 大B比資料一組每一組叫做Batch怎麼分組 隨便分就好隨便分就好那本來我們是把所有的Data拿出來算一個Loss那現在我們不這麼做我們只拿一個Batch裡面的Data 只拿B比θ出來算一個Loss我們這邊把它叫L1那跟這個L是區別因為你把全部的資料拿出來算Loss跟只拿一個Batch拿出來的資料拿出來算Loss它不會一樣嘛所以這邊用L1來表示它但是你可以想像說假設這個B夠大也許L跟L1會很接近也說不定好 所以實作上的時候每次我們會先選一個Batch用這個Batch來算L根據這個L1來算Gradient用這個Gradient來更新參數接下來再選下一個Batch算出L2根據L2算出Gradient然後再更新參數再取下一個Batch算出L3根據L3 算出Gradient再用L3算出來的Gradient來更新參數所以我們並不是拿大L來算Gradient實際上我們是拿一個Batch算出來的L1 L2 L3來計算Gradient那把所有的Batch都看過一次叫做一個APOC每一次更新參數叫做一次Update所以在文獻上常常會有人聽到Update這個詞彙常常有人聽到APOC這個詞彙那Update跟APOC是不一樣的東西每次更新一次參數叫做一次Update把所有的Batch都看過一遍叫做一個APOC為了要 那至於為什麼要分一個一個Batch那這個我們下週再講好但是為了讓大家更清楚認識Update跟Apple的差別這邊就舉個例子假設我們用1萬筆Data也就是大N等於1萬假設我們的Batch的大小是這10也就是大B等於10接下來問你我們在一個Apple中總共Update了幾次參數呢那你就算一下這個大N個Example1萬筆Example總共形成了幾個Batch總共形成了1萬除以10也就是1000個Batch所以在一個Apple裡面你其實已經更新了參數1000次所以一個App並不容易 不是更新參數一次在這個例子裡面一個APP已經更新了參數一千次了第二個例子那就是假設有一千個資料Batch Size是一檔那其實Batch Size的大小也是你自己決定的所以這邊我們又多了一個Hyperparameter所謂Hyperparameter剛才講過就是你自己決定的東西人手設的東西不是機器自己找出來的叫做Hyperparameter我們今天已經聽到了London Rail是個Hyperparameter幾個Symbol也是一個HyperparameterBatch Size也是一個Hyperparameter好 一千個exampleBatch Size 那一個AirPods總共更新幾次參數呢?是10次所以有人跟你說我做了一個AirPods的訓練那你其實不知道他更新了幾次參數有可能1000次有可能10次取決於他的Batch Size有多大好 那我們其實還可以對模型做更多的變形剛才我同學問到說咦 這個Hard的Sigmoid不好嗎?為什麼我們一定要把它換成Soft的Sigmoid你確實可以不一定要換成Soft的Sigmoid有其他的做法舉例來說這個Hard的Sigmoid 我剛才說它的函數有點難寫出來其實也沒有那麼難寫出來它可以看作是兩個rectifier linear unit的加總所謂rectifier linear unit它就是長這個樣子就是它有一個水平的線走到某一個地方有一個轉折的點然後變成一個斜坡那這種function它的式子寫成C乘上max0b加wx1這個max0b加wx1的意思就是看0跟b加wx1誰比較大比較大的那個就會被當作輸出所以如果b加wx1小於0那輸出就是0如果b加 Wx1大於0 輸出就是B加Wx1那總之這一條線可以寫成Cmax0B加Wx1那你調不同的W不同的B不同的C你就可以挪動它的位置你就可以改變這條線的斜率那這種線在機器學習裡面我們叫做Rectifier Linear Unit它的縮寫叫做ReLU它名字唸起來蠻有趣的它真的就是唸ReLU那你把兩個ReLU疊起來就可以變成Hard Simulator對不對我們把這樣子的一個ReLU疊這樣子的一個ReLU把它們加起來它就變成Hard Simulator 所以我們能不能用ReLU呢?可以所以如果我們不要用Sigmoid你想用ReLU的話你就把Sigmoid的地方換成max括號0EI加summation over jWIJXI那本來這邊只有I個Sigmoid但我想說你要兩個ReLU才能夠合成一個hard Sigmoid所以這邊有I個Sigmoid那如果ReLU要做到一樣的事情你可能需要兩倍的ReLU因為兩個ReLU合起來才是一個hard Sigmoid所以這邊要兩倍的ReLU好 所以我們把Sigmoid換成ReLU這邊就是把一個式子換了因為要表示一個這個hard的Sigmoid表示那個藍色的方式不是只有一種做法 也完全可以用其他的做法好 那這個Sigmoid或是ReLU他們在機器學習裡面我們就叫它Activation Function他們是有名字的他們統稱為Activation Function當然還有其他常見的還有其他的Activation Function但Sigmoid跟ReLU應該是今天最常見的Activation Function那哪一種比較好呢這個我們下次再見哪一種比較好呢我接下來的實驗都選擇用了ReLU簡單ReLU比較好至於它為什麼比較好那就是下週的事情了好 接下來就真的做了這個實驗這個都是真實的數據你知道嗎真的做了這個實驗 如果是linear的model我們現在考慮56天訓練資料上面的Loss是0.32K沒看過的資料2021年的資料是0.46K如果用10個relu好像沒有進步太多這邊跟用linear是差不多的所以看起來10個relu不太夠100個relu就有顯著的差別了100個relu在訓練資料上的Loss就可以從0.32K降到0.28K100個relu我們就可以製造比較複雜的曲線本來linear就是一直線但是100個relu我們就可以產生100個有100個折線的piecewise linear 在測試資料上也好了一些接下來換1000個review1000個review在訓練資料上Loss更低了一些但是在沒看過的資料上看起來也沒有太大的進步好 接下來還可以做什麼呢我們還可以繼續改我們的模型舉例來說剛才我們說從X到A做的事情是什麼是把X乘上W加B再通過Sigmoid Function不過我們現在知道說不一定要通過Sigmoid Function通過review也可以然後得到A我們可以把這個同樣的事情再反覆的多做幾次剛才我們把W X A x 乘上 w 加 b 通過 sigmoid function 得到 a我們可以把 a 再乘上另外一個 w'再加上另外一個 b'再通過 sigmoid 或 reduce function 得到 a'所以我們可以把 x 做這一連串的運算產生 a接下來把 a 做這一連串的運算產生 a'那我們可以反覆的多做幾次那要做幾次欸 這個又是另外一個 hyperparameter這是另外一個你要自己決定的事情你要做兩次嗎 三次嗎 四次嗎 一百次嗎這個你自己決定不過這邊的 w 跟這邊的 w'他們不是同一個參數喔這個 b 跟這邊的 b' 他們不是同一個參數 是同一個參數是增加了更多的未知的參數好 那就是接下來就真的做了實驗了我們就是每次都加100個review那我們就是input的feature就是56天前的資料如果是只做一次只做一次就那個乘上W再加B再通過review或signal這件事只做一次的話這是我們剛才看到的結果兩次 哇 這個Loss降低很多啊0.28K降到0.18K沒看過的資料上也好了一些三成 哇 又有進步從0.18K降到0.14K 所以從一層到就是乘一次W到通過一次REDUCE到通過三次REDUCE我們可以從0.28K到0.14K在訓練資料上在沒看過資料上從0.43K降到了0.38K看起來也是有一點進步的好 那這個是真實的實驗結果了就我們來看一下今天有做通過三次REDUCE的時候做出來的結果怎麼樣橫軸剛才已經看過了就是時間就是日子縱軸是觀看的人次是千人紅色的線代表的是真實的數據藍色的線 是預測出來的數據那你會發現說欸 在這種低點的地方啊你看紅色的數據是每隔一段時間就會有兩天的低點就會有兩天的低點在低點的地方機器的預測還算是蠻準確的它都準確抓到說這兩天就是低的這兩天都是低的這兩天就是低的這兩天就是低的那這邊有一個神奇的事情這個機器高估了真實的觀看人次尤其是在這一天這一天有一個很明顯的低谷但是機器沒有預測到這一天有明顯的低谷它是晚一天才預測出低谷那你知道是怎麼回事嗎蛤 欸 論年 不是 因為還沒有到2月28號嘛 大家有什麼想法嗎 各位 過年啦這一天最低點是什麼今天最低點就是除夕啦誰除夕還學機器學習對不對所以對機器來說你不能怪他他根本不知道除夕是什麼他只知道看前56天的值來預測下一天會發生什麼事他不知道那一天是除夕所以你不能怪他預測的不準這一天就是除夕好 那到目前為止我們講了很多各式各樣的模型那我們現在還缺了一個東西你知道缺什麼東西嗎缺一個好名字你知道這個外表啊是很重要的一個死破酸宅穿上西裝以後就潮了起來或者是芝奇範呂的 說他是漢佐將軍夷城停留中山晉王之後也就潮了起來 對不對所以我們的模型也需要一個好名字其實它叫做什麼名字呢這些Sigmund 或 Relu他們叫做 Neuron我們這邊有很多的 Neuron很多的 Neuron 叫什麼很多的 Neuron 就叫做 Neuron NeuronNeuron 是什麼Neuron 就是神經元人腦中就是有很多神經元很多神經元串起來就是一個神經網路跟你的腦是一樣的接下來你就可以到處騙麻瓜說看到沒有這個模型就是在模擬人腦 知道嗎這就是在模擬人腦 這個就是人工詞彙然後麻瓜就會下午把錢掏出來但是這個把戲在80、90年代的時候已經玩過了Neural Network不是什麼新的技術80、90年代就已經用過了當時已經把這個技術的名字搞到臭掉了Neural Network因為之前吹捧的太過浮誇所以後來大家對Neural Network這個名字都非常感冒它就像是個髒話一樣寫在paper上面都會註定害你的paper被拒絕所以後來為了要重振Neural Network的雄風所以怎麼辦呢 需要新的名字怎麼樣新的名字呢這邊有很多的 Neural每一排 Neural我們就叫它一個 Layer他們叫 Hidden Layer有很多的 Hidden Layer就叫做 Deep這整套技術就叫做 Deep Learning好 我們就把 Deep Learning 講完了就是這個 就是這麼回事就是這樣來好 所以人們就開始把內神經網路越疊越多越疊越深12年的時候有一個 ANS NET他有八成他的錯誤率是16.4%兩年之後 VGG 19成錯誤率在影像辨識上 進步到7.3%這個都是在影像辨識上一個這個基準的資料庫上面的結果後來Google內有錯誤率降到6.7%有22成但這些都不算是什麼residual內有152成啊他比101還要高啊但是這個residual內啊其實要訓練這麼深的Navigate是有訣竅的這個我們之後再講但是講到這邊如果你仔細思考一下我們一路的講法的話你有沒有發現一個奇妙的違和的地方不知道大家有沒有發現 什麼樣違和的地方呢我們一開始說我們想要用relu或者是sigmoid去逼近一個複雜的function實際上只要夠多的relu夠多的sigmoid就可以逼近任何的連續的function對不對我們只要夠多的sigmoid就可以製造夠複雜的線段就可以逼近任何的continuous function所以我們只要一排relu一排sigmoid夠多就足夠了那生的意義到底何在呢把relu sigmoid反復用到底有什麼好處呢為什麼不把它們直接排一排呢直接排一排也可以表示任何function所以把它反復用沒什麼道理啊 所以有人就說把deep learning把reducing memory反復用不過是個噱頭你之所以喜歡deep learning只是因為deep這個名字好聽reducing memory排成一排你只可以製造一個肥胖的networkfat neural network跟deep neural network聽起來量級就不太一樣deep聽起來就比較厲害fat neural network還以為是死肥宅network就不厲害這樣子那到底deep的理由為什麼我們不把network變胖只把network變深呢這個是我們日後要再講的話題好 那有人就說那怎麼不變得更深呢剛才只做到三成應該要做得更深嘛現在network都是別級擺深的 每幾百層都不好意思說你在做deep learning對不對所以要做更深所以確實做了更深做四層四層在訓練資料上他的loss是0.1K在沒有看過2021年的資料上是如何呢是0.44K慘掉了欸怎麼會這樣子呢在訓練資料上三成比四成差四成比三成好但是在沒看過的資料上四成比較差三成比較好在有看過資料上在訓練資料上跟沒看過的資料上他的結果是不一致的這種訓練資料跟測試這種訓練資料跟沒看過的資料 它的結果是不一致的狀況這個狀況叫做overfeeding那你常常聽到有人說機器學習會發生overfeeding的問題指的就是在訓練資料上有變好但是在沒看過的資料上沒有變好這件事情但是做到目前為止我們都還沒有真的發揮這個模型的力量你知道我們要發揮這個模型的力量2021年的資料到2月14號之前的資料我們也都已經手上有了所以我們要真正做的事情是什麼我們要做的事情就是預測未知的資料但是如果我們要預測未知的資料我們應該選三層的內容 還是四層的內臥呢舉例來說今天是2月26號今天的觀看人數我們還不知道如果我們要用一個Neural Network用我們已經訓練出來的Neural Network去預測今天的觀看人數你覺得應該要選三層的還是選四層的呢好這個我們來問一下大家的意見吧你覺得應該選三層的同學舉手一下好手放下好應該選四層的同學舉手一下好比較少好至於怎麼選模型這個是夏多會講問題但是大家都非常有sense知道我們要選三層的多數人都決定要選三層的你怎麼會說我總會選三層的 我們選四成呢四成在訓練資料上的結果比較好啊可是我們並不在意訓練資料的結果啊我們在意的是沒有看過的資料而2月26號是沒有看過的資料我們應該選一個在訓練的時候沒有看過的資料上表現會好的模型所以我們應該選三成的內網那你可能以為這門課就到這邊結束了其實不是我們真的來預測一下2月26號應該要我的觀看次數是多少好 但是因為其實YouTube的統計它沒有那麼及時啦所以它現在只統計到2月24號沒關係 我們先計算一下2月25號的觀看次數 人數是多少這個三成的內幕告訴我說2月25號這個頻道的總觀看人次應該是5250人那我們先假設2月25號是對的但實際上我們還不知道2月25號對不對因為YouTube後台統計的數據還沒有出來但我們先假設這一天就是對的然後再給我們的模型去預測2月26號的數字得到的結果是3.96K有3960次那你會發現為什麼這邊特別低因為模型知道說這個禮拜五觀看的人數就是比較少所以他預測特別低聽起來也是很合理但是你覺得這個預測個人這邊 0.38K比起來哪一個會比較準確呢你覺得你覺得我們下週來看看2月26號實際的值是多少但是你覺得這個值他跟真實值的誤差會小於0.38K的同學舉手一下絕對大於0.38K的同學舉手一下哇好可怕大家都對我這麼沒有信心這樣好我們就來看看這個下一週誤差會有多少但是我想應該是不會準啦因為你看這麼多人都覺得誤差會大你們回去每個人都去點那個影片的話哇誤差就大了今天講這麼久其實就是騙大家 去點影片而已啦好那今天其實就講了深度學習那今天講的不是一般的介紹方式如果想要聽一般的介紹方式過去的課程影片也是有的我就把連結附在這邊然後深度學習的訓練會用到一個東西叫Fabrication其實它就是比較有效率算規範的方法跟我們今天講的東西沒有什麼不同但如果你真的很想知道Fabrication是什麼的話影片連結也附在這邊好今天上課就上到這邊 謝謝大家謝謝 感谢观看 