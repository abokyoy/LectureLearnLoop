# 01. MCP 的核心概念与基础功能

- **MCP 的全称**：Model Context Protocol（简称 MCP），由Anthropic 开发
- **核心概念**：
  - **数据流转**：MCP 是一个分布式协议，允许不同硬件设备之间进行信息传递和处理
  - **基本用法**：
    - 提供跨设备交互的基础框架
    - 支持多用户环境下的协作与协同工作

## 进阶挑战（挑战 MCP 的进阶能力）

### 从零构建 MCP Server

- **基本功能**：通过配置 Python 库，实现对本地或异构网络的 MCP 环境搭建
- **挑战点**：
  - 创建完整的 MCP Server 需要熟悉文件结构和配置方式
  - 实现异构环境下的数据交互需要理解不同设备的通信协议

### 掌握进阶功能

- **从零构建 MCP Server 的具体实现**：
  - 使用 VS Code 编程插件“MCP Host”
  - 加载 OpenRouter 广室（OpenRouter）作为 MCP 齿轮
  - 运行模型配置，选择最优质的 MCP 模型进行训练

## 尝试解决实际应用问题

- **问题：如何配置天气 MCP？**
  - 新增 MCP 函数，将 MAPI 调用到 Google 天气 API 中
  - 确保 API 设置符合 MCP 标准
  - 实现异构环境的天气交互，确保数据一致性

## 小结与展望

通过本次整理，我们对 MCP 的核心概念有了清晰的理解，并认识到其在跨设备协作中的重要性。未来，随着技术的不断进步，MCP 可能会扩展到更多场景的应用中，例如智能硬件、物联网设备等。或者是Python启动的只不过在使用的过程中可能会连网当然它也可能不连网纯本地使用也是可以的不管是连不连网它都可以叫做MCP Server所以我觉得MCP Server的这个名字里面带Server这个词是有一定的误导性的你不要觉得这个玩意儿很高端 很玄妙它其实本质上就是一个程序跟你手机上面的应用没有什么太大的区别不管是MCP Server还是手机应用它们都内置了一些功能模块来解决你的问题我用iPhone上面的时钟进行举例它内置了四个功能模块分别是世界时钟 闹钟 钟表 计时器分别可以给我们解决四个场景的诉求而刚才Client想要给我们安装的这个Open Weather Map


略有不同的问题时它又会给出一个错的离谱的答案让你觉得它根本什么都不懂只是一个更高级大家好 这里是最佳拍档我是大飞经常接触大模型的朋友一定会有过这样的经历当你向AI提出一个复杂的问题时它不仅给出了答案还给出了详细的一步一步的看起来逻辑严密的解题过程那一瞬间你是不是感觉屏幕对面的不再是一堆冰冷的代码而是一个真正能够推理的智能体但是有的时候你换一个同样复杂但是略有不同的问题时它又会给出一个错的离谱的答案让你觉得它根本什么都不懂只是一个更高级的复读机这种体验上的巨大反差正是当前AI领域最核心的谜题之一大圆模型展现出的推理能力究竟是一种真正的智能涌现还是一种基于海量数据训练出来的更高级的模式匹配他是在進行邏輯推導還是在模仿他在網上看過的無數解題步驟關於這個問題學術界和工業界爭論不休但是爭論的意義遠不如我們去搞清楚這種所謂的推理能力到底是怎麼來的以及我們如何才能穩定的可靠的駕馭它踏入 Wix 工作室,從一個平台開始處理每個部份的建築。將你的項目圖畫出來,用 AI 能力的視覺地圖圖畫,能夠在秒數,而不是日子內製造視覺結構和線條。當你的客戶登機後,進入設計,自由地創作,甚至可以自行調整最小的元素。使用視覺方式,以積極地延伸品牌到每個頁面,並觀察它適應地穿越磨擦點。Now, Scale.而要解答這個問題有一個人的名字我們就繞不開他就是來自於谷歌DingMine的丹妮周他和他的團隊可以說是奠定了我們今天理解和使用大圓模型推理能力的基石他們開創性地提出了像思維鏈提示和自洽性這類的關鍵技術並且深度參與了谷歌靜奶模型推理能力的構建最近丹妮周在斯坦福大學做了一場演講系統性地梳理了從他創立谷歌大腦的推理團隊開始到今天我們所看到的強大的AI這條技術路線是如何一步一步演進的這場講座的信息量巨大它不僅揭示了AI推理能力的本質更是對過去幾年中所有相關技術的一次趣味所以今天這期視頻我們就將以丹妮周的這場講座為藍本帶著大家從最基礎的概念出發層層遞進徹底搞懂大圓模型思考秘密我們會看到那些看似神奇的技術並且會看到這些看似神奇的技術是如何一步一步的發展我們會看到這些看似神奇的技術是如何一步一步的發展我們會看到這些看似神奇的技術是如何一步一步的發展往往遵循著一些極其簡單而深刻的原理相信看完這些視頻你再看待大圓模型的時候會有一個全新的更加清晰的視角在深入探討之前我們必須先明確一件事情當我們討論大圓模型的推理時我們到底在談論什麼單機周一上來就拋出了一個非常清晰而且可操作的定義這個定義也成為了整個領域的共識他說關於模型到底會不會推理的哲學辯論他從不參加因為沒有一個明確的定義大家都是在自說自話而在他的團隊裡推理是有一個非常具體的含義的那就是在模型的輸入也就是你的問題和最終輸出也就是答案之間生成的所有中間步驟這個定義非常關鍵他把一個模糊的哲學層面的思考概念轉化成了一個具體的工程上可以實現和優化的目標為了讓我們更好的理解這個問題丹妮周设计了一个非常巧妙的任务叫做末尾字母拼接这个任务听起来很简单比如说我问模型请拼接artificial intelligence这两个单词的末尾字母如果我们直接让模型输出答案它可能会凭借着语言的惯性直接猜一个答案比如说LE这时候它只是在预测下一个最可能的字符而不是在执行一个多步骤的逻辑操作但是如果我们引导模型先生成中间步骤它的输出就会变成这样artificial的最后一个字母是Lintelligence的最后一个字母是E将L和E拼接起来得到LE这就是丹妮周所定义的推理她把一个复杂的任务分解成了一系列简单的可执行的子任务最终导出了正确的答案你可能会觉得这不就是我们人类解决问题的方式吗先思考再作答但是丹妮周提醒我们作为研究者你应该先想清楚你应该先想清楚你应该先想清楚你应该先想清楚你应该先想清楚你应该先想清楚你应该先想清楚你应该先想清楚你应该先想清楚你应该先想清楚你应该先想清楚必须时刻记住大猿模型不是人类它们只是概率模型把它们拟人化虽然有助于我们理解但是也很容易让我们走入歧途一个有趣的故事是丹琪周最初尝试的是首字母拼接但是他发现当时所有的模型都能够做得很好为什么呢因为互联网上有大量的缩写词模型在预训练阶段已经背会了如何拼接首字母于是他换成了末尾字母拼接结果当时所有的模型都失败了这恰恰说明模型并没有真正理解拼接这个动作而只是记住了某种常见的模式那么为什么要如此执着于生成这些中间步骤呢仅仅是为了模仿人类吗当然不是这背后有着非常坚实的理论依据丹琪周提到了他们和斯坦福大学教授滕上华团队合作的一项理论研究这个研究得出了一个非常强大的结论对于任何一个可以被大小都可以被拼接的模型来说是一个非常强大的证据这就是为什么丹琪周在2014年在中国的一个大学研究院他发现了一个模型它是一个非常强大的模型为T的布尔电路解决的问题一个常数大小的transformer模型可以通过生成OT长度的中间步骤来解决它这句话听起来有点过于技术我们来把它翻译一下布尔电路可以被看作是执行逻辑运算的基本单元任何复杂的计算任务比如说运行一个大型的软件本质上都可以被分解成一个巨大规模的布尔电路这里的大小T就代表了问题的计算复杂度这个理论告诉我们哪怕是一个相对简单的transformer模型只要你允许它生成也就是中间步骤它就有潜力解决几乎任何可计算的问题反过来说如果我们强迫模型直接蹦出最终答案就相当于要求这个模型的网络深度本身要能够模拟整个复杂的计算过程这要么需要一个巨大到不切实际的深度要么就根本无法解决问题所以让模型的计算方式是一种很简单的计算方式生成中間步驟不是一個可有可無的選項而是在計算原理上解鎖模型解決複雜問題能力的一把金鑰匙這徹底改變了我們訓練和使用大圓模型的範式從單純的追求答案轉向追求過程好 既然我們知道了推理過程如此的重要那麼下一個問題自然就是如何讓模型來生成這個過程呢這裡丹妮周提出了一個顛覆了當時很多人認知的觀點當時普遍認為一個普通的只經過預訓練的大圓模型是不會推理的你必須通過像思維鏈提示這樣的高級技巧或者進行專門的微調才能夠教會他們推理但是丹妮周說這個觀點是錯的而且大錯特錯她認為預訓練模型早就已經準備好進行推理了我們所需要做的僅僅是改變解碼的過程這又是一個非常深刻的洞察為了證明這一點她舉了一個經典的例子的数学硬题我有三个苹果我的爸爸比我多两个苹果我们总共有多少个苹果如果你把这个问题直接输入给一个原始的预训的模型比如说早期的GPT-3或者是拉马然后使用默认的贪婪解码方式会发生什么呢贪婪解码的意思是模型在生成每一个词的时候总是会选择当前概率最高的那一个在这种模式下模型很可能会直接输出一个看似合理但却是错误的答案比如说5个苹果因为他看到了3个和多两个就直接联想到了5这是模型的一种直觉反应接著丹妮周向我們展示了這些隱藏的候選答案比如說候選二可能以我字開頭模型會聲稱我有三個蘋果我爸爸比我多兩個所以他有五個蘋果三加五等於八所以我們總共有八個蘋果這是一個完美的推理鏈答案也正確候選三可能以我們開頭模型會聲稱我們總共有八個蘋果雖然沒有過程但是答案也對了候選四可能以你字開頭模型會聲稱你有三個蘋果你爸爸有三加二等於五個蘋果你們總共有三加五等於八個蘋果這同樣是一個清晰的推理鏈看到了嗎其實一直都存在於模型的輸出空間裏他們就像是隱藏在主幹道旁邊的小路默認的貪婪解碼因為只看到了眼前最寬的路所以錯過了他們這個發現被稱為思維鏈解碼它告訴我們推理能力不是被注入到模型裏面的而是模型在選擇的時間裏面海量文本中蘊含的邏輯關係之後自然湧現出來的於是我們的任務從交會模型推理變成了如何引導模型把他已經知道的東西以正確的形式表達出來那麼問題就變成了在這麼多候選的輸出裏我們怎麽知道哪一個是最好的呢一個簡單的想法是看長度在有思考過程的回答通常會更長但是丹妮周的團隊發現一個更可靠的指標那就是答案自信度他們觀察到一個驚人的現象對於那些包含了正確思維鏈的回答模型在生成最終答案的那個詞比如說數字8的時候其內部的自信度也就是概率會異常的高在蘋果的例子裏模型預測8這個詞的概率可能會高達98%這是一個非常強的型號因為對於一個擁有巨大詞彙表的模型來說通常每個詞的概率都接近於0這就像是一個人在經過深思熟慮之後他想要的答案对自己得出的结论会非常笃定一样所以说思维链解码的核心就两步一超越贪婪解码生成并且检查更多的候选输出二选择那个对最终答案自信度最高的候选这个方法虽然简单有效但还是需要写一些代码对于普通用户来说不够友好于是研究者们开始思考我们能不能用更自然的方式比如说自然语言来重塑模型的输出概率分布让那些带有思考过程的优秀答案能够自动排到第一名这样我们用最简单的贪婪解码就能够直接得到它这就引出了我们后来耳熟能详的一系列提示工程技术首先最著名的就是思维链提示它的做法非常直观在你提出你的问题之前先给模型看一两个类似的从问题到思考过程再到答案的例子比如说你想让模型解决前面苹果的问题你可以先给它一个例子问题是一个农村農民有5個香蕉他又買了6個後來吃了2個還剩幾個答案是農民開始有5個香蕉買了6個之後他有5加6等於11個然後他吃了2個所以他剩下11減2等於9個答案是9然後你再提出你的問題我有3個蘋果我爸爸比我多2個我們總共有多少個蘋果神奇的事情發生了模型會模仿你給出的例子的風格自動的開始一步一步分析生成詳細的解題步驟最後給出正確答案從概率分佈的角度來看你給出的例子極大的提升了模型生成類似思考過程的巨式的概率反而把原本隱藏在後面的正確推理路徑推到了最前面但是這種方法有一個問題你需要為不同類型的任務手動編寫高質量的實例這很麻煩而且如果你自己都知道怎麼解決一個類似的問題那你為什麼還要問AI呢於是一個更加神奇的提示就出現了它就是讓我們以後一步一步思考Let's think step by step但尼州坦言這篇論文剛出來的時候他以為這是個玩笑怎麼可能在問題後面加上這麼一句簡單的話模型就會自動開始思考了呢他當時就在谷歌內部的PALM模型上做了測試他非常清楚PALM的訓練數據裏絕對沒有針對這個咒語做過任何的優化結果他震驚地發現它真的有效模型真的開始輸出一步一步的解題過程了這個發現極大的啟發了他儘管Let's think step by step這種零樣本提示效果通常比不過提供具體事例的少樣本思維鏈提示但是它證明了我們可以用非常通用的方式來激發模型的推理潛能然而無論是哪種提示方法都感覺有點奇怪想象一下你問一個聰明人問題還必須得在後面加上一句請一步一步思考否則他就不會思考了這顯然不符合我們對於一個真正智能體系的看法所以我们需要一种更稳定更内化的方式让推理能力成为模型固有的一部分而不是需要外部咒语来触发这就把我们带到了下一个阶段微调 我们先说尖度微调SFT它的思路非常的直接我们不就是希望模型能够生成从问题到思考过程再到答案这样的数据吗那我们就雇佣一批人针对大量的问题写出高质量的一步一步的解题方案然后我们再把这些标准答案喂给模型 让模型去学习这个方法在机器学习里边叫做最大自然估计简单来说就是让模型生成的序列跟人类专家写的序列尽可能的一模一样这个想法其实很早就有了单极周提到早在2017年丁麦的一篇论文就在做类似的事情他们收集了一批数学应用题和人类手写的解题步骤来训练一个序列模型后来在2021年OPI更进一步构建了一个更稳定的模型更著名的數據集也就是GSM8K包含了8000多個小學水平的數學題和詳細解法用來微調GPT-3模型這種方法訓練出來的模型在你給它一個新問題的時候確實能夠生成不錯的解析步驟看起來問題似乎解決了一旦模型訓練好就可以隨時部署不再需要複雜的提示了然而在2021年夏天丹尼州的團隊發現了一個嚴重的問題那就是SFT訓練出來的模型算話能力很差它在那些和訓練數據很相似的問題上表現很好但是一旦遇到一個新的類型稍微不同的問題就很容易失敗他們嘗試了大力出奇技的方法擴大了數據的規模找更多的人標註更多的數據可惜結果卻是無論如何擴大規模這個問題始終存在丹尼州在這裡給出了一個重要的教訓不要盲目的擴大規模當你的範式本身是錯誤的時候再多的數據也不代表你那麼SFT的範式錯在哪裡了呢問題又出在流程裏的哪一步呢丹妮周給出的答案可能會讓你大吃一驚她說錯誤出在人身上這個轉折點來自於自我提升後來也被稱為self-improve或者是start方法當她第一次聽到機器生成的訓練數據可能比人類專家寫得還好這個想法的時候她自己也感到非常驚訝這個新範式的流程是這樣的首先我們仍然從一批問題開始但是我們不再找人類去寫解題步驟我們讓一個已經比較強大的大圓模型自己去針對這些問題生成大量的多樣的解題步驟最關鍵的一步是我們用一個驗證器去檢查模型生成的這些解題步驟看哪個最終得出了正確的答案比如說對於數學題我們知道標準答案就可以直接的判斷於是我們只保留下來那些過程多樣但是結果正確的生成結果把解題步驟寫出來把它们当作新的高质量的训练数据然后用这些由模型自己生成的并且经过验证的好数据再去微调模型自己这个过程可以不断地迭代一个微调后变得更强的模型又可以去生成质量更高更复杂的解题步骤用来进一步的训练自己这就形成了一个自我进化的闭环赖铁周提到一篇在2024年1月发表的来自字节跳动的论文Reasoning with reinforced for ITUny是他在学术界看到的最早公开阐述类似思想的出版物之一他相信在OpenAI等多个机构内部大家可能都独立地发现了这个简单而又极其有效的思想现在我们必须回答那个核心的问题为什么模型自己生成的数据会比人类专家手写的数据在训练效果上更好呢这背后其实蕴含着技艺学习的一个第一性原理那就是直接优化你想要的东西在SFT的范式里我们优化的目标是让模型更有效我是大飞经常接触大模型的朋友一定会有过这样的经历当你向AI提出一个复杂的问题时它不仅给出了答案还给出了详细的一步一步的看起来逻辑严密的解题过程那一瞬间你是不是感觉屏幕对面的不再是一堆冰冷的代码而是一个真正能够推理的智能体但是有的时候你换一个同样复杂但是略有不同的问题时它又会给出一个错的离谱的答案让你觉得它根本什么都不懂只是一个更高级的复读机这种体验上的巨大反差正是当前AI领域最核心的谜题之一大元模型展现出的推理能力究竟是一种真正的智能涌现还是一种基于海量数据训练出来的更高级的模式匹配它是在进行逻辑推导还是在模仿它在网上看过的无数解题步骤关于这个问题学术界和工业界争论不休但是我们还是要看但是争论的意义远不如我们去搞清楚这种所谓的推理能力到底是怎么来的以及我们如何才能稳定的可靠的驾驭它而要解答这个问题有一个人的名字我们就绕不开他就是来自于谷歌丁曼的丹尼周他和他的团队可以说是奠定了我们今天理解和使用大元模型推理能力的基石他们开创性的提出了像思维链提示和自下行这类的关键技术并且深度参与了谷歌静态模型推理能力的构建最近丹尼周在斯坦福大学做了一场演讲他系统性的梳理了从他创立谷歌大脑的推理团队开始到今天我们所看到的强大的AI这条技术路线是如何一步一步演进的这场讲座的信息量巨大它不仅揭示了AI推理能力的本质更是对过去几年中所有相关技术的一次趣味所以今天这期视频我们就将以丹尼周的这场讲座为了帶著大家從最基礎的概念出發層層遞進徹底搞懂大約模型思考秘密我們會看到那些看似神奇的技術背後往往遵循著一些極其簡單而深刻的原理相信看完這期視頻你再看待大約模型的時候會有一個全新的更加清晰的視角在深入探討之前我們必須先明確一件事情當我們討論大約模型的推理時我們到底在談論什麼單機中一上來就拋出了一個非常清晰而且可操作的定義這個定義也成為了整個領域的共識他說關於模型到底會不會推理的哲學辯論因為沒有一個明確的定義大家都是在自說自話而在他的團隊裡推理是有一個非常具體的含義的那就是在模型的輸入也就是你的問題和最終輸出也就是答案之間生成的所有中間步驟這個定義非常關鍵他把一個很簡單的模糊的哲学层面的思考概念转化成了一个具体的工程上可以实现和优化的目标为了让我们更好的理解这个目标丹尼周设计了一个非常巧妙的任务叫做末尾字母拼接这个任务听起来很简单比如说我问模型请拼接artificial intelligence这两个单词的末尾字母如果我们直接让模型输出答案它可能会凭借着语言的惯性直接猜一个答案比如说LE这时候它只是在预测下一个最可能的字符而不是在执行一个多步骤的逻辑操作但是如果我们引导模型先生成中间步骤它的输出就会变成这样artificial的最后一个字母是Lintelligence的最后一个字母是E将L和E拼接起来得到LE这就是丹尼周所定义的推理他把一个复杂的任务分解成了一系列简单的可执行的子任务最终导出了一个正確答案现在我们可以更好地加入功能符合你的工作模式我会加入掃描收入 数据化和支付费用现在我可以在照片中立刻刷出一张图片没有任何的手册费用最后我们加入费用预测档预测月和年费现在你有一个应用软件专注于你的生意需要建立在你工作的方式里开始你的工作你可能会觉得这不就是我们人类解决问题的方式吗先思考再作答但是赶紧周提醒我们作为研究者必须时刻记住大猿模型无视人类他們只是概率模型把他們擬人化雖然有助於我們理解但是也很容易讓我們走入歧途一個有趣的故事是丹琪周最初嘗試的是手字母拼接但是他發現當時所有的模型都能夠做得很好為什麼呢因為互聯網上有大量的縮寫詞模型在預訓練階段已經背會了如何拼接手字母於是他換成了末尾字母拼接結果當時所有的模型都失敗了這恰恰說明模型並沒有真正理解拼接這個動作而只是記住了某種常見的模式那麼為什麼要如此執著於生成這些中間步驟呢僅僅是為了模仿人類嗎當然不是這背後有著非常堅實的理論依據丹琪周提到了他們和斯坦福大學教授滕上華團隊合作的一項理論研究這個研究得出了一個非常強大的結論對於任何一個可以被大小為T的物理電路解決的問題一個常識的問題可以通过生成OT长度的中间步骤来解决它这句话听起来有点过于技术我们来把它翻译一下布尔电路可以被看作是执行逻辑运算的基本单元任何复杂的计算任务比如说运行一个大型的软件本质上都可以被分解成一个巨大规模的布尔电路这里的大小T就代表了问题的计算复杂度这个理论告诉我们哪怕是一个相对简单的Transformer模型只要你允许它生成足够长的思考过程也就是中间步骤它就有潜力解决几乎任何可计算的问题反过来说如果我们强迫模型直接蹦出最终答案就相当于要求这个模型的网络深度本身要能够模拟整个复杂的计算过程这要么需要一个巨大到不切实际的深度要么就根本无法解决问题所以让模型思考生成中间步骤不是一个可有可无的计算过程而是在计算原理上解锁模型解决复杂问题能力的一把金钥匙这彻底改变了我们训练和使用大学模型的方式从单纯的追求答案转向追求过程好既然我们知道了推理过程如此的重要那么下一个问题自然就是如何让模型来生成这个过程呢这里呢丹妮周提出了一个颠覆了当时很多人认知的观点当时普遍认为一个普通的只经过预训练的大学模型是不会推理的你必须通过像思维链提示这样的高级技巧或者进行专门的微调才能够交配但是丹妮周说这个观点是错的而且大错特错她认为预训的模型早就已经准备好进行推理了我们所需要做的仅仅是改变解码的过程这又是一个非常深刻的洞察为了证明这一点她举了一个经典的数学硬题我們總共有多少個蘋果如果你把這個問題直接輸入給一個原始的預訓練模型比如說早期的GDP3或者是拉馬然後使用默認的貪婪解碼方式會發生什麼呢貪婪解碼的意思是模型在生成每一個詞的時候總是會選擇當前概率最高的那一個在這種模式下模型很可能會直接輸出一個看似合理但卻是錯誤的答案比如說5個蘋果因為它看到了3個和多2個就直接聯想到了5這是模型的一種直覺反應或者說是一種系統思維但是模型的強大之處在於它的輸出概率分佈中並不僅僅只有這一個選項在生成第一個詞的時候5個可能是概率最高的但是還有第二第三第四高的選項如果我們不那麼貪婪而是去探索一下那些概率稍低一些的岔路奇蹟就會發生了接著丹妮周向我們展示了這些隱藏的候選答案比如說就說候選二可能以我字開頭模型會聲稱我有三個蘋果我爸爸比我多兩個所以他有五個蘋果三加五等於八所以我們總共有八個蘋果這是一個完美的推理鏈答案也正確候選三可能以我們開頭模型會聲稱我們總共有八個蘋果雖然沒有過程但是答案也對了候選四可能以你字開頭模型會聲稱你有三個蘋果你爸爸有三加二等於五個蘋果你們總共有三加五等於八個蘋果這同樣是一個清晰的推理鏈看到了嗎正確的推理路徑其實一直都存在於模型的輸出空間裏它們就像是隱藏在主幹道旁邊的小路默認的貪婪解碼因為只看到了眼前最寬的路所以錯過了它們這個發現被稱為思維鏈解碼它告訴我們推理能力不是被注入到模型裏面的而是模型在學習海量文本中蘊含的邏輯關系之後自然有了這個能力於是我們的任務從教會模型推理變成了如何引導模型把他已經知道的東西以正確的形式表達出來那麼問題就變成了在這麼多候選的輸出裏我們怎麽知道哪一個是最好的呢一個簡單的想法是看長度在有思考過程的回答通常會更長但是丹尼州的團隊發現一個更可靠的指標那就是答案自信度他們觀察到一個驚人的現象對於那些包含了正確思維鏈的回答模型在生成最終答案的那個詞比如說數字8的時候其內部的自信度也就是概率會異常的高在這個蘋果的例子裏模型預測8這個詞的概率可能會高達98%這是一個非常強的信號因為對於一個擁有巨大詞彙表的模型來說通常每個詞的概率都接近於0這就像是一個人在經過深思熟慮之後對自己得出的結論會非常篤定一樣所以說思维链解码的核心就两步一超越贪婪解码生成并且检查更多的候选输出二选择那个对最终答案自信度最高的候选这个方法虽然简单有效但还是需要写一些代码对于普通用户来说不够友好于是研究者们开始思考我们能不能用更自然的方式比如说自然语言来重塑模型的输出概率分布让那些带有思考过程的优秀答案能够自动排到第一名这样我们用最简单的贪婪解码就能够直接得到它这就引出了我们后来耳熟能详的一系列提示工程技术首先最著名的就是思维链提示它的做法非常直观在你提出你的问题之前先给模型看一两个类似的从问题到思考过程再到答案的例子比如说你想让模型解决前面那个苹果的问题你可以先给它一个例子问题是一个农民有五个香蕉他又买了六个后来吃了两个还吃了三个这就叫做贪婪解码这就是我们的思考过程我们的思考过程我们的思考过程我们的思考过程我们的思考过程答案是農民開始有五個香蕉買了六個之後他有5加6等於11個然後他吃了兩個所以他剩下11減2等於9個答案是9然後你再提出你的問題我有三個蘋果我爸爸比我多兩個我們總共有多少個蘋果神奇的事情發生了模型會模仿你給出的例子的風格自動的開始一步一步分析生成詳細的解題步驟最後給出正確答案從概率分佈的角度來看你給出的例子極大的提升了模型生成類似思考過程的巨式的概率從而把原本隱藏在後面的正確推理路徑推到了最前面但是這種方法有一個問題你需要為不同類型的任務手動編寫高質量的實例這很麻煩而且如果你自己都知道怎麼解決一個類似的問題那你為什麼還要問AI呢於是一個更加神奇的提示就出現了它就是讓我們一步一步思考Let's think step by step單機周談一下这篇论文刚出来的时候他以为这是个玩笑怎么可能在问题后面加上这么一句简单的话模型就会自动开始思考了呢他当时就在谷歌内部的PALM模型上做了测试他非常清楚PALM的训练数据里绝对没有针对这个咒语做过任何的优化结果他震惊地发现它真的有效模型真的开始输出一步一步的解题过程了这个发现极大的启发了他尽管Let's think step by step这种零样本提示效果通常比不过提供具体示例的但是他证明了我们可以用非常通用的方式来激发模型的推理潜能然而无论是哪种提示方法都感觉有点奇怪想象一下你问一个聪明人问题还必须得在后面加上一句请一步一步思考否则他就不会思考了这显然不符合我们对于一个真正智能体的期望所以我们需要一种更稳定更内化的方式讓推理能力成為模型固有的一部分而不是需要外部咒語來觸發這就把我們帶到了下一個階段微調我們先說監督微調SFT它的思路非常的直接我們不就是希望模型能夠生成從問題到思考過程再到答案這樣的數據嗎那我們就僱用一批人針對大量的問題寫出高質量的一步一步的解題方案然後我們再把這些標準答案背給模型讓模型去學習這個方法在機器學習裏面叫做最大三人估計簡單來說就是讓模型生成的序列跟人類專家寫的序列盡可能的一模一樣這個想法其實很早就有了單機周提到早在2017年DMITE的一篇論文就在做類似的事情他們收集了一批數學應用題和人類手寫的解題步驟來訓練一個序列模型後來在2021年OPI更進一步構建了一個更大更著名的數據集也就是GSM8K包含了百分之一百的模型大圆模型展现出的推理能力究竟是一种真正的智能涌现还是一种基于海量数据训练出来的更高级的模式匹配呢它是在进行逻辑推导还是在模仿它在网上看过的无数解题步骤呢关于这个问题学术界和工业界争论不休但是争论的意义远不如我们去搞清楚这种所谓的推理能力到底是怎么来的以及我们如何才能稳定的可靠的驾驭它而要解答这个问题有一个人的名字我们就绕不开他就是来自于谷歌丁曼的丹尼周他和他的团队可以说是奠定了我们今天理解和使用大圆模型推理能力的基石他们开创性的提出了像思维链提示和自下行这类的关键技术并且深度参与了谷歌金本奶模型推理能力的构建最近丹尼周在斯坦福大学做了一场演讲系统性的梳理了从他创立谷歌丁曼的基础大腦的推理團隊開始到今天我們所看到的強大的AI這條技術路線是如何一步一步演進的這場講座的信息量巨大它不僅揭示了AI推理能力的本質更是對過去幾年中所有相關技術的一次去媚所以今天這期視頻我們就將以丹尼周的這場講座為藍本帶著大家從最基礎的概念出發層層遞進徹底搞懂大約模型思考秘密我們會看到那些看似神奇的技術背後往往遵循著一些極其簡單而深刻的原理相信看完這期視頻你再看待大約模型的時候會有一個全新的更加清晰的視角在深入探討之前我們必須先明確一件事情當我們討論大約模型的推理時我們到底在談論什麼丹尼周一上來就拋出了一個非常清晰而且可操作的定義這個定義也成為了整個領域的共識他說關於模型到底会不会推理的哲学辩论他从不参加因为没有一个明确的定义大家都是在自说自话而在他的团队里推理是有一个非常具体的含义的那就是在模型的输入也就是你的问题和最终输出也就是答案之间生成的所有中间步骤这个定义非常关键他把一个模糊的哲学层面的思考概念转化成了一个具体的工程上可以实现和优化的目标为了让我们更好的理解这个目标丹尼周设计了一个非常巧妙的任务叫做末尾字母拼接这个任务听起来很简单比如说我问模型请拼接artificial intelligence这两个单词的末尾字母如果我们直接让模型输出答案它可能会凭借着语言的惯性直接猜一个答案比如说LE这时候它只是在预测下一个最可能的字符而不是在执行一个多步骤的逻辑操作但是如果我们拼接这个最可能的字符它可能会直接猜一个答案比如说LE这时候它只是在预测下一个最可能的字符而不是在执行一个多步骤的逻辑操作但是如果我们拼接artificial intelligence如果我们引导模型先生成中间步骤它的输出就会变成这样artificial的最后一个字母是Lintelligence的最后一个字母是E将L和E拼接起来得到LE这就是丹妮周所定义的推理她把一个复杂的任务分解成了一系列简单的可执行的子任务最终导出了正确的答案你可能会觉得这不就是我们人类解决问题的方式吗先思考再作答但是丹妮周提醒我们作为研究者必须时刻记住大猿模型不是人类它们只是概率模型把它们拟人化虽然有助于我们理解但是也很容易让我们走入歧途一个有趣的故事是丹妮周最初尝试的是首字母拼接但是她发现当时所有的模型都能够做得很好为什么呢因为互联网上有大量的缩写词模型在预训那阶段已经背会了如何拼接首字母于是她换成了末尾字母拼接结果当时所有的模型都失败了这恰恰说明模型并没有真正理解拼接这个动作而只是记住了某种常见的模式那么为什么要如此执着于生成这些中间步骤呢仅仅是为了模仿人类吗当然不是这背后有着非常坚实的理论依据丹琪州提到了他们和斯坦福大学教授腾讯华团队合作的一项理论研究这个研究得出了一个非常强大的结论对于任何一个可以被大小为T的布尔电路解决的问题一个常数大小的Transformer模型可以通过生成OT长度的中间步骤来解决它这句话听起来有点过于技术我们来把它翻译一下布尔电路可以被看作是执行逻辑运算的基本单元任何复杂的计算任务比如说运行一个大型的软件本质上都可以被分解成一个巨大规模的布尔电路这里的大小T就代表了问题的计算复杂度这个理论告诉我们哪怕是一个相对简单的transformer模型只要你允许它生成足够长的思考过程也就是中间步骤它就有潜力解决几乎任何可计算的问题反过来说如果我们强迫模型直接蹦出最终答案就相当于要求这个模型的网络深度本身要能够模拟整个复杂的计算过程这要么需要一个巨大到不切实际的深度要么就根本无法解决问题所以让模型思考生成中间步骤不是一个可有可无的选项而是在计算原理上解锁模型解决复杂问题能力的一把金钥匙这彻底改变了我们训练和使用大学模型的范式从单纯的追求答案转向追求过程好 既然我们知道了推理过程如此的重要那么下一个问题自然就是如何让模型来生成这个过程呢这里丹妮周提出了一个颠覆了当时很多人认知的最简单的模型思考方法就是用模型思考的方法来解决问题这种方法是非常简单的就是用模型思考的方法来解决问题然后用模型思考的方法来解决问题当时普遍认为一个普通的只经过预训练的大圆模型是不会推理的你必须通过像思维链提示这样的高级技巧或者进行专门的微调才能够教会他们推理但是丹尼周说这个观点是错的而且大错特错他认为预训模型早就已经准备好进行推理了我们所需要做的仅仅是改变解码过程全面的设计和设置都包括了全面的设计 没有微管使用图形设计来连接多个桌子并且设置不同的团队同样的位置没有项目费用没有需要的加装没有无聊的日常站台搅拌一下我们的Shuffle模式并且给每个团队员提供AI的建议没有什么像不可预料的公开说话的好玩MondayDev的GitHub的融合使用了强大的自动化以给你全面的设计进行日常进步并且让你的发展者能够在GitHub和关注CodeGeek不需要手册更新Monday Dev计划 追踪 和 运行程序 迅速订阅免费试试不需要信用卡Atera是IT的痛苦凶手遥控和管理遥控和接收助援设备和订单聪明的自动化编程 软件解决编程最神奇的是它能做到这些却又轻易使用真是神奇只是一个清晰 优雅的UI所有东西都在它应该的位置它就做了它应该做的事情对不起我感到激动不过以伪想性警告你解决了真正的IT问题才會成為IT的問題對不起我聽不懂你在修理電腦的聲音有多酷以後以後在科學上的報告管理層終於明白我為什麼是真正的MVP另外以設備價格來看這就省了50%的IT費用讓我成為新公司的英雄你的腦袋在發瘋真正的英雄是戴著耳機你會不會也許不會你不需要買任何東西你可以現在立即試試免費的 設置成風景只要點擊連結 試試看很棒Atera這是我需要的接受餅乾餅乾接受了这又是一个非常深刻的洞察为了证明这一点他举了一个经典的数学硬题我有三个苹果我的爸爸比我多两个苹果我们总共有多少个苹果如果你把这个问题直接输入给一个原始的预训量模型比如说早期的GPT-3或者是拉玛然后使用默认的贪婪解码方式会发生什么呢贪婪解码的意思是模型在生成每一个词的时候总是会选择当前概率最高的那一个在这种模式下模型很可能会直接输出一个看似合理但却是错误的答案比如说五个苹果那些概率稍低一些的岔路奇迹就会发生了接着单机周向我们展示了这些隐藏的候选答案比如说候选二可能以我字开头模型会生成我有三个苹果我爸爸比我多两个所以他有五个苹果3加5等于8所以我们总共有八个苹果这是一个完美的推理链答案也正确候选三可能以我们开头模型会生成我们总共有八个苹果虽然没有过程但是答案也对了候选四可能以你字开头模型会生成你有三个苹果你爸爸有3加2等于五个苹果你们总共有3加5等于八个苹果这同样是一个清晰的推理链看到了吗正确的推理路径其实一直都存在于模型的输出空间里它们就像是隐藏在主干道旁边的小路默认的贪婪解码因为只看到了眼前最宽的路所以错过了它们这个发现被称为思维链解码它告诉我们推理能力不是被注入到模型里面的而是模型在学习海量文本中蕴含的逻辑关系之后自然涌现出来的于是我们的任务从交汇模型推理变成了如何引导模型把它已经知道的东西以正确的形式表达出来那么问题就变成了在这么多候选的输出里我们怎么知道哪一个是最好的呢一个简单的想法是看长度带有思考过程的回答通常会更长但是丹基州的团队发现一个更可靠的指标那就是答案自信度他们观察到一个惊人的现象对于那些包含了正确思维链的回答模型在生成最终答案的那个词比如说数字8的时候其内部的自信度也就是概率会异常的高在这个苹果的例子里模型预测8这个词的概率可能会高达98%这是一个非常强的信号因为对于一个拥有巨大词汇表的模型来说通常每个词的概率都会比较高都接近于0这就像是一个人在经过深思熟虑之后对自己得出的结论会非常笃定一样所以说思维链解码的核心就两步一超越贪婪解码生成并且检查更多的候选输出二选择那个对最终答案自信度最高的候选这个方法虽然简单有效但还是需要写一些代码对于普通用户来说不够友好于是研究者们开始思考我们能不能用更自然的方式比如说自然语言来重塑模型的输出概率分布让那些带有思考过程的优秀答案能够自动排到第一名这样我们用最简单的贪婪解码就能够直接得到它这就引出了我们后来耳熟能详的一系列提示工程技术首先最著名的就是思维链提示它的做法非常直观在你提出你的问题之前先给模型看一两个类似的从问题到思考过程再到答案的例子比如说你想让模型解决前面那个苹果的问题你可以先给他一个例子问题是一个农民有5个香蕉他又买了6个后来吃了两个还剩几个答案是农民开始有5个香蕉买了6个之后他有5加6等于11个然后他吃了两个所以他剩下11-2等于9个答案是9然后你再提出你的问题我有三个苹果我爸爸比我多两个我们总共有多少个苹果神奇的事情发生了模型会模仿你给出的例子的风格自动的开始一步一步分析生成详细的解题步骤最后给出正确答案从概率分布的角度来看你给出的例子极大的提升了模型生成类似思考过程的巨式的概率反而把原本隐藏在后面的正确推理路径推到了最前面但是这种方法有一个问题你需要为不同类型的任务手动编写高质量的实例这很麻烦而且如果你自己都知道怎么解决一个类似的问题那你为什么还要问AI呢因为你自己都知道於是一個更加神奇的提示就出現了它就是讓我們一步一步思考Let's think step by step但尼周坦言這篇論文剛出來的時候他以為這是個玩笑怎麼可能在問題後面加上這麼一句簡單的話模型就會自動開始思考了呢他當時就在谷歌內部的PALM模型上做了測試他非常清楚PALM的訓練數據裏絕對沒有針對這個咒語做過任何的優化結果他震驚地發現它真的有效模型真的開始輸出一步一步的解題過程了這個發現極大的啟發了他儘管Let's think step by step這種零樣本提示效果通常比不過提供具體事例的少樣本思維鏈提示但是他證明了我們可以用非常通用的方式來激發模型的推理潛能然而無論是哪種提示方法都感覺有點奇怪想象一下你問一個聰明人問題還必須得在後面加上一句請一步一步思考否則他就不會思考了這顯然不符合我們對於一個真正智能體的期望所以我們需要一種更穩定更內化的方式讓推理能力成為模型固有的一部分而不是需要外部咒語來觸發這就把我們帶到了下一個階段微調我們先說監督微調SFT它的思路非常的直接我們不就是希望模型能夠生成從問題到思考過程再到答案這樣的數據嗎那我們就僱用一批人針對大量的問題寫出高質量的一步一步的解題方案然後我們再把這些標準答案餵給模型讓模型去學習這個方法在機器學習裏面叫做最大自然估計簡單來說就是讓模型生成的序列跟人類專家寫的序列盡可能的一模一樣這個想法其實很早就有了賴清周提到早在2017年DMI的一篇論文就在做類似的事情他們收集了一批數學硬體和人類手寫的解題步驟來訓練一個人類的智能體他們開創性地提出了像思維鏈提示和自洽性這類的關鍵技術並且深度參與了谷歌靜態模型推理能力的構建最近丹尼州在斯坦福大學做了一場演講系統性地梳理了從它創立谷歌大腦的推理團隊開始到今天我們所看到的強大的AI這條技術路線是如何一步一步演進的這場講座的信息量巨大它不僅揭示了AI推理能力的本質更是對過去幾年中所有相關技術的一次趣味所以今天這期視頻我們就將以丹尼州的這場講座為藍本帶著大家從最基礎的概念出發層層遞進徹底搞懂大約模型思考秘密我們會看到那些看似神奇的技術背後往往遵循著一些極其簡單而深刻的原理相信看完這期視頻你再看待大約模型的時候會有一個全新的更加清晰的視角在深入探討之前我們必須先來個探討當我們討論大圓模型的推理時我們到底在談論什麼丹妮周一上來就拋出了一個非常清晰而且可操作的定義這個定義也成為了整個領域的共識她說關於模型到底會不會推理的哲學辯論她從不參加因為沒有一個明確的定義大家都是在自說自話而在她的團隊裡推理是有一個非常具體的含義的那就是在模型的輸入也就是你的問題和最終輸出也就是答案之間生成的所有中間步驟這個定義非常關鍵她把一個模糊的哲學層面的思考概念轉化成了一個具體的工程上可以實現和優化的目標為了讓我們更好地理解這個目標丹妮周設計了一個非常巧妙的任務叫做末尾字母拼接這個任務聽起來很簡單比如說我問模型請拼接artificial intelligence這兩個單詞的末尾字母請拼接如果我们直接让模型输出答案它可能会凭借着语言的惯性直接猜一个答案比如说LE这时候它只是在预测下一个最可能的字符而不是在执行一个多步骤的逻辑操作但是如果我们引导模型先生成中间步骤它的输出就会变成这样artificial的最后一个字母是Lintelligence的最后一个字母是E将L和E拼接起来得到LE这就是丹妮周所定义的推理她把一个复杂的任务分解成了一系列简单的可执行的子任务最终导出了正确的答案你可能会觉得这不就是我们人类解决问题的方式吗先思考再作答但是丹妮周提醒我们作为研究者必须时刻记住大猿模型不是人类它们只是概率模型把它们擬人化虽然有助于我们理解但是也很容易让我们走入歧途一个有趣的故事是丹妮周最初尝试的是手动模型但是他发现当时所有的模型都能够做得很好为什么呢因为互联网上有大量的缩写词模型在预训练阶段已经背会了如何拼接首字母于是他换成了末尾字母拼接结果当时所有的模型都失败了这恰恰说明模型并没有真正理解拼接这个动作而只是记住了某种常见的模式那么为什么要如此执着于生成这些中间步骤呢仅仅是为了模仿人类吗当然不是这背后有着非常坚实的理论依据单极州提到了他们和斯坦福大学教授腾上华团队合作的一项理论研究这个研究得出了一个非常强大的结论对于任何一个可以被大小为T的布尔电路解决的问题一个常数大小的Transformer模型可以通过生成OT长度的中间步骤来解决它这句话听起来有点过于技术我们来把它翻译一下布尔电路可以被固定为T看作是执行逻辑运算的基本单元任何复杂的计算任务比如说运行一个大型的软件本质上都可以被分解成一个巨大规模的布尔电路这里的大小t就代表了问题的计算复杂度这个理论告诉我们哪怕是一个相对简单的transformer模型只要你允许它生成足够长的思考过程也就是中间步骤它就有潜力解决几乎任何可计算的问题反过来说如果我们强迫模型直接蹦出最终答案就相当于要求这个模型的网络深度本身要能够模拟整个复杂的计算过程这要么需要一个巨大到不切实际的深度要么就根本无法解决问题所以让模型思考生成中间步骤不是一个可有可无的选项而是在计算原理上解锁模型解决复杂问题能力的一把金钥匙这彻底改变了我们训练和使用大约模型的范式單純的追求答案轉向追求過程好 既然我們知道了推理過程如此的重要那麼下一個問題自然就是如何讓模型來生成這個過程呢這裡丹妮周提出了一個顛覆了當時很多人認知的觀點當時普遍認為一個普通的只經過預訓練的大圓模型是不會推理的你必須通過像思維鏈提示這樣的高級技巧或者進行專門的微調才能夠教會他們推理但是丹妮周說這個觀點是錯的而且大錯特錯他認為預訓練模型早就已經準備好進行推理了我們所需要做的僅僅是改變解碼的過程這又是一個非常深刻的洞察為了證明這一點他舉了一個經典的數學硬題我有三個蘋果我的爸爸比我多兩個蘋果我們總共有多少個蘋果如果你把這個問題直接輸入給一個原始的預訓練模型比如說早期的GDP-3或者是拉馬然後使用默認的推理模式貪婪解碼方式會發生什麼呢貪婪解碼的意思是模型在生成每一個詞的時候總是會選擇當前概率最高的那一個在這種模式下模型很可能會直接輸出一個看似合理但卻是錯誤的答案比如說五個蘋果因為它看到了三個和多兩個就直接聯想到了五這是模型的一種直覺反應或者說是一種系統思維但是模型的強大之處在於它的輸出概率分布中並不僅僅只有這一個選項在生成第一個詞的時候五個可能是概率最高的但是還有第二第三第四高的選項如果我們不那麼貪婪而是去探索一下那些概率稍低一些的岔路奇蹟就會發生了接著丹尼周向我們展示了這些隱藏的候選答案比如說候選二可能以我字開頭模型會生成我有三個蘋果我爸爸比我多兩個所以他有五個蘋果三加五等於八所以我們總共有八個蘋果這是一個完美的推理鏈答案也正確候選三可能以我們開頭模型會聲稱我們總共有八個蘋果雖然沒有過程但是答案也對了候選四可能以你自開頭模型會聲稱你有三個蘋果你爸爸有3加2等於五個蘋果你們總共有3加5等於八個蘋果這同樣是一個清晰的推理鏈看到了嗎正確的推理路徑其實一直都存在於模型的輸出空間裏它們就像是隱藏在主幹道旁邊的小路默認的貪婪解碼因為只看到了眼前最寬的路所以錯過了它們這個發現被稱為思維鏈解碼它告訴我們推理能力不是被注入到模型裏面的而是模型在學習海量文本中蘊含的邏輯關系之後自然湧現出來的於是我們的任務從教會模型推理變成了如何引導模型把它已經知道的東西以正確的形式表達出來那麽問題就變成了在正確的推理模型裏面模型裏面的東西是否存在著一個正確的推理模型的存在這就是我們要學習的模型的正確性模型的正確性模型的正確性模型的正確性模型的正確性后选的输出里我们怎么知道哪一个是最好的呢一个简单的想法是看长度带有思考过程的回答通常会更长但是丹妮周的团队发现一个更可靠的指标那就是答案自信度他们观察到一个惊人的现象对于那些包含了正确思维链的回答模型在生成最终答案的那个词比如说数字8的时候其内部的自信度也就是概率会异常的高在这个苹果的例子里模型预测8这个词的概率可能会高达98%这是一个非常强的信号因为对于一个拥有巨大词汇表的模型来说通常每个词的概率都接近于0这就像是一个人在经过深思熟虑之后对自己得出的结论会非常笃定一样所以说思维链解码的核心就两路一 超越贪婪解码生成并且检查更多的候选输出二 选择那个对最终答案自信度最高的候选这个方法呢是可以用来解决很多问题的雖然簡單有效踏入 Wix Studio 公司和企業的平台在你設計的工作環境中 踏出新項目穩定地工作 自由地實驗以精準和精準的意識 去設計複雜的設計到那最後的一張畫面穩定地結構和集中 動態內容以優雅和有效率的 管理豐富內容以 AI 為主體,以每個螢幕為主體,以 AI 為主體,以每個螢幕為主體,創造和自訂互動,從細節動作到進步行為,並通過視覺,從策劃到客戶手機。這是網路創作的規模,複雜,快速,無限,這是你的工作室。邏輯推導還是在模仿他在網上看過的無數解題步驟呢關於這個問題學術界和工業界爭論不休但是爭論的意義遠不如我們去搞清楚這種所謂的推理能力到底是怎麼來的以及我們如何才能穩定的可靠的駕馭它而要解答這個問題有一個人的名字我們就繞不開他就是來自於谷歌DeepMind的丹尼周他和他的團隊可以說是奠定了我們今天理解和使用大圓模型推理能力的基石他們開創性的提出了像思維鏈提示和自下行這類的關鍵技術並且深度參與了谷歌GNI模型推理能力的構建最近丹尼周在斯坦福大學做了一場演講系統性的梳理了從他創立谷歌大腦的推理團隊開始到今天我們所看到的強大的AI這條技術路線是如何一步一步演進的這場講座的信息量巨大它不僅揭示了AI推理能力的真實性更是对过去几年中所有相关技术的一次去媚所以今天这期视频我们就将以丹尼周的这场讲座为蓝本带着大家从最基础的概念出发层层递进彻底搞懂大约模型思考秘密我们会看到那些看似神奇的技术背后往往遵循着一些极其简单而深刻的原理相信看完这期视频你再看待大约模型的时候会有一个全新的更加清晰的视角在深入探讨之前我们必须先明确一件事情当我们讨论大约模型的推理时我们到底在谈论什么丹尼周一上来就抛出了一个非常清晰而且可操作的定义这个定义也成为了整个领域的共识他说关于模型到底会不会推理的哲学辩论他从不参加因为没有一个明确的定义大家都是在自说自话而在他的团队里推理是有一个非常具体的含义的那就是在模型的推理中我们可以看到模型的推理是一个非常具体的语言它是一个非常具体的语言它是一个非常具体的语言它是一个非常具体的语言它是一个非常具体的语言它是一个非常具体的语言也就是你的問題和最終輸出也就是答案之間生成的所有中間步驟這個定義非常關鍵它把一個模糊的哲學層面的思考概念轉化成了一個具體的工程上可以實現和優化的目標為了讓我們更好地理解這個目標丹尼周設計了一個非常巧妙的任務叫做末尾字母拼接這個任務聽起來很簡單比如說我問模型請拼接artificial intelligence這兩個單詞的末尾字母如果我們直接讓模型輸出答案它可能會憑藉著語言的慣性直接猜一個答案比如說LE這時候它只是在預測下一個最可能的字符而不是在執行一個多步驟的邏輯操作但是如果我們引導模型先生成中間步驟它的輸出就會變成這樣artificial的最後一個字母是Lintelligence的最後一個字母是E將L和E拼接起來得到這個結果这就是丹妮周所定义的推理她把一个复杂的任务分解成了一系列简单的可执行的子任务最终导出了正确的答案你可能会觉得这不就是我们人类解决问题的方式吗先思考再作答但是丹妮周提醒我们作为研究者必须时刻记住大猿模型不是人类它们只是概率模型把它们拟人化虽然有助于我们理解但是也很容易让我们走入歧途一个有趣的故事是丹妮周最初尝试的是首字母拼接但是她发现当时所有的模型都能够做得很好遠不如我們去搞清楚這種所謂的推理能力到底是怎麼來的以及我們如何才能穩定的可靠的駕馭它而要解答這個問題有一個人的名字我們就繞不開他就是來自於谷歌DeepMind的丹尼周他和他的團隊可以說是奠定了我們今天理解和使用大圓模型推理能力的基石他們開創性的提出了像思維鏈提示和自洽性這類的關鍵技術並且深度參與了谷歌近代模型推理能力的構建最近丹尼周在斯坦福大學做了一場演講系統性的梳理了從他創立谷歌大腦的推理團隊開始到今天我們所看到的強大的AI這條技術路線是如何一步一步演進的這場講座的信息量巨大它不僅揭示了AI推理能力的本質更是對過去幾年中所有相關技術的一次趣味所以今天這期視頻我們就將以丹尼周的這場講座為藍本帶著大家從谷歌大腦開始最基础的概念出发层层递进彻底搞懂大学模型思考秘密我们会看到那些看似神奇的技术背后往往遵循着一些极其简单而深刻的原理相信看完这期视频你再看待大学模型的时候会有一个全新的更加清晰的视角在深入探讨之前我们必须先明确一件事情当我们讨论大学模型的推理时我们到底在谈论什么单机中一上来就抛出了一个非常清晰而且可操作的定义这个定义也成为了整个领域的共识他说关于模型到底会不会推理的他从不参加因为没有一个明确的定义大家都是在自说自话而在他的团队里推理是有一个非常具体的含义的那就是在模型的输入也就是你的问题和最终输出也就是答案之间生成的所有中间步骤这个定义非常关键他把一个模糊的推理当成了一个中间步骤哲学层面的思考概念转化成了一个具体的工程上可以实现和优化的目标为了让我们更好的理解这个目标丹妮周设计了一个非常巧妙的任务叫做末尾字母拼接这个任务听起来很简单比如说我问模型请拼接artificial intelligence这两个单词的末尾字母如果我们直接让模型输出答案它可能会凭借着语言的惯性直接猜一个答案比如说LE这时候它只是在预测下一个最可能的字符而不是在执行一个多步骤的逻辑操作但是如果我们引导模型先生成中间步骤它的输出就会变成这样artificial的最后一个字母是Lintelligence的最后一个字母是E将L和E拼接起来得到LE这就是丹妮周所定义的推理她把一个复杂的任务分解成了一系列简单的可执行的子任务最终导出了正确的答案那我们就来看看你可能会觉得这不就是我们人类解决问题的方式吗先思考再作答但是丹妮周提醒我们作为研究者必须时刻记住大猿模型不是人类它们只是概率模型把它们拟人化虽然有助于我们理解但是也很容易让我们走入歧途一个有趣的故事是丹妮周最初尝试的是手字母拼接但是她发现当时所有的模型都能够做得很好为什么呢因为互联网上有大量的缩写词模型在预训练阶段已经背会了如何拼接手字母于是她换成了末尾字母拼接结果当时所有的模型都失败了这恰恰说明模型并没有真正理解拼接这个动作而只是记住了某种常见的模式那么为什么要如此执着于生成这些中间步骤呢仅仅是为了模仿人类吗当然不是这背后有着非常坚实的理论依据丹妮周提到了她们和斯坦福大学教授腾尚合作的一項理論研究這個研究得出了一個非常強大的結論對於任何一個可以被大小為T的布爾電路解決的問題一個常數大小的Transformer模型可以通過生成OT長度的中間步驟來解決它這句話聽起來有點過於技術我們來把它翻譯一下布爾電路可以被看作是執行邏輯運算的基本單元任何復雜的計算任務比如說運行一個大型的軟件本質上都可以被分解成一個巨大規模的布爾電路這裡的大小T就代表了問題的計算復雜度這個理論告訴我們哪怕是一個相對簡單的Transformer模型只要你允許它生成足夠長的思考過程也就是中間步驟它就有潛力解決幾乎任何可計算的問題反過來說如果我們強迫模型直接蹦出最終答案就相當於要求這個模型的網絡深度本身要能夠模擬整個復雜度这要么需要一个巨大到不切实际的深度要么就根本无法解决问题所以让模型思考生成中间步骤不是一个可有可无的选项而是在计算原理上解锁模型解决复杂问题能力的一把金钥匙这彻底改变了我们训练和使用大圆模型的范式从单纯的追求答案转向追求过程好 既然我们知道了推理过程如此的重要那么下一个问题自然就是如何让模型来生成这个过程呢这里丹基周提出了一个颠覆了当时很多人认知的观点当时普遍认为一个普通的只经过预训练的大圆模型是不会推理的你必须通过像思维链提示这样的高级技巧或者进行专门的微调才能够教会他们推理但是丹基周说这个观点是错的而且大错特错他认为预训模型早就已经准备好进行推理了我们所需要做的就是僅僅是改變解碼的過程這又是一個非常深刻的洞察為了證明這一點他舉了一個經典的數學硬題我有三個蘋果我的爸爸比我多兩個蘋果我們總共有多少個蘋果如果你把這個問題直接輸入給一個原始的預訓練模型比如說早期的GDP3或者是拉馬然後使用默認的貪婪解碼方式會發生什麼呢貪婪解碼的意思是模型在生成每一個詞的時候總是會選擇當前概率最高的那一個在這種模式下模型很可能會直接輸出一個看似合理但卻是錯誤的答案比如說五個蘋果因為它看到了三個和多兩個就直接聯想到了五這是模型的一種直覺反應或者說是一種系統思維但是模型的強大之處在於它的輸出概率分佈中並不僅僅只有這一個選項在生成第一個詞的時候五個可能是概率最高的但是還有第二第三第四高的選項如果我們把這個模型直接輸入給一個原始的預訓練模型我們總共會有多少個蘋果我們總共會有多少個蘋果我們總共會有多少個蘋果我們總共會有多少個蘋果我們總共會有多少個蘋果我們總共會有多少個蘋果我們不那麼貪婪而是去探索一下那些概率稍低一些的岔路奇蹟就會發生了接著單機周向我們展示了這些隱藏的候選答案比如說候選二可能以我字開頭模型會生成我有三個蘋果我爸爸比我多兩個所以他有五個蘋果3加5等於8所以我們總共有八個蘋果這是一個完美的推理鏈答案也正確候選三可能以我們開頭模型會生成我們總共有八個蘋果雖然沒有過程但是答案也對了候選四可能以你字開頭模型會生成你有三個蘋果你爸爸有3加2等於五個蘋果你們總共有3加5等於八個蘋果這同樣是一個清晰的推理鏈看到了嗎正確的推理路徑其實一直都存在於模型的輸出空間裏它們就像是隱藏在主幹道旁邊的小路默認的貪婪解碼因為只看到了眼前最寬的路所以錯過了它們這個發現被探索了他告诉我们推理能力不是被注入到模型里面的而是模型在学习海量稳稳中蕴含的逻辑关系之后自然涌现出来的于是我们的任务从交汇模型推理变成了如何引导模型把它已经知道的东西以正确的形式表达出来那么问题就变成了在这么多候选的输出力我们怎么知道哪一个是最好的呢一个简单的想法是看长度带有思考过程的回答通常会更长但是丹妮周的团队发现一个更可靠的指标那就是答案自信度他们观察到一个惊人的现象对于那些包含了正确思维链的回答模型在生成最终答案的那个词比如说数字8的时候其内部的自信度也就是概率会异常的高在这个苹果的例子里模型预测8这个词的概率可能会高达98%这是一个非常强的信号因为对于一个拥有巨大词汇表的模型来说它是一个非常强的信号所以我们就要更加的注意模型的自信度它是一个非常强的信号因为对于一个拥有巨大词汇表的模型来说它是一个非常强的信号所以我们就要更加的注意模型的自信度通常每個詞的概率都接近於零這就像是一個人在經過深思熟慮之後對自己得出的結論會非常篤定一樣所以說思維鏈解碼的核心就兩步一超越貪婪解碼生成並且檢查更多的候選輸出二選擇那個對最終答案自信度最高的候選這個方法雖然簡單有效但還是需要寫一些代碼對於普通用戶來說不夠友好於是研究者們開始思考我們能不能用更自然的方式比如說自然語言來重塑模型的輸出概率分布讓那些帶有思考過程的優秀答案能夠自動排到第一名這樣我們用最簡單的貪婪解碼就能夠直接得到它這就引出了我們後來耳熟能詳的一系列提示工程技術首先最著名的就是思維鏈提示它的做法非常直觀在你提出你的問題之前先給模型看一兩個類似的從問題到思考過程再到答案经常接触大模型的朋友一定会有过这样的经历当你向AI提出一个复杂的问题时它不仅给出了答案还给出了详细的一步一步的看起来逻辑严密的解题过程那一瞬间你是不是感觉屏幕对面的不再是一堆冰冷的代码而是一个真正能够推理的智能体但是有的时候你换一个同样复杂但是略有不同的问题时它又会给出一个错的离谱的答案让你觉得它根本什么都不懂只是一个更高级的复读机这种体验上的巨大反差正是当前AI领域最核心的谜题之一大圆模型展现出的推理能力究竟是一种真正的智能涌现还是一种基于海量数据训练出来的更高级的模式匹配它是在进行逻辑推导还是在模仿它在网上看过的无数解题步骤关于这个问题学术界和工业界争论不休但是争论的意义远不如理解不如我们去搞清楚这种所谓的推理能力到底是怎么来的以及我们如何才能稳定的可靠的驾驭它而要解答这个问题有一个人的名字我们就绕不开他就是来自于谷歌丁曼的丹尼周他和他的团队可以说是奠定了我们今天理解和使用大元模型推理能力的基石他们开创性的提出了像思维链提示和自下行这类的关键技术并且深度参与了谷歌静态模型推理能力的构建最近丹尼周在斯坦福大学做了一场演讲系统性的梳理了最基础的概念出发层层递进彻底搞懂大学模型思考秘密我们会看到那些看似神奇的技术背后往往遵循着一些极其简单而深刻的原理相信看完这期视频你再看待大学模型的时候会有一个全新的更加清晰的视角在深入探讨之前我们必须先明确一件事情当我们讨论大学模型的推理时我们到底在谈论什么单机中一上来就抛出了一个非常清晰而且可操作的定义这个定义也成为了整个领域的共识他说关于模型到底会不会推理的哲学辩论他从不参加因为没有一个明确的定义大家都是在自说自话而在他的团队里推理是有一个非常具体的含义的那就是在模型的输入也就是你的问题和最终输出也就是答案之间生成的所有中间步骤这个定义非常关键他把一个模糊的哲学辩论完全放在了模型里面这就是模型的输入也就是你的问题和最终输出也就是答案之间生成的所有中间步骤这个定义非常关键他把一个模糊的哲学辩论从科学层面的思考概念转化成了一个具体的工程上可以实现和优化的目标为了让我们更好地理解这个目标丹尼周设计了一个非常巧妙的任务叫做末尾字母拼接这个任务听起来很简单比如说我问模型请拼接Artificial Intelligence这两个单词的末尾字母如果我们直接让模型输出答案它可能会凭借着语言的惯性直接猜一个答案比如说LE这时候它只是在预测下一个最可能的字符而不是在执行一个多步骤的逻辑操作如果我们引导模型先生成中间步骤它的输出就会变成这样Artificial的最后一个字母是LIntelligence的最后一个字母是E将L和E拼接起来得到LE这就是丹尼周所定义的推理他把一个复杂的任务分解成了一系列简单的可执行的子任务最终导出了正确的答案你可能问這不就是我們人類解決問題的方式嗎先思考再作答但是丹妮周提醒我們作為研究者必須時刻記住大圓模型無視人類它們只是概率模型把它們擬人化雖然有助於我們理解但是也很容易讓我們走入歧途一個有趣的故事是丹妮周最初嘗試的是手字母拼接但是她發現當時所有的模型都能夠做得很好為什麼呢因為互聯網上有大量的縮寫詞模型在預訓練階段已經背會了如何拼接手字母於是她換成了末尾字母拼接結果當時所有的模型都失敗了這恰恰說明模型並沒有真正理解拼接這個動作而只是記住了某種常見的模式那麼為什麼要如此執著於生成這些中間步驟呢僅僅是為了模仿人類嗎當然不是這背後有著非常堅實的理論依據丹妮周提到了她們和斯坦福大學教授藤上華曾經說過合作的一項理論研究這個研究得出了一個非常強大的結論對於任何一個可以被大小為T的布爾電路解決的問題一個常數大小的Transformer模型可以通過生成OT長度的中間步驟來解決它這句話聽起來有點過於技術我們來把它翻譯一下布爾電路可以被看作是執行邏輯運算的基本單元任何復雜的計算任務比如說運行一個大型的軟件本質上都可以被分解成一個巨大規模的布爾電路這裡的大小T就代表了問題的計算復雜度這個理論告訴我們哪怕是一個相對簡單的Transformer模型只要你允許它生成足夠長的思考過程也就是中間步驟它就有潛力解決幾乎任何可計算的問題反過來說如果我們強迫模型直接蹦出最終答案就相當於要求這個模型的網絡深度本身要能夠模擬整個復雜度计算过程这要么需要一个巨大到不切实际的深度要么就根本无法解决问题所以让模型思考生成中间步骤不是一个可有可无的选项而是在计算原理上解锁模型解决复杂问题能力的一把金钥匙这彻底改变了我们训练和使用大学模型的方式从单纯的追求答案转向追求过程好 既然我们知道了推理过程如此的重要那么下一个问题自然就是如何让模型来生成这个过程这里丹妮周提出了一个颠覆了当时很多人认知的观点当时普遍认为一个普通的只经过预训练的大学模型是不会推理的你必须通过像思维链提示这样的高级技巧或者进行专门的微调才能够教会他们推理但是丹妮周说这个观点是错的而且大错特错她认为预训模型早就已经准备好进行推理了我们所需要的就是只要改變解碼的過程這又是一個非常深刻的洞察為了證明這一點他舉了一個經典的數學硬題我有三個蘋果我的爸爸比我多兩個蘋果我們總共有多少個蘋果如果你把這個問題直接輸入給一個原始的預訓練模型比如說早期的GDP-3或者是拉馬然後使用默認的貪婪解碼方式會發生什麼呢貪婪解碼的意思是模型在生成每一個詞的時候總是會選擇當前概率最高的那一個在這種模式下模型很可能會直接輸出一個看似合理但卻是錯誤的答案比如說五個蘋果因為他看到了三個和多兩個就直接聯想到了五這是模型的一種直覺反應或者說是一種系統思維但是模型的強大之處在於它的輸出概率分佈中並不僅僅只有這一個選項在生成第一個詞的時候五個可能是概率最高的但是還有第二第三第四高的選項任何一個都可以后来在2021年OPI更进一步构建了一个更大更著名的数据集也就是GSM给出了详细的一步一步的看起来逻辑严密的解题过程大家好这里是最佳拍档我是大飞经常接触大模型的朋友一定会有过这样的经历当你向AI提出一个复杂的问题时它不仅给出了答案还给出了详细的一步一步的看起来逻辑严密的解题过程那一瞬间你是不是感觉屏幕对面的不再是一堆冰冷的代码而是一个真正能够推理的智能体但是有的时候你换一个同样复杂但是略有不同的问题时它又会给出一个错的离谱的答案让你觉得它根本什么都不懂只是一个更高级的复读机这种体验上的巨大反差正是当前AI领域最核心的谜题之一大圆模型展现出的推理能力究竟是一种真正的智能涌现还是一种基于海量的推理能力训练出来的更高级的模式匹配呢它是在进行逻辑推导还是在模仿它在网上看过的无数解题步骤呢关于这个问题学术界和工业界争论不休但是争论的意义远不如我们去搞清楚这种所谓的推理能力到底是怎么来的以及我们如何才能稳定的可靠的驾驭它而要解答这个问题有一个人的名字我们就绕不开他就是来自于谷歌DeepMind的丹尼周他和他的团队可以说是奠定了我们今天理解和使用大元模型推理能力的基石他们开创性的提出了像思维链提示和自下行这类的关键技术并且深度参与了谷歌Gemini模型推理能力的构建最近丹尼周在斯坦福大学做了一场演讲系统性的梳理了从他创立谷歌大脑的推理团队开始到今天我们所看到的强大的AI这条技术路线是如何一步一步演进的這場講座的信息量巨大它不僅揭示了AI推理能力的本質更是對過去幾年中所有相關技術的一次去媚所以今天這期視頻我們就將以丹妮周的這場講座為藍本帶著大家從最基礎的概念出發層層遞進徹底搞懂大約模型思考秘密我們會看到那些看似神奇的技術背後往往遵循著一些極其簡單而深刻的原理相信看完這期視頻你再看待大約模型的時候會有一個全新的更加清晰的視角在深入探討之前我們必須先明確一件事情當我們討論大約模型的推理時我們到底在談論什麼丹妮周一上來就拋出了一個非常清晰而且可操作的定義這個定義也成為了整個領域的共識她說關於模型到底會不會推理的哲學辯論她從不參加因為沒有一個明確的定義大家都是在自說自話而在她的團體中推理是有一個非常具體的含義的那就是在模型的輸入也就是你的問題和最終輸出也就是答案之間生成的所有中間步驟這個定義非常關鍵它把一個模糊的哲學層面的思考概念轉化成了一個具體的工程上可以實現和優化的目標為了讓我們更好地理解這個目標單機周設計了一個非常巧妙的任務叫做末尾字母拼接這個任務聽起來很簡單比如說我問模型請拼接artificial intelligence這兩個單詞的末尾字母如果我們直接讓模型輸出答案它可能會憑藉著語言的慣性直接猜一個答案比如說LE這時候它只是在預測下一個最可能的字符而不是在執行一個多步驟的邏輯操作但是如果我們引導模型先生成中間步驟它的輸出就會變成這樣artificial的最後一個字母是Lintelligence的中間步驟最後一個字母是E將L和E拼接起來得到LE這就是丹妮周所定義的推理他把一個復雜的任務分解成了一系列簡單的可執行的子任務最終導出了正確的答案你可能會覺得這不就是我們人類解決問題的方式嗎先思考再作答但是丹妮周提醒我們作為研究者必須時刻記住大猿模型不是人類它們只是概率模型把它們擬人化雖然有助於我們理解但是也很容易讓我們走入歧途一個有趣的故事是丹妮周最初嘗試的是手字母拼接但是他發現當時所有的模型為什麼呢因為互聯網上有大量的縮寫詞模型在預訓練階段已經背會了如何拼接手字母於是它換成了落尾字母拼接結果當時所有的模型都失敗了這恰恰說明模型並沒有真正理解拼接這個動作而只是記住了某種動作那么为什么要如此执着于生成这些中间步骤呢仅仅是为了模仿人类吗当然不是这背后有着非常坚实的理论依据丹妮周提到了他们和斯坦福大学教授腾尚华团队合作的一项理论研究这个研究得出了一个非常强大的结论对于任何一个可以被大小为T的布尔电路解决的问题一个常数大小的Transformer模型可以通过生成OT长度的中间步骤来解决它这句话听起来有点过于技术我们来把它翻译一下布尔电路可以被看作是执行逻辑运算的基本单元任何复杂的计算任务比如说运行一个大型的软件本质上都可以被分解成一个巨大规模的布尔电路这里的大小T就代表了问题的计算复杂度这个理论告诉我们哪怕是一个相对简单的Transformer模型只要你允许它生成足够长的思考过程也就是一个中間步驟它就有潛力解決幾乎任何可計算的問題反過來說如果我們強迫模型直接蹦出最終答案就相當於要求這個模型的網絡深度本身要能夠模擬整個復雜的計算過程這要麼需要一個巨大到不切實際的深度要麼就根本無法解決問題所以讓模型思考生成中間步驟不是一個可有可無的選項而是在計算原理上解鎖模型解決復雜問題能力的一把金鑰匙這徹底改變了我們訓練和使用大圓模型的範式從單純的追求答案轉向追求過程好 既然我們知道了推理過程如此的重要那麼下一個問題自然就是如何讓模型來生成這個過程呢這裡丹妮周提出了一個顛覆了當時很多人認知的觀點當時普遍認為一個普通的只經過預訓練的大圓模型是不會推理的你必須通過向模型進行推理思维链提示这样的高级技巧或者进行专门的微调才能够交费他们推理但是丹基周说这个观点是错的而且大错特错他认为预训类模型早就已经准备好进行推理了我们所需要做的仅仅是改变解码的过程这又是一个非常深刻的洞察为了证明这一点他举了一个经典的数学硬题我有三个苹果我的爸爸比我多两个苹果我们总共有多少个苹果如果你把这个问题直接输入给一个原始的预训类模型比如说早期的GPT-3或者是拉玛然后使用默认的贪婪解码方式会发生什么呢贪婪解码的意思是模型在生成每一个词的时候总是会选择当前概率最高的那一个在这种模式下模型很可能会直接输出一个看似合理但却是错误的答案比如说五个苹果因为他看到了三个和多两个就直接联想到了五这是模型的一种直觉反应或者说是真正能夠推理的智能體但是有的時候你換一個同樣複雜但是略有不同的問題時它又會給出一個錯的離譜的答案讓你覺得它根本什麼都不懂只是一個更高級的復讀機這種體驗上的巨大反差正是當前AI領域最核心的謎題之一大圓模型展現出的推理能力究竟是一種真正的智能湧現還是一種基於海量數據訓練出來的更高級的模式匹配呢它是在進行邏輯推導還是在模仿它在網上看過的無數解題步驟呢關於這個問題學術界和工業界爭論不休但是爭論的意義遠不如我們去搞清楚這種所謂的推理能力到底是怎麼來的以及我們如何才能穩定的可靠的駕馭它而要解答這個問題有一個人的名字我們就繞不開他就是來自於谷歌DeepMind的丹尼周他和他的團隊可以說是奠定了我們今天理解的科學思想和使用大圆模型推理能力的基石他们开创性的提出了像思维链提示和自下性这类的关键技术并且深度参与了谷歌金奶模型推理能力的构建最近丹尼州在斯坦福大学做了一场演讲系统性的梳理了从它创立谷歌大脑的推理团队开始到今天我们所看到的强大的AI这条技术路线是如何一步一步演进的这场讲座的信息量巨大它不仅揭示了AI推理能力的本质更是对过去几年中所有相关技术的一次去媚所以今天这期视频我们就将以丹尼州的这场讲座为蓝本带着大家从最基础的概念出发层层递进彻底搞懂大圆模型思考秘密我们会看到那些看似神奇的技术背后往往遵循着一些极其简单而深刻的原理相信看完这期视频你再看待大圆模型的时候会有一个全新的更加清晰的理解在深入探讨之前我们必须先明确一件事情当我们讨论大约模型的推理时我们到底在谈论什么丹尼周一上来就抛出了一个非常清晰而且可操作的定义这个定义也成为了整个领域的共识他说关于模型到底会不会推理的哲学辩论他从不探加因为没有一个明确的定义大家都是在自说自话而在他的团队里推理是有一个非常具体的含义的那就是在模型的输入也就是你的问题和最终输出也就是答案之间生成的所有中间步骤这个定义非常关键他把一个模糊的哲学层面的思考概念转化成了一个具体的工程上可以实现和优化的目标为了让我们更好地理解这个目标丹尼周设计了一个非常巧妙的任务叫做末尾字母拼接这个任务听起来很简单比如说我问模型请拼接Artificial IntelligenceIntelligence這兩個單詞的末尾字母如果我們直接讓模型輸出答案它可能會憑藉著語言的慣性直接猜一個答案比如說LE這時候它只是在預測下一個最可能的字符而不是在執行一個多步驟的邏輯操作但是如果我們引導模型先生成中間步驟它的輸出就會變成這樣artificial的最後一個字母是Lintelligence的最後一個字母是E將L和E拼接起來得到LE這就是丹妮周所定義的推理她把一個復雜的任務分解成了一系列簡單的可執行的子任務最終導出了正確的答案你可能會覺得這不就是我們人類解決問題的方式嗎先思考再作答但是丹妮周提醒我們作為研究者必須時刻記住大約模型不是人類它們只是概率模型把它們擬人化雖然有助於我們理解但是也很容易讓我們走入歧途一個有趣的故事是丹妮周最初尝试的是手字母拼接但是她发现当时所有的模型都能够做得很好为什么呢因为互联网上有大量的缩写词模型在预训练阶段已经背会了如何拼接手字母于是她换成了末尾字母拼接结果当时所有的模型都失败了这恰恰说明模型并没有真正理解拼接这个动作而只是记住了某种常见的模式那么为什么要如此执着于生成这些中间步骤呢仅仅是为了模仿人类吗当然不是这背后有着非常坚实的理论依据丹妮周提到了他们和斯坦福大学教授腾尚华团队合作的一项理论研究这个研究得出了一个非常强大的结论对于任何一个可以被大小为T的布尔电路解决的问题一个常数大小的Transformer模型可以通过生成OT长度的中间步骤来解决它这句话听起来有点过于技术化但是实际上是一个很好的解决方案所以这就是为什么我们要学习的这种模型是要学习的我们要学习的我们要学习的我们来把它翻译一下布尔电路可以被看作是执行逻辑运算的基本单元任何复杂的计算任务比如说运行一个大型的软件本质上都可以被分解成一个巨大规模的布尔电路这里的大小T就代表了问题的计算复杂度这个理论告诉我们哪怕是一个相对简单的Transformer模型只要你允许它生成足够长的思考过程也就是中间步骤它就有潜力解决几乎任何可计算的问题反过来说如果我们强迫模型直接蹦出最终答案就相当于要求这个模型的网络深度本身要能够模拟整个复杂的计算过程这要么需要一个巨大到不切实际的深度要么就根本无法解决问题所以让模型思考生成中间步骤不是一个可有可无的选项而是在计算原理上解锁模型解决复杂问题能力的一把金钥匙这彻底改变了我们学习的方式和使用大猿猿模型的方式從單純的追求答案轉向追求過程好 既然我們知道了推理過程如此的重要那麼下一個問題自然就是如何讓模型來生成這個過程呢這裡丹妮周提出了一個顛覆了當時很多人認知的觀點當時普遍認為一個普通的只經過預訓練的大猿猿模型是不會推理的你必須通過向思維鏈提示這樣的高級技巧或者進行專門的微調才能夠教會他們推理但是丹妮周說這個觀點是錯的而且大錯特錯他認為預訓練模型早就已經準備好進行推理了GDP3或者是拉玛然后使用默认的贪婪解码方式会发生什么呢贪婪解码的意思是模型在生成每一个词的时候总是会选择当前概率最高的那一个在这种模式下模型很可能会直接输出一个看似合理但却是错误的答案比如说5个苹果因为它看到了3个和多两个就直接联想到了5这是模型的一种直觉反应或者说是一种系统思维但是模型的强大之处在于它的输出概率分布中并不仅仅只有这一个选项在生成第一个词的时候5个可能是概率最高的但是还有第二第三第四高的选项如果我们不那么贪婪而是去探索一下那些概率稍低一些的岔路奇迹就会发生了接着单机周向我们展示了这些隐藏的候选答案比如说候选二可能以我字开头模型会生成我有3个苹果我爸爸比我多两个所以还有5个苹果3加5等于8所以我们总共有8个苹果这是一个完美的推理链答案也正确候选三可能以我们开头模型会生成我们总共有8个苹果虽然没有过程但是答案也对了候选四可能以你自开头模型会生成你有3个苹果你爸爸有3加2等于5个苹果你们总共有3加5等于8个苹果这同样是一个清晰的推理链看到了吗正确的推理路径其实一直都存在于模型的输出空间里它们就像是隐藏在主干道旁边的小路默认的贪婪解码因为只看到了眼前最宽的路所以错过了他们这个发现被称为思维链解码它告诉我们推理能力不是被注入到模型里面的而是模型在学习海量文本中蕴含的逻辑关系之后自然涌现出来的于是我们的任务从交汇模型推理变成了如何引导模型把它已经知道的东西以正常的方式推理正確的形式表達出來那麼問題就變成了在這麼多候選的輸出裏我們怎麽知道哪一個是最好的呢一個簡單的想法是看長度帶有思考過程的回答通常會更長但是丹妮周的團隊發現一個更可靠的指標那就是答案自信度他們觀察到一個驚人的現象對於那些包含了正確思維鏈的回答模型在生成最終答案的那個詞比如說數字8的時候其內部的自信度也就是概率會異常的高在這個蘋果的例子裏模型預測8這個詞的概率可能會高達98%這是一個非常強的型號因為對於一個擁有巨大詞彙表的模型來說通常每個詞的概率都接近於0這就像是一個人在經過深思熟慮之後對自己得出的結論會非常篤定一樣所以說思維鏈解碼的核心就兩步一超越貪婪解碼生成並且檢查更多的候選輸出二選擇那個最合理的答案對最終答案信證度最高的候選這個方法雖然簡單有效但還是需要寫一些代碼對於普通用戶來說不夠友好於是研究者們開始思考我們能不能用更自然的方式比如說自然語言來重塑模型的輸出概率分布讓那些帶有思考過程的優秀答案能夠自動排到第一名這樣我們用最簡單的貪婪解碼就能夠直接得到它這就引出了我們後來耳熟能詳的一系列提示工程技術首先最著名的就是思維鏈提示它的做法非常直觀在你提出你的問題之前先給模型看一兩個類似的從問題到思考過程再到答案的例子比如說你想讓模型解決前面蘋果的問題你可以先給它一個例子問題是一個農民有5個香蕉他又買了6個後來吃了2個還剩幾個答案是農民開始有5個香蕉買了6個之後他有5加6等於11個然後他吃了2個所以他剩下了5個11-2等于9个 答案是9然后你再提出你的问题我有三个苹果 我爸爸比我多两个我们总共有多少个苹果呢神奇的事情发生了模型会模仿你给出的例子的风格自动的开始一步一步分析声称详细的解析步骤最后给出正确答案从概率分布的角度来看你给出的例子极大的提升了模型声称类似思考过程的巨式的概率从而把原本隐藏在后面的正确推理路径推到了最前面但是这种方法有一个问题你需要为不同类型的任务手动编写高质量的实例 这很麻烦而且如果你自己都知道怎么解决一个类似的问题那你为什么还要问AI呢于是一个更加神奇的提示就出现了它就是让我们一步一步思考Let's think step by step但泥洲坦言这篇论文刚出来的时候他以为这是个玩笑怎么可能在问题后面加上这么一句简单的话模型就会自动模仿出来他当时就在谷歌内部的PALM模型上做了测试他非常清楚PALM的训练数据里绝对没有针对这个咒语做过任何的优化结果他震惊地发现它真的有效模型真的开始输出一步一步的解题过程了这个发现极大的启发了他尽管Let's think step by step这种零样本提示效果通常比不过提供具体事例的少样本思维链提示但是它证明了我们可以用非常通用的方式来激发模型的推理潜能然而无论是哪种提示方法都感觉有点奇怪想象一下你问一个聪明人问题还必须得在后面加上一句请一步一步思考否则他就不会思考了这显然不符合我们对于一个真正智能体的期望所以我们需要一种更稳定更内化的方式让推理能力成为模型固有的一部分而不是需要外部咒语来触发这就把我们带到了下一个阶段如果你想建立一个应用软件,但不知道该怎么制作或开始,请去看看Base44我建立了一个日常的习惯追踪器,帮助使用者计算小赢利,追踪自己的心情,反省过去的进度,并看他们走到哪里它是完全功能的,我没有写一条线你开始是要描述你的想法,你想做什么,功能,看法和感觉现在看Base做它的魔术设计互联网,建立数据库,连接逻辑工程师 设计师和产品经纪人来创造你的视野立刻当你第一版本完成了你就继续进行我加入黑色模式处理过渡确保它能顺利运行最好的部分我可以保持流畅没有阻挡没有切换工具只是建造所以开始你的想法BASE44做其他微调 我们先说监督微调SFT它的思路非常的直接我们不就是希望模型能够生成从问题到思考过程再到谈论这样的数据吗那我们就雇佣一批人针对大量的问题写出高质量的一步一步的解题方案然后我们再把这些标准答案备给模型让模型去学习这个方法在机器学习里面叫做最大三人估计简单来说就是让模型生成的序列跟人类专家写的序列尽可能的一模一样这个想法其实很早就有了单一周提到早在2017年DMI的一篇论文就在做类似的事情他们收集了一批数学硬体和人类手写的解题步骤来训练一个序列模型后来在2021年OPI更进一步构建了一个更大更著名的数据集也就是GSM8K包含了8000多个小学水平的数学题和详细解法用来微调GPT-3模型这种方法训练出来的模型在你给它一个新问题的时候确实能够生存无错的解题步骤看起来问题似乎解决了一旦模型训练好就可以随时部署不再需要分析然而在2021年夏天丹妮周的团队发现了一个严重的问题那就是SFT训练出来的模型算话能力很差它在那些和训练数据很相似的问题上表现很好但是一旦遇到一个新的类型稍微不同的问题就很容易失败他们尝试了大力出奇迹的方法扩大了数据的规模找更多的人标注更多的数据可惜结果却是无论如何扩大规模这个问题始终存在丹妮周在这里给出了一个重要的教训不要盲目地扩大规模当你的犯事本身是错误的时候再多的数据也无济于事那么SFT的犯事错在哪里了呢问题又出在流程里的哪一步呢丹妮周给出的答案可能会让你大吃一惊她说错误出在人身上这个转折点来自于自我提升后来也被称为Self-improve或者是START方法按照第一次听到机器生成的训练数据可能比较容易理解人類專家寫的還好這個想法的時候他自己也感到非常驚訝這個新範式的流程是這樣的首先我們仍然從一批問題開始但是我們不再找人類去寫解題步驟我們讓一個已經比較強大的大約模型自己去針對這些問題生成大量的多樣的解題步驟最關鍵的一步是我們用一個驗證器去檢查模型生成的這些解題步驟看哪個最終得出了正確的答案比如說對於數學題我們知道標準答案就可以直接的判斷於是我們只保留下來那些過程多樣但是結果正確的生成結果把它們當作新的高質量的訓練數據一篇在2024年1月发表的来自字节跳动的论文Rhythm with Reinforced by Tune是他在学术界看到的最早公开阐述类似思想的出版物之一他相信在OpenAI等多个机构内部大家可能都独立地发现了这个简单而又极其有效的思想现在我们必须回答那个核心的问题为什么模型自己生成的数据会比人类专家手写的数据在训练效果上更好呢这背后其实蕴含着既学习的一个第一性原理那就是直接优化你想要的东西在SFT的范式里我们优化的目标是让模型的输出模仿人类的解题步骤我们假设人类的思维过程就是最优的但是实际上人类的思维方式千差万别充满了跳跃和不一致而且人类专家写的标准答案对于模型来说可能并不是最容易学习和范化的路径而在新的范式里我们的目标变了我们不再关心模型的发展解題過程是否和人類一模一樣我們只關心一件事情它最終的答案是否正確我們用最終答案的正確性這個指標相當於強化學習裏的獎勵信號來指導模型的學習這在數學上就等同於我們要求解一個策略梯度問題模型需要調整自己的參數使得生成能夠獲得高獎勵的序列的概率最大化但金周強調我們不需要用激勵模型去思考這種擬人化的神秘的語言來描述這個過程本質上就是三件事情定義你的目標 計算梯度然後反向傳播這就是記憶學習的全部通過這種方式模型會自己去探索什麼樣的思考過程能夠最穩定最泛化的導向正確的答案這些過程可能看起來跟人類的思維不完全一樣但是它們更符合模型自身內部結構的學習路徑這個方式的轉變威力是巨大的它也讓我們明白在整個自我進化的循環中最終的答案是否正確最最关键的环节不是什么花哨的强化学习算法而是那个验证器一个可靠的能够自动判断答案好坏的验证器也是整个新范式的基石这样他想起了加拿大计算机科学家强化学习之父Richard Sutton在2001年写的一篇文章标题验证是通往人工智能的关键20多年前的冻结在今天的大猿模型时代得到了完美的印证通过这种自我进化的方式训练出来的模型推理能力达到了一个前所未有的高度他所展现出来的智慧与经典的人工智能有着本质的不同戴季周在这里引用了一句名言来自于国际象棋大师加里·卡斯·帕罗夫在1997年输给IBM的深蓝之后说的话他说深蓝的智能就像你给闹钟编程让它准时响起一样是程序化的智能卡斯帕罗夫说的没错深蓝的强大来自于穷举式的搜索它会暴力计算未来几个月的数据甚至幾十步棋的所有可能性然後選擇最優繼這是經典AI的核心思想但是大圓模型的推理完全不同它是一種類人的啟發式的推理過程是從海量的語言數據中湧現出來的而不是依賴於任何顯示的暴力的搜索為了展示這一點單機周分享了一個令人攀岸較絕的例子這個例子來自於谷歌內部的一個模型問題是這樣的請使用數字1到10每個數字只能夠用一次通過加法和乘法運算得到結果2025這是一個非常難的組合優化問題如果用傳統的方法你需要寫一個程序去進行暴力搜索嘗試各種組合但是讓我們看看這個GNDI模型是怎麼思考的單機周展示了模型在生成最終答案之前內部的思考過程模型首先判斷2025是一個相對較大的數字這表明乘法將在其中扮演重要角色這是一個非常像人類的思考方法然後模型突然冒出了一個驚人的洞察值得注意的是2025是45的平方單線周坦言他自己出這道題的時候都完全沒有意識到這一點這給解決問題提供了一個巨大的線索接下來模型的思考繼續深入目標很大我們應該考慮如何得到較大的中間成績我們的目標是構建一些成績讓它接近2025的平方也就是45在經過一長串類似這樣的自我對話和推理之後模型最終給出了答案並且它的答案完美地遵循了自己的思考路徑它將1到10的數字分成了兩組每一組都通過運算得到了45最後模型將兩個45相乘得到了2025整個過程沒有任何窮舉搜索模型就像是一個頂尖的數學家通過洞察啟發著思考和目標才能夠達到最大的成績一步一步逼近了答案這個例子有力的回應了Richard Sutton在他著名的文章《苦澀的教訓》中提出的觀點Sutton在看到AlphaGo的成功之後總結到人工智能領域幾十年的研究表明最終能夠規模化並且取得成功的只有兩種方法 學習和搜索但是丹尼周在這裡對這個苦澀的教訓提出了一個更進一步的看法也許我們只需要學習就夠了一個通過大規模學習訓練出來的模型它的內部湧現出來的推理能力本身就可以完成過去需要依賴搜索當然這並不是說搜索完全沒有用搜索可以作為一種外部工具被模型調用就像是我們使用計算器一樣但是在構建模型的核心推理能力時重點應該放在學習上通過強化學習微調訓練出來的模型已經非常強大但這還不是終點丹尼周接著介紹了兩種在推理時進一步壓縮模型性能的方法