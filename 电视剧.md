# 笔记总结

![[Pasted image 20250914224901515.png]]

- GPU平行運算能力有限，Batch Size過大時，計算一個Batch所需時間會隨之增加。
- 使用V100 GPU，Batch Size為6萬時，計算Gradient可在10秒內完成。
- 與過去相比，GPU運算速度大幅提升(例如，從980/760到V100)。
- Batch Size小時(例如1)，需要更多次Update才能跑完一個Epoch，總時間較長。
- Batch Size大時(例如1000)，雖每次Update時間略長，但Update次數少，跑完一個Epoch總時間反而較短。
- 考慮平行運算，大Batch Size跑完一個Epoch所需時間反而更少。
- 單純比較Batch Size大小與時間的關係，用「技能時間冷卻」來描述不夠精確。

## 深入学习总结

<think>
嗯，我现在需要深入分析用户提供的关于GPU并行运算能力和batch size的问题，并按照用户的要求进行分析和描述。首先，我需要理解每个部分的具体内容。

### 核心概念解释

1. **核心概念**：GPU（Graphical Processing Unit）是一种高性能的多核处理器，特别擅长并行计算。在深度学习模型训练中，数据被划分为多个批次来处理，每个批次由多个样本组成。
   
2. **具体应用场景**：
   - 理解GPU如何提升并行运算能力，尤其是在处理不同batch size时。
   - 探讨如何选择合适的batch size以优化训练效率和模型性能。

### 具体应用场景

1. **处理大规模批数据**：在某些深度学习任务中，如图像分类，使用较大的batch size可以减少训练时间。V100的并行计算能力使其能够高效处理大量批次的数据。
   
2. **轮询策略优化**：当需要频繁切换处理单元（like、stride）时，GPU通过轮询策略来提高资源利用率和性能。这种轮询策略在处理不同batch size时表现出色。

3. **神经网络结构优化**：许多模型设计以并行计算为核心，如残差网络等。这些结构在处理大规模数据时能更有效地利用GPU资源，确保梯度计算的高效性。

### 相关技术原理

1. **数据并行**：将数据划分为多个部分，分别由不同的GPU或CPU处理。
   
2. **轮询策略**：在需要时切换处理单元（like、stride）来优化任务完成速度。V100通过轮询提升计算效率。

### 实际案例分析

1. **batch size = 6万, GPU V100**：
   - 在处理6万批次的数据时，GPU V100展现了其强大的并行运算能力，显著提高了训练效率。
   
2. **batch size = 1**:
   - 当处理每个样本单独的情况时，可能需要更多次轮询以处理较小的批量大小。虽然每次更新的时间略长，但总运行时间反而较短。

3. **batch size = 6000, GPU V100**：
   - 在中等批量大小的数据处理下，GPU通过轮询和并行计算机制，显著缩短了训练时间。

4. **batch size = 32, GPU V100**:
   - 在较小的批量大小下，每次更新的时间稍长，但总运行时间较短，说明GPU在处理小批量时仍具备较高的效率。

5. **batch size = 6000, GPU V100**：
   - 当处理较大的批量时，GPU通过轮询和并行计算，进一步优化了训练过程，提高了整体性能。

### 总结

- **并行计算能力**：GPU V100具备强大的多核架构，能够并行处理大量数据，特别是在处理大规模batch size时表现突出。
  
- **神经网络结构优化**：许多模型设计以并行计算为核心，如残差网络等，确保了在处理大批量数据时的高效性。

- **训练策略优化**：通过调整batch size和算法设计，使得较大的batch size能够结合并行计算的优势，在固定时间内完成更多批次的数据处理。

总体而言，V100在处理6万 batch size 的情况下表现良好，主要得益于其强大的并行计算能力、高效的轮询策略以及神经网络的结构优化。

---

# 原始语音转文字

Batch的話會怎麼樣呢?如果是一個Batch如果Batch Size等於1的話代表我們每次update參數的時候我們只要看一筆資料就好Batch Size等於1代表我們只需要拿一筆資料出來算Loss我們就可以update我們的參數所以每次我們update參數的時候看一筆資料就好所以我們開始的點在這邊看一筆資料就update一次參數再看一筆資料就update一次參數再看一筆資料就update一次參數如果今天總共有20筆資料的話那在一個App裡面 我們的參數會update20次那不過因為我們現在是只看一筆資料就update這參數所以用一筆資料算出來的loss顯然是比較noisy的所以我們今天update的方向你會發現它是曲曲折折的所以如果我們比較左邊跟右邊哪一個比較好呢它們有什麼差別呢你會發現左邊這種方式沒有用fetch的方式它蓄力的時間比較長它技能冷卻的時間比較長 把所有的資料都看過一遍才能夠update一次參數而右邊的這個方法Batch Size等於1的時候序列的時間比較短每次看到一筆參數每次看到一筆資料你就會更新一次你的參數所以今天假設2筆是10筆資料看完所有資料看過一遍你已經更新了20次的參數但是左邊這樣子的方法有一個優點就是它這一步走的是穩的那右邊這個方法它的缺點就是它每一步走的是不穩那左邊跟右邊 到底哪邊比較好呢看起來左邊的方法跟右邊的方法他們各自都有擅長跟不擅長的東西左邊是蓄力時間長但是威力比較大右邊技能冷卻時間短但是他是比較不準的他是亂槍打鳥型的到底誰比較好呢看起來各自有各自的優缺點但是你會覺得說左邊的方法技能冷卻時間長右邊的方法技能冷卻時間短那只是你沒想到 沒有考慮平行運算而已實際上考慮平行運算的話左邊這個並不一定時間比較長怎麼說呢這邊是真正的實驗結果事實上比較大的Batch Size你要算Loss再進而算Gradient所需要的時間不一定比小的Batch Size要花的時間長以下是做在一個叫做Admiss的Component上面Admiss是所謂的 手寫數字辨識的Component就是機器要做的事情就是給他一張圖片然後判斷這張圖片是0到9的哪一個數字然後做數字的分類那At least是機器學習的果蠅就是假設你今天從來沒有做過機器學習的任務一般大家第一個會嘗試的機器學習的任務往往就是做At least 做手寫數字辨識這邊我們就是做了一個實驗我們想要知道說給機器一個Batch他要計算出Gradient進了Update參數 到底需要花多少的時間這邊列出了Batch Size等於1、等於10、等於100、等於1000所需要耗費的時間你會發現說Batch Size從1到1000需要耗費的時間幾乎是一樣的為什麼呢你可能會說直覺上有1000筆資料那需要計算Loss然後計算歸零花的時間不會是一筆資料的1000倍嗎那但是實際上並不是這樣的因為在實際上做運算的時候我們要GPU可以做 平行運算,所以因為你可以做平行運算的關係這一千筆資料是平行處理的所以一千筆資料所花的時間並不是一筆資料的一千倍當然GPU平行運算的能力還是有它的極限當你的Batch Size真的非常非常巨大的時候GPU在跑完一個Batch計算出規點所花費的時間還是會隨著Batch Size的增加而逐漸增長所以今天如果Batch Size是 自從1到1000所需要的時間幾乎是一樣的但是當你的Batch Size增加到1萬乃至增加到6萬的時候你就會發現GPU要算完一個Batch把這個Batch裡面的資料都拿出來算Loss再進而算Gradient所要耗費的時間確實有隨著Batch Size的增加而逐漸增長但你會發現這邊用的是V100所以他挺厲害的給他6萬筆資料一個Batch裡面塞了6萬筆資料他在10秒鐘之內也是把Gradient塞完 一年就算出來好 那這個Fetch Size的大小跟時間的關係其實每年都會做這個實驗啦我特別把舊的投影片放在這邊啦如果你有興趣的話可以看一下可以看到什麼呢可以看到這個時代的眼鏡這樣嘿17年的時候用的是那個9802015年的時候用的是那個760然後980要跑什麼6萬個Fetch那要跑好幾分鐘才跑得完啊現在只要10秒鐘就可以跑得完了你可以看到這個時代的眼鏡那這個Fetch Size的大小跟時間的關係其實每年都會做這個實驗啦我特別把舊的投影片放在這邊啦如果你有興趣的話可以看一下可以看到什麼呢可以看到這個時代的眼鏡這樣嘿17年的時候用的是那個9802015年的時候用的是那個760然後980要跑好幾分鐘才跑得完啊你可以看到這個時代的眼鏡這樣嘿17年的時候用的是那個760然後980要跑好幾分鐘才跑得完啊你可以看到這個時代的眼鏡 所以GPU雖然有平行運算的能力但平行運算能力終究是有個極限所以你Batch Size真的很大的時候時間還是會增加的但是因為有平行運算的能力因此實際上當你的Batch Size小的時候你要跑完一個APOC花的時間是大的Batch Size是比大的Batch Size還要多的怎麼說呢如果今天假設我們的訓練資料是有6萬筆那Batch Size1那你要6萬個Update才能跑完一個APOC如果今天是 Batch Size等於1000,你要60個Update才能跑完一個EPA假設今天一個Batch Size等於一根Batch Size等於1000要算Gradient的時間根本差不多那60,000次Update跟60次Update比起來它的時間的差距量就非常可怕所以左邊這個圖是Update一次參數拿一個Batch出來計算一個GradientUpdate一次參數所需要的時間右邊這個圖是跑完一個完整的EPA需要花的時間你會發現左邊的圖跟右邊的圖它的趨勢正好是相同的 相反的,假設你Batch Sizer 1跑完一個EPA你要update6萬次參數,它的時間是非常可觀的但是假設你的Batch Sizer 1000你只要跑60次,update60次參數就會跑完一個EPA所以你跑完一個EPA,看完所有資料的時間如果你的Batch Sizer 1000其實是比較短的Batch Sizer 1000的時候,把所有資料看過一遍其實是比Batch Sizer 1還要更快所以如果我們看右邊這個圖的話看完一個Batch Sizer 1000,把所有資料看過一遍 經過一次這件事情,大的Batch Size反而是比較有效率的跟你直接想的不太一樣喔在沒有考慮平行運算的時候,你覺得大的Batch比較慢但實際上在有考慮平行運算的時候一個Epoc大Batch花的時間反而是比較少的所以這邊我們如果要比較Batch Size大小的差異的話看起來直接用技能時間冷卻的長短並不是一個精確的描述看起來在技能時間冷卻的長短 上面大的Batch並沒有比較吃虧甚至還佔到優勢了所以事實上呢這邊update一次的時間20筆資料update一次的時間跟這邊看1筆資料update一次的時間如果你用GPU的話其實可能根本就是一樣所以大的Batch他的技能時間他技能冷卻的時間並沒有比較長那所以這時候你可能就會說欸 那大的Batch的劣勢消失了那難道他真的就就那這樣看起來大的Batch應該比較好囉你不是說大的Batch這個update比較 穩定,小的Batch它的gradient的方向比較noisy嗎?那這樣看起來大的Batch好像應該比較好喔小的Batch應該比較差喔因為現在大的Batch的劣勢已經因為平行運算被拿掉了它好像只剩下優勢而已那神奇的地方是noisy的gradient反而可以幫助權力這個也是跟直覺正好相反的如果你今天拿不同的Batch來訓練你的模型你可能會得到這樣子的結果左邊是做在Enlist上 右邊是坐在Cypher 10上不管是Amnesia還是Cypher 10都是影像電視的問題橫軸代表的是Batch Size從左到右越來越大縱軸代表的是正確率越上面正確率越高當然正確率越高越好如果你今天看Validation Set上的結果會發現說Batch Size越大Validation Set上的結果越差但這個是 是over fitting嗎?這個不是over fitting因為如果你看你的training的話會發現說batch size越大training的結果也是越差的而我們現在用的是同一個模型同一個模型、同一個network照理說他們可以表示的function就是一模一樣的但神奇的事情是大的batch size往往在training的時候會給你帶來比較差的結果所以這個是怎麼樣的問題同樣的model 所以這個問題 這不是Model Bias的問題這個是Optimization的問題代表當你用大的Batch Size的時候你的Optimization可能會有問題小的Batch Size Optimization的結果反而是比較好的為什麼會這樣子呢為什麼小的Batch Size在Trending Set上會得到比較好的結果為什麼Noisy的UpdateNoisy的Gradient會在Trending的時候給我們比較好結果呢一個可能的解釋是這樣子的假設你是Full Batch那你今天再Update你的參數 你就是沿著一個Loss Function來Update參數進行Update參數的時候走到一個Local Minima走到一個Settled Point顯然就停下來了規定是0如果你不特別去看Hessian的話那你用Gradient Descent的方法你就沒有辦法再更新你的參數但是假如是Small Batch的話假如你有用Batch的話會發生什麼事呢因為我們每次是挑一個Batch出來算它的Loss所以等於是等於你每次Update你的參數的時候你用的Loss Function都是略有差異的你選到 第一個Batch的時候你是用L1來算你的歸電你選到第二個Batch的時候你是用L2來算你的歸電假設你用L1算歸電的時候發現歸電是0卡住了但L2它的function跟L1又不一樣所以L1卡住了L2不一定會卡住啊所以L1卡住了沒關係換下一個Batch來有L2再算歸電你還是有辦法train你的model你還是有辦法讓你的Loss變小所以今天這種noisy的update的方式結束 結果反而對training其實是有幫助的那這邊還有另外一個更神奇的事情那這個神奇的事情是什麼呢這個神奇的事情是其實小的batch也對testing有幫助假設我們今天在training的時候不管是大的batch還是小的batch都training到一樣好剛才case是training的時候就已經training不好了那假設你有一些方法你努力的調大的batch的learning rate然後想辦法把大的batch跟小的batchtraining到一樣好 实验的一样好,结果你会发现小的Batch居然在Testing的时候会是比较好的。以下这个实验结果是引用自Unlarged Batch Training for Deep Learning Generalization Gap and Sharp Minimum这篇Paper的实验结果。这篇Paper里面作者Trained了6个Network,里面有3N的,有Fully Connected V4Ware Network的,然后做在不同Codec上,来代表说这个实验是很泛用的,在很多不同的Case都观察到一样的结果。它有小的Batch,一个Batch的Codec。 裡面有256筆sample大的batch就是那個data set乘0.1啦data set乘0.1data set有6萬筆那就是一個batch裡面有6000筆資料然後他想辦法在大的batch跟小的batch都train到差不多的training的accuracy所以剛才我們看到的結果是batch size大的時候training accuracy就已經差掉了這邊不是想辦法train到大的batch的時候training accuracy跟小的batch其實是差不多的但是就算是 但是在training的時候結果差不多testing的時候你還是看到了小的batch居然比大的batch差training的時候都很好testing的時候小的batch差那代表什麼?代表overfitting這個才是overfitting對不對?好,那為什麼會有這樣子的現象呢?在這篇文章裡面也給出了一個解釋它的解釋是這個樣子的假設這個是我們的training loss在這個training loss上面可能 有很多個local minima有不只一個local minima那這些local minima他們的loss都很低他們loss可能都趨近於零但是這個local minima還是有好minima跟壞minima之分什麼叫做好minima跟壞minima呢我們會認為如果一個local minima它在一個峽谷裡面它是壞的minima然後它在一個平原上它是好的minima為什麼會有這樣的差異呢因為假設現在全球 training 跟 testing 中間有一個 mismatchtraining loss 跟 testing loss 他們那個 function 不一樣為什麼會不一樣呢有可能是本來你 training 跟 testing distribution 就不一樣也有可能是因為 training 跟 testing 你都是從 sample 的 data 算出來的也許 training 跟 testing 你 sample 到的 data 不一樣所以他們算出來的 loss 當然是有一點差距那我們就假設說這個 training 跟 testing 它的差距就是把 training loss 這個 function 往右比一這時候你會發現對這個 對左邊這個minima來說,對這個在一個盆地裡面的minima來說,它的在training跟testing上面的結果不會差太多,只差了一點點。但是對右邊這個在峽谷裡面的minima來說,一差就可以天差地遠。它在這個training set上算出來的loss很低,但是因為training跟testing之間的不一樣,所以testing的時候,這個error surface一變,它算出來的loss就變得大。而很多人相信說, 大的Batch Size會讓我們傾向於走到峽谷裡面而小的Batch Size傾向於讓我們走到盆地裡面那他直覺上的想法是這樣就是小的Batch它有很多的Noise它每次Update的方向都不太一樣所以如果今天這個峽谷非常的窄它可能一個不小心就跳出去了因為每次Update的方向都不太一樣嘛它的Update的方向就隨機性所以一個很小的峽谷沒有辦法困住小的Batch如果峽谷 它可能動一下就跳出來什麼時候停下來呢如果有一個非常寬的盆地它才會停下來那對於大的Batch Size反正它就是順著Gradient Update它就很有可能走到一個比較小的峽谷裡面但這只是一個解釋啦那也不是每個人都相信這個解釋那這個其實還是一個上代可以研究的問題好 那這邊就是比較了一下大的Batch跟小的Batch左邊這個是小的Batch 第一個column是小的batch,第二個column是大的batch如果在沒有平行運算的情況下我們會覺得小的batch比較有效大的batch比一個batch的時間要算得比較長但是在有平行運算的情況下小的batch跟大的batch其實運算的時間並沒有太大的差距除非你的大的batch那個大是真的非常大才會顯示出差距來但是一個epoch需要的時間小的batch比較長,大的batch反而是比較快的所以從一個epoch需要的時間來看大的batch 其實是占到優勢的而小的Batch你會Update的方向比較Noisy大的BatchUpdate的方向比較穩定但是Noisy的Update的方向反而在Optimization的時候會占到優勢而且在Testing的時候占到有他們擅長的地方所以Batch Size變成另外一個Hyperparameter這個Hyperparameter是需要去調它的這個也是一個需要去調整的Hyperparameter講到這邊有人就會想說 那我們能不能夠與雄掌兼得呢?我們能不能夠擷取大的Batch的優點跟小的Batch的優點我們用大的Batch Size來做訓練用平行運算的能力來增加訓練的效率但是訓練出來的結果同時又得到好的結果呢?又得到好的訓練結果呢?這是有可能的有很多文章都在探討這個問題那今天我們就不細講我們把一些Reference列在這邊給大家參考來發現這些Paper往往它想要做的事情就是什麼 76分鐘Trend Bird15分鐘Trend ResNet1分鐘Trend ImageNet等等為什麼他們可以做到那麼快就是因為他們的Batch Size是真的開很大比如說在第一篇paper裡面Batch Size裡面有3萬筆Example這樣Batch Size開很大Batch Size開大真的就可以算很快可以在很短的時間內看到大量的資料那他們需要有一些特別的方法來解決Batch Size可能會帶來的劣勢 好,那最後想要跟大家分享的另外一個技術是Momentum這也是另外一個有可能可以對抗Zero Point或Local Minima的技術這個Momentum是怎麼運作的呢?Momentum的運作是這個樣子的它的概念可以想像成在物理的世界裡面假設Aero Surface就是真正的斜坡而我們的參數是一個球你把球從斜坡上滾下來如果今天是Gradient Descent,走到Local Minima就得到一個 停住了走到saddle point就停住了在物理的世界裡面一個球會這樣子嗎一個球如果從高處滾下來從高處滾下來就算滾到saddle point如果有慣性他從左邊滾下來因為慣性的關係他還是會繼續往右走甚至他走到一個local minima如果今天他的動量夠大的話還是會繼續往右走甚至翻過這個小坡然後繼續往右走那所以今天在物理的世界裡面一個球從高處滾下來的時候他並不會被saddle point或local minima 不一定會被Sable Code 或 Logo Minima 卡住我們有沒有辦法運用這樣子的概念到 Gradient Descent 裡面呢這個就是我們等一下要講的 Momentum 這個技術那我們先很快的複習一下原來的 Gradient Descent 長的是什麼樣子這個是 Vanilla 的 Gradient DescentVanilla 的意思就是一般的意思它直譯是香草的,但其實就是一般的一般的 Gradient Descent 長什麼樣子呢一般的 Gradient Descent 是說我們有一個初始的參數叫做 Celery我們繼續 我們計算一下Gradient然後計算完這個Gradient以後呢我們往Gradient的反方向去Update參數我們到了新的參數以後再計算一次Gradient再往Gradient的反方向再Update一次參數到了新的位置以後再計算一次Gradient再往Gradient的反方向去Update參數這個process就一直這樣子下去那如果加上Momentum的話會是什麼樣子呢加上Momentum以後Gradient Descent變成這個樣子每一次我們在移動我們的參數的時候不甚失望 Gradient Descent 的反我們不是指往Gradient的反方向來移動參數在一般的Gradient Descent裡面我們都是往Gradient的反方向去移動參數但現在不指往Gradient的反方向去移動參數我們是Gradient的反方向加上前一步移動的方向兩者加起來的結果去調整去動我們的參數具體說起來是這個樣子一樣找一個初始的參數然後我們假設在一開始的時候前一步 前一步的變化量 前一步的參數的update量就設為0接下來在θ0的地方你要計算G0 計算G0 計算gradient的方向接下來你要決定下一步要怎麼走那我們說下一步要怎麼走呢它是gradient的方向加上前一步的方向不過因為前一步正好是0 現在是剛初始的時候所以前一步是0所以θ的方向跟原來的gradient descent是一樣的 這沒有什麼差別 有趣的地方但從第二步開始有加上 momentum 以後就不太一樣了從第二步開始我們計算 g1然後接下來我們 update 的方向不是 g1 的反方向而是根據上一次 update 的方向也就是 M1減掉 g1當作我們新的 update 的方向這邊寫成 M2如果你看數學式子覺得有點模糊的話那我們就看左邊這個圖g1 告訴我們gradient 告訴我們要往這邊走但是我們不是只聽 gradient 的話 加上momentum以後我們不是只根據gradient的反方向來調整我們的參數我們也會看前一次update的方向如果前一次說要往這個方向走gradient說要往這個方向走就把兩者相加起來走兩者的折衷也就是往這一個方向走所以我們就移動了M2走到θ2這個地方接下來就反覆進行同樣的過程在這個位置我們計算出gradient但我們不是只根據gradient反方向走 我們看前一步怎麼走的前一步走這個方向走這個藍色虛線的方向我們把藍色的虛線加紅色的虛線前一步指示的方向跟Gradient指示的方向當作我們下一步要移動的方向以此類推反覆進行之後的操作Gradient在這個地方說要往這個方向走然後前一步說要往這個方向走兩者綜合起來就往左下走所以這個一般Gradient Design不一樣我們不是只看Gradient的方向來調整參數我們還會考慮之前移動的方向來調整 那這邊的每一步的移動我們都用N來表示那這個N其實可以寫成之前所有算出來的Gradient的Weighted Sum怎麼說呢從右邊的這個式子其實就可以輕易的看出來N0我們把它設為0N1是N0減掉G0N0為0所以N1就是G0乘上負的AN2呢N2是λ乘上N1N1就是 就是另外一個參數啦就跟Learning Rate一樣是要調的就好像Eta是Learning Rate我們要調啊Language是另外一個參數這個也是需要調的啦M2等於Language乘上M1減掉Eta乘上G1然後M1在哪裡呢M1在這邊你把M1帶進來就知道說M2等於負的Language乘上Eta乘上G0減掉Eta乘上G1它是G0跟G1的規定上意思類推所以你會發現說這個加上Momentum以後啊一個解讀是Momentum是 是Gradient的負反方向加上前一次移動的方向那另外一個解讀方式是所謂的Momentum當加上Momentum的時候我們Update的方向不是只考慮現在的Gradient而是考慮過去所有Gradient的綜合好,那假設你這邊也沒有聽得很懂的話那這邊有一個更簡單的例子希望幫助你了解Momentum是怎麼回事好,那我們從這個地方開始Update參數根據Gradient的方向 告訴我們應該往右update參數那現在沒有前一次update的方向所以我們就完全按照規定呢給我們的指示往右移動參數好 那我們的參數呢就往右移動了一點到這個地方規定變得很小告訴我們往右移動但是只有往右移動一點點那前一步是往右移動的我們把前一步的方向用虛線來表示放在這個地方我們把之前規定告訴我們要走的方向跟前一步移動的方向加起來得到往右走的方向那再往右走走到一個local minimum照理說走到local minimum 不然Gradient Descent就無法向前走了因為已經沒有這個Gradient的方向那走到Settle Point也一樣沒有Gradient的方向已經無法向前走了但沒有關係如果有Momentum的話你還是有辦法繼續走下去因為Momentum不是只看GradientGradient就算是0你還有前一步的方向前一步的方向告訴我們向右走我們就繼續向右走甚至你走到這種地方Gradient告訴你應該要往左走了但是假設你前一步的影響力比Gradient要大的話 你還是有可能繼續往右走甚至翻過一個小丘搞不好就可以走到更好的Local Mini Market這個就是Momentum有可能帶來的好處好 那這個就是今天想要跟大家說的內容那沒想到就講到6點20了我們在這邊就只好下課了謝謝大家 謝謝謝謝 謝謝 好,那上週我們講到了Critical Point的問題那接下來我要告訴大家說Critical Point其實不一定是你在訓練一個Network的時候會遇到的最大的障礙那今天這份投影片裡面要告訴大家的是一個叫做Adaptive Learning Rate的技術也就是我們要給每一個參數不同的Learning Rate 好,那為什麼我說這個critical point不一定是我們訓練過程中最大的阻礙呢?這個往往同學們在訓練一個network的時候你會把他的loss記錄下來所以你會看到說你的loss原來很大那隨著你參數不斷的update橫軸代表參數update的次數隨著你參數不斷的update這個loss會越來越小最後就卡住了卡住的意思就是你的loss不再下降那多數時候這個時候大家就會猜說 是不是走到了critical point因為gradient等於0的關係所以我們沒有辦法再更新參數但是真的是這樣嗎當我們說走到critical point的時候意味著gradient非常的小但是你有確認過當你的loss不再下降的時候gradient真的很小嗎其實多數的同學可能都沒有確認過這件事而事實上在這個例子裡面在今天有show的這個例子裡面當我們的loss不再下降的時候 Gradient並沒有真的變得很小下面是Gradient的None也就是Gradient的這個向量Gradient是一個向量嘛Gradient這個向量的長度隨著參數更新的時候的變化那你會發現說雖然Loss不再下降但是這個Gradient的NoneGradient的大小並沒有真的變得很小事實上在最後訓練的最終結的時候Loss已經幾乎沒有在動了Loss幾乎沒有在減少了回點去 突然還上升了一下這是怎麼一回事呢那這樣子的結果其實也不難猜想也許你遇到的是這樣子的狀況這個是我們的Aero Surface然後你現在的Gradient呢在Aero Surface三股的兩個股幣間不斷的來回的震盪這個時候你的Loss不會再下降所以你會覺得看到這樣子的狀況你的Loss不會再越來越小但是實際上他真的卡到了Critical Point卡到了Settled Point卡到了 local minima嗎?不是的,他的規點仍然很大只是loss不見得在減小了所以你要注意啊當你今天你訓練一個network卻到後來發現loss不再下降的時候你不要隨便說啊我卡在local minima我卡在saddle point有時候根本兩個都不是你只是單純的loss沒有辦法再下降就是為什麼在作業2-2會有一個作業叫大家算一下規點的none然後算一下說你現在是卡在saddle point還是critical point 多數的時候,當你說你訓練卡住了,很少有人會去分析卡住的原因。為了強化你的印象,我們有一個作業,讓你來分析一下卡住的原因是什麼。那講到這邊,有的同學就會有一個問題了。如果我們在訓練的時候,其實很少卡到set of points 或者是local minima,那這一個圖是怎麼做出來的呢?你還記得這個圖嗎?我們上次有畫過這個圖是說,我們現在訓練一個內臥,訓練到現在參數在快10分。 critical point附近然後我們再來根據eigenvalue的正負號來判斷說這個critical point它比較像是saddle point是local minima那如果實際上在訓練的時候要走到saddle point或者是local minima是一件困難的事情那這個圖到底是怎麼畫出來的呢這邊告訴大家一個秘密啦這個圖呢你要訓練出這樣子的結果你要訓練到你的參數很接近critical point用一般的gradient descent其實是做不到的用一般的gradient descent 你往往會得到的結果是你在這個gradient還很大的時候你的loss就已經掉不下去了這個是需要特別方法training的所以做完這個實驗以後我更感覺你要走到一個critical point其實是困難的一件事多數時候training在還沒有走到critical point的時候就已經停止了但這並不代表說critical point不是一個問題我只想要告訴你說我們真正目前當你用gradient descent來做optimization的時候你真正的模糊