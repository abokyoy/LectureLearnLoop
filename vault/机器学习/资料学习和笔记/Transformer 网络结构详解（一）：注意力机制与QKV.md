[[ReadItLater]] [[Article]]

# [Transformer 网络结构详解（一）：注意力机制与QKV](https://mp.weixin.qq.com/s/gk6uj0nFPWMC-fzG7CFXBg)

在Transformer机制中，注意力机制到底是什么？QKV 三个向量又是怎么工作的？

我们先来看这句句子：

> “我昨天在图书馆读了一本很有趣的书。”

假设你是个语言模型，正处理最后一个词“书”。你怎么知道这个“书”是“有趣的”，还是“图书馆的”？它和句子中哪个词关系最密切？

这就需要模型具备：

> **注意力机制（Attention）——自动判断和谁关系最强，并重点关注它。**

## 一、人类视觉中的注意力机制

人类的注意力，主要由两种机制驱动：

### ① 非自主性提示：被动地被吸引

假设你面前有这些物品：

📰 报纸、📄 论文、📒 笔记本、📚 一本书，☕ 还有一个**红色的咖啡杯**

除了咖啡杯，其它全是黑白印刷。结果你自然会**第一眼就盯向咖啡杯**。

这就是一种“非自主”的视觉注意力行为：

> 颜色/形状/位置等显著特征 → 引发下意识的注意力聚焦

![](ReadItLater%20Inbox/assets/Transformer%20网络结构详解（一）：注意力机制与QKV-5liPXxc0Us.png)  

图 1：由于突出性的非自主性提示，注意力不自主地指向了咖啡杯（图片引自\[1\]）

### ② 自主性提示：目标驱动的注意

喝完咖啡后，你决定读书，于是把注意力从咖啡转向书本。这就是你**根据目标意图，主动调整注意力焦点**。

> 我现在有读书的任务 → 我的注意力就应该去“找书”

![](ReadItLater%20Inbox/assets/Transformer%20网络结构详解（一）：注意力机制与QKV-vMODlDjZ6E.png)  

图 2：目光从咖啡杯转向了书本（图片引自\[1\]）

### 回到神经网络中的注意力机制

Transformer 模型的注意力操作，其实就是在模拟这两种选择行为：

-   模型自动识别哪些词**在上下文中显著（非自主）**
    
-   同时结合当前任务目标，关注相关词（自主）
    

## 二、Q、K、V：工程实现的三大核心向量

为了实现“选择性关注”，Transformer 使用三组向量来操作：

|   符号   |   含义   |   类比于人类   |
| --- | --- | --- |
|   Q（Query）   |   查询向量   |   当前的注意目标   |
|   K（Key）   |   键向量   |   上下文中各位置的信息标签   |
|   V（Value）   |   值向量   |   上下文中各位置实际携带的内容   |

注意力机制的工作流程：

1.  当前词（Query）对所有词（Key）做相似度计算（谁最相关？）
    
2.  用 softmax 得到权重分布（非均匀注意）
    
3.  用权重加权所有 Value，得到新的表示（重点整合信息）
    

数学公式如下：
![[Pasted image 20250905093050.png]]
注意力机制的最终输出结果可以理解为“查询（Query）得到的值（Value）”。

再回到最初那句话

> “我昨天在图书馆读了一本很有趣的书。”

当模型处理“书”这个词时：

-   Q 是“书”的查询向量表示，**代表含义是“我想看谁”**
    
-   K/V 是整个句子中**所有词的 Key（键）/Value（值）向量表示**
    

通过计算注意力权重，模型会更关注“读”、“有趣”、“图书馆”这些词，从而理解“书”与它们的关系。

📌 **举一个更具体案例，来辅助理解QKV作用以及注意力机制**

假设存在一个数据表，其中**“键”为数据表中的人名，“值”为他们各自的年龄**，如下所示：

|   张三   |   张三-2   |   李四   |   张麻子   |
| --- | --- | --- | --- |
|   18   |   20   |   22   |   19   |

如果我需要做一次查询，获取姓张所有人的平均年龄，那么在这次查询（Query）中，注意力机制计算的是查询获得的值（平均年龄）

> 在这个案例中，注意力可以理解为对全局信息的一次查询（Query），获得相应的查询值（Value）

在计算机实际计算时，“键”通常会被建模为嵌入表示（向量），如上例中：

```
张三-->[1, 2, 0]; 李四-->[0, 0, 2]; 张麻子-->[1, 4, 0]
```

那么数据表变成了：

|   \[1, 2, 0\]   |   \[1, 2, 0\]   |   \[0, 0, 2\]   |   \[1, 4, 0\]   |
| --- | --- | --- | --- |
|   18   |   20   |   22   |   19   |

在上表中，查询键和值为：

```
K = [[1, 2, 0], [1, 2, 0], [0, 0, 2], [1, 4, 0]]  =>  dk = 3 = dqV = [[18], [20], [22], [19]]  =>  dv = 1
```

一次查询Query为查找表中张姓的平均年龄，那么查询向量可以看作为`q = [1, 0, 0]`（注：Attention中查询Q为多次查询构成的矩阵），那么查询值（张姓平均年龄）为：

其中**softmax** 是一种归一化函数，常用于将一组实数转换为概率分布。它会放大数值之间的差异，使较大的输入对应更高的权重，常用于注意力机制中对不同信息的重要性进行加权：

其输出总和为 1，便于作为“注意力权重”使用。

## 图像引用

\[1\] https://zh-v2.d2l.ai/chapter\_attention-mechanisms/attention-cues.html