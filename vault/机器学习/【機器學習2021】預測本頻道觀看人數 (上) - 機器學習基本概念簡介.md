# 笔记总结

- 機器學習的核心概念：讓機器具備尋找函數的能力。
- 機器學習的應用範例：語音辨識（輸入：聲音訊號；輸出：文字）、影像辨識（輸入：圖片；輸出：圖片內容）、自動下圍棋（輸入：棋盤佈局；輸出：下一步落子位置）。
- 機器學習的類型：Regression，指輸出為單一數值的機器學習任務，例如預測PM2.5數值（輸入：當前PM2.5、溫度、臭氧濃度等；輸出：未來PM2.5數值）。

- 機器學習的主要任務包括分類 (Classification)、回歸 (Regression) 和結構學習 (Structure Learning)。
- 分類問題是指機器從預設選項中選擇一個輸出，例如垃圾郵件偵測 (yes/no) 或 AlphaGo 的落子位置選擇。
- 回歸問題在文本中未明確解釋。
- 結構學習是指機器學習生成有結構的物件，例如圖畫或文章。
- 演講者以YouTube頻道訂閱人數為例，將進一步說明機器如何找到函式。

- 使用YouTube頻道每日觀看次數預測明日觀看次數，作為機器學習模型示例。
- 模型建立分三個步驟：
    - 建立帶有未知參數的函數(model)：例如，Y = B + W * X1，其中Y為預測值(明日觀看次數)，X1為已知值(前一日觀看次數)，B和W為未知參數。
    - 根據領域知識(Domain Knowledge)猜測函數形式，例如，假設明日觀看次數與前一日觀看次數相關。
    -  函數中的X1稱為特徵(feature)，W稱為權重(weight)，B和W是未知參數，需要透過資料訓練得出。

- 定义了“Bias”：与Feature相乘的数值。
- 定义了“Loss”：一个函数，其输入为模型参数（Y=D+W*X1）。
- 讲解了模型的两个步骤：第一步定义Bias，第二步定义Loss函数。

- L 是一个函数，其输入是模型参数 b 和 w。
- L 函数的输出值代表当前参数设置（b 和 w 的值）的好坏。
-  以预测视频点击次数为例，如果 b=0.5k, w=1，则预测函数为 y = 0.5k + x1。L 函数计算这个函数的好坏程度。
- L 函数的计算需要使用训练数据，例如2017年到2020年每天的视频点击次数。

- 使用函數Y = 0.5K + 1*X1 (其中X1為前一天點閱數，Y為預測點閱數) 預測點閱數。
- 以2017年1月1日至2020年12月31日的數據進行預測，並與實際點閱數(Label)比較，計算誤差。
- 誤差計算方法：使用絕對值差(MAE)，即|Y - Y实际值|。 也提及了均方误差(MSE)作为另一种误差计算方法。
- 將三年每日的誤差(E)相加取平均，得到總誤差(Loss, L)。 L值越小，表示參數W和B越好。
- 通過調整參數W和B，計算不同參數組合下的Loss，並繪製等高線圖，圖中顏色越偏藍，Loss越小，參數組合越好。

- 使用前一天的瀏覽次數預測隔天的瀏覽次數，通常預測值與實際值相近。
- 機器學習步驟二：建立等高線圖(ERROR SERVICE)以顯示不同參數下的模式。
- 機器學習步驟三：尋找最佳參數 W 和 B，使損失函數 (loss) 最小化，即找到 W* 和 B*。
- 使用梯度下降法 (gradient descent) 尋找最佳參數。
- 梯度下降法簡化說明：先假設只有一個參數 w，通過計算 w 對 loss 的微分 (或觀察曲線斜率)，判斷 w 應該增加還是減少以降低 loss。
- 微分值代表曲線斜率，正斜率表示 w 應減少，負斜率表示 w 應增加。
- 步伐大小取決於斜率和學習率 (learning rate, eta)。學習率是超參數 (hyperparameter)，需要自行設定。

- 梯度下降法 (Gradient Descent) 中，需要自行設定超參數 (Hyperparameter)，例如參數更新次數上限。
- 梯度下降法目標是找到使損失函數 (Loss) 最小的參數值。
- 停止更新參數的條件：達到設定的迭代次數上限，或微分值為 0。
- 梯度下降法可能陷入局部最小值 (Local Minima)，而非全局最小值 (Global Minima)。
- 然而，局部最小值的問題在深度學習實踐中通常被過度強調。
-  文中以單參數和雙參數 (w, b) 的例子說明梯度下降法的運作過程，其原理相同。  
- 更新參數的公式：新的參數值 = 原參數值 - learning rate * 微分值。

- 机器学习过程包含三个步骤：1. 建立包含未知数的函数；2. 定义损失函数；3. 使用优化方法找到使损失函数最小的一组参数（W, B）。
- 使用2017-2020年的数据训练模型，得到最佳参数W=0.97，B=0.1K，损失值为0.48K（约500人次）。
-  训练集上的误差较小，但在2021年（未见数据）的预测中，平均误差为0.58K（约580-600人次）。
-  模型预测结果图显示了2021年1月1日至2月14日每天的观众人次预测值与实际值之间的差异。

- 使用前一天觀看次數預測隔天觀看次數的模型準確率低。
- 真實數據顯示觀看次數有週期性，週五和週六觀看次數特別低。
- 建立一個新的模型，考慮前七天觀看次數預測隔天觀看次數，訓練資料Loss為0.38K，測試資料Loss為0.49K。
- 新模型中，前一天的權重(W1)最高(0.79)，部分日期的權重為負值。
- 將考慮天數擴展到28天(一個月)，訓練資料Loss為0.33K，測試資料Loss為0.46K。
- 將考慮天數擴展到56天，訓練資料Loss略微改善(0.32K)，測試資料Loss保持不變(0.46K)。
- 以上模型均為線性模型(Linear Model)。

---

# 原始语音转文字

由 Amara.org 社群提供的字幕 好 那我們就開始上課吧那第一堂課啊是要簡單跟大家介紹一下Machine Learning還有 Deep Learning 的基本概念那等一下呢會講一個跟寶可夢完全沒有關係的故事告訴你機器學習還有深度學習的基本概念好 那什麼是機器學習呢那我想必大家在報章雜誌上其實往往都已經聽過機器學習這個詞彙 知道說機器學習就是跟今天很熱門的AI好像有那麼一點關聯那所謂的機器學習到底是什麼呢顧名思義好像是說機器它具備有學習的能力那些科普文章往往把機器學習這個東西吹得玄之又玄好像機器會學習以後我們就有了人工智慧有人工智慧以後機器接下來就要統治人類了好那機器學習 到底是什麼呢事實上機器學習概括來說可以用一句話來描述機器學習這件事什麼叫機器學習呢機器學習就是讓機器具備找一個函式的能力那機器具備找函式的能力以後它可以做什麼樣的事情呢它確實可以做很多事舉例來說假設你今天想要叫機器做語音辨識機器聽一段聲音產生這段聲音對應的文字 那你需要的就是一個函式這個函式的輸入是聲音訊號輸出是這段聲音訊號的內容那你可以想像說這個可以把聲音訊號當作輸入文字當作輸出的函式顯然非常非常的複雜它絕對不是你只可以用人手寫出來的方程式這個函式它非常非常的複雜人類絕對沒有能力把它寫出來所以我們期待憑藉著機器的力量 韓式自動找出來這件事情就是機器學習那剛才舉的例子是語音辨識還有好多好多的任務我們都需要找一個很複雜的韓式舉例來說假設我們現在要做影像辨識那這個影像辨識我們需要什麼樣的韓式呢這個韓式的輸入是一張圖片它的輸出是什麼呢它是這個圖片裡面有什麼樣的內容或者是大家都知道的 其實也可以看作是一個函式要讓機器下圍棋我們需要的就是一個函式這個函式的輸入是棋盤上黑子跟白子的位置輸出是什麼輸出是機器下一步應該落子的位置假設你可以找到一個函式這個函式的輸入就是棋盤上黑子跟白子的位置輸出就是下一步應該落子的位置那我們就可以讓機器做自動下圍棋這件事就更容易了 就可以做一個AlphaGo那隨著我們要找的函式不同機器學習有不同的類別那這邊介紹幾個專有名詞給大家認識一下第一個專有名詞叫做RegressionRegression的意思是說假設我們今天要找的函式它的輸出是一個數值它的輸出是一個scalar那這樣子的機器學習的任務我們稱之為Regression那這邊 先舉一個regression的例子假設我們今天要機器做的事情是預測未來某一個時間的PM2.5的數值你要叫機器做的事情是找一個函式這個我們用F來表示這個函式的輸出是明天中午的PM2.5的數值它的輸入可能是種種跟預測PM2.5有關的指數包括今天的PM2.5的數值今天的平均溫度今天平均的臭氧濃度等等這個函式可以拿這些數值當作輸出 輸出明天中午的PM2.5的數值那這個找這個函式的任務叫做regression那還有別的任務嗎還有別的任務除了regression以外另外一個大家耳熟能詳的任務呢叫做classification那classification這個任務要繼續做的是選擇題我們人類先準備好一些選項那這些選項呢又叫做類別又叫做 我們現在要找的函式它的輸出就是從我們設定好的選項裡面選擇一個當作輸出這個問題這個任務就叫做Lassocation舉例來說現在每個人都有Gmail Account那Gmail Account裡面呢有一個函式這個函式可以幫我們偵測一封郵件是不是垃圾郵件這個函式的輸入是一封電子郵件那它的輸出是什麼呢你要先準備好 你要機器選擇選項在偵測垃圾郵件這個問題裡面可能的選項就是兩個是垃圾郵件或不是垃圾郵件yes或者是no那機器要從yes跟no裡面選一個選項出來這個問題叫做classification那classification不一定只有兩個選項也可以有多個選項舉例來說AlphaGo本身也是一個classification的問題只是這個classification它的選項 是比較多的那如果要叫機器下圍棋你想追著alpha go的話我們要給機器多少個選項呢你就想想看棋盤上有多少個位置那我們知道棋盤上有19x19個位置那叫機器下圍棋這個問題其實就是一個有19x19個選項的選擇題你要叫機器做的就是找一個函式這個函式的輸入是棋盤上A子跟白子的位置輸出就是從19x19個選項裡面 選出一個正確的選項從19x19個可以落子的位置裡面選出下一步應該要落子的位置這個問題也是一個分類的問題那其實很多教科書在講機器學習的種種不同類型的任務的時候往往就講到這邊告訴你說機器學習兩大類任務一個叫做 Regression一個叫做 Classification然後就結束了 機器學習的認知只停留在機器學習就是兩大類任務regression跟transportation那就好像你以為說這個世界只有五大洲一樣你知道這個世界不是只有五大洲對不對這個世界不是外面是有一個黑暗大陸的這鬼滅之刃連載之前我們就已經出發前往黑暗大陸了鬼滅之刃連載以後我們居然都還沒有到可見這個黑暗大陸距離我們遠那在機器學習那個領域裡面所謂的黑暗大陸是什麼呢 在regression跟translocation以外大家往往害怕碰觸的問題叫做structure learning也就是機器今天不只是要做選擇題不只是輸出一個數字還要產生一個有結構的物件舉例來說機器畫一張圖寫一篇文章這怎麼叫機器產生有結構的東西的這個問題啊就叫做structure learning那如果要講的比較擬人話比較潮一點structure learning你可以 用擬人化的講法說我們就是要叫機器學會創造這件事情好 那到目前為止我們就是講了三個機器學習的任務Regression, Classification, Structure Learning接下來我們要講的是我們說機器學習就是要找一個函式 Agents gives you the orchestration layer to allow LLMs to do work in your businessas well as a platform to manage all your agentsCreate custom tools, allow LLMs to execute themand track agent performance all in our secure enterprise-grade platform那機器怎麼找一個函式呢那這邊要用一個例子跟大家說明說機器怎麼找一個函式這邊的例子是什麼呢這邊的例子在講這個例子之前建綸大家 那說一下說這門課有一個YouTube的頻道然後我會把上課的錄影放到這個YouTube的頻道上面那這個頻道呢感謝過去修過這門課的同學不嫌棄其實也蠻多人訂閱所以我算是一個三流的YouTuber是沒有什麼太多流量但是也是有這邊也說7萬多訂閱了那為什麼突然提到這個YouTube的頻道呢因為我們等一下要舉的例子啊 跟YouTuber是有關係的那你知道身為一個YouTuberYouTuber在意的東西是什麼呢YouTuber在意的就是這個頻道的流量對不對假設有一個YouTuber是靠著YouTuber維生的他會在意說頻道有沒有流量這樣他才會知道他可以獲利多少所以我在想說我們有沒有可能找一個函式這個函式他的輸入是YouTuber後台的資訊輸出是這個頻道隔天的 明天的總點閱率總共有多少假設你自己有YouTube頻道的話你會知道說在YouTube後台你可以看到很多相關的資訊比如說每一天按讚的人數有多少每一天訂閱的人數有多少每一天觀看的次數有多少我們能不能夠根據一個頻道過往所有的資訊去預測他明天有可能觀看的次數是多少呢我們能不能夠找一個函式函式的輸入是 YouTube上面 YouTube後台是我的資訊輸出就是某一天隔天這個頻道會有的總觀看的次數呢那你可能會問說為什麼要做這個嗯如果我有盈利的話我可以知道我未來可以賺到多少錢但我其實沒有開盈利所以我也不知道為什麼東西要做這個就是完全沒有任何軟用我單純就是想舉一個例子而已好那接下來啊我們就要問怎麼找出這個韓式呢怎麼找這個韓式FB 輸入是 YouTube 後排的資料輸出是這個頻道隔天的點閱的總人數呢那機器學習找這個函式的過程分成三個步驟那我們就用 YouTube 頻道點閱人數預測這件事情來跟大家說明這三個步驟是怎麼運作的第一個步驟是我們要寫出一個帶有未知參數的函式簡單來說就是我們先猜測一下我們打算 我們打算找的這個函是F它的數學式到底長什麼樣子舉例來說我們這邊先做一個最初步的猜測這個F長什麼樣子呢這個數目跟Y之間有什麼樣的關係呢我們寫成這個樣子Y等於B加W乘以X1這邊的每一個數值是什麼呢這個Y啊是就假設是今天吧這個Y因為今天還沒有過完所以我們還不知道今天總共的點閱次數是多少這件事情 是我們未知的Y 是我們準備要預測的東西我們準備要預測的是今天2月26號這個頻道總共觀看的人數那X1是什麼呢X1是這個頻道前一天總共觀看的人數Y跟X跟X1都是組織都是我們這個Y呢是我們準備要預測的東西而X1是我們已經知道的資訊那B跟W是什麼呢B跟W是 未知的参数它是准备要透过资料去找出来的我们还不知道W跟B应该是多少我们只是隐约的猜测说那这个猜测为什么会有这个猜测呢这个猜测往往就来自于对这个问题的纸上的了解也就是Domain Knowledge所以常常会听到有人说啊这个做机器学习啊你就需要一些Domain Knowledge这个Domain Knowledge通常是用在哪里呢 就是用在你寫這個代表未知數的函數的時候所以我們怎麼知道說這個能夠預測未來點閱次數的函數是 F它就一定是前一天的點閱次數乘上 W 再加上 B 呢我們其實不知道 這是一個猜測也許我們覺得說今天的點閱次數總是會跟昨天的點閱次數有點關聯吧所以我們把昨天的點閱次數乘上一個數值但是總是不會一模一樣 加上一個 b 做修正當作是對於2月26號點閱次數的預測這是一個猜測它不一定是對的我們等一下回頭會再來修正這個猜測好那現在總之我們就隨便猜說 y 等於 b 加 w 乘以 x1而 b 跟 w 是未知的這個帶有未知的參數這個 parameter 中文通常翻成參數了這個帶有 unknown parameter 的這個 function我們就叫做 model 常常聽到有人說模型 model 這個東西model 這個東西在機器學習裡面就是一個太有未知的 parameter 的 function那這個 S1 是這個 function 裡面我們已經知道的東西它是來自於 YouTube 的後台資訊我們已經知道2月25號點閱的總人數是多少這個東西叫做 feature而 W 跟 B 是我們不知道的它是 unknown 的 parameter那這邊我們也給 W 跟 B 給它一個名字這個跟 feature 做相乘的未知的參數這個 W 我們叫它 weight 跟 Feature 相乘的 是直接加上去的這個我們叫它 Bias那這個只是一些名詞的定義而已讓等一下我們講課的時候在稱呼模型裡面的每一個東西的時候可以更為方便好 那這個是第一個步驟那第二個步驟是什麼呢第二個步驟呢是我們要定義一個東西叫做 Loss什麼是 Loss 呢Loss 啊 它也是一個 Function那這個 Function 它的輸入是我們 Model 裡面的參數我剛才已經把我們的 Model 寫出來了對不對 叫做Y等於D加W乘以X1 而 b 跟 w 是未知的是我們準備要找出來那所謂的 L所謂的這個 loss它是一個 function這個 function 的輸入是什麼這個 function 的輸入就是 b 跟 w所以 L 它是一個 function它的輸入是 parameter是 model 裡面的 parameter但是這個 loss這個 function 的輸出的值代表什麼呢這個 function 輸出的值代表說現在如果我們把這一組未知的參數設定某一個數值的時候這個數值好還是不好那這樣講可能你覺得有點抽象 具體的例子假設現在我們給未知的參數的設定是B這個 Bias 等於 0.5k這個 W 呢直接等於 1那這個 Loss 怎麼計算如果我們 B 設 0.5k這個 W 設 1那我們拿來預測未來的這個點閱次數的函式啊就變成 y 等於 0.5k加一倍的 x1那這樣子的一個函式這個 0.5k 跟 1他們所代表的這個函式他有多好呢這個東西就是 Loss 在這個問題裡面我們要怎麼計算這個Loss呢這個我們就要從訓練資料來進行計算在這個問題裡面我們的訓練資料是什麼呢我們的訓練資料是這個頻道過去的點閱次數舉例來說從2017年到2020年的點閱次數每天的這個頻道點閱次數都知道嘛這邊是假的數字啦隨便亂編的好那所以我們知道2017年1月1號到2020年12月31號的點閱數字是多少好接下來我們就可以計算Loss 我們把2017年1月1號的點閱次數帶入這一個函式裡面我們已經說我們想要知道D設定為0.5K W設定為1的時候這個函式有多棒當B設定為0.5K W設定為1的時候我們拿來預測的這個函數是Y等於0.5K加1倍的X1那我們就把這個X1帶4.8K看看預測出來的結果是多少所以根據這個函式根據D設0.5K W設1的這個函式如果1月1號是4.8K的點閱次數的話隔天應該是4.8K 一加0.5K也就是5.3K的點閱次數隔天實際上的點閱次數1月2號的點閱次數我們知道嗎從後台的資訊裡面我們是知道的所以我們可以比對一下現在這個函式預估的結果跟真正的結果它的差距有多大這個函式預估的結果是5.3K真正的結果是多少呢真正的結果是4.9K是高估了高估了這個頻道可能的點閱的人數就可以計算一下這個差距計算一下公測的值跟真實的值的差距這邊公測的值用Y來表示真實的值用YM來表示 你可以計算Y跟Ys之間的差距得到一個E1代表膚色的值跟真實的值之間的差距那計算差距其實有不只一種方式我們這邊把Y跟Ys相減直接取絕對值賺出來的值是0.4K好的我們今天有的資料不是只有1月1號跟1月2號的資料我們有2017年1月1號到2020年12月31號總共三年的資料好的這個真實的值啊叫做Label所以常常聽到有人說做機器學習就需要LabelLabel就是正確的數值這個東西叫做Label 做我們的Label那我們不是只能夠看用1月1號來預測1月2號的詞我們可以用1月2號的詞來預測1月3號的詞如果我們現在的函數是Y等於0.5K加1倍的X1那1月2號根據1月2號的點閱次數預測的1月3號的點閱次數只是多少呢是5.4K你X1大概是4.9K進去乘以1倍加0.5K等於5.4K接下來計算這個5.4K跟真正的答案跟Label之間的差距Label是7.5K再來是一個低估低估了這個頻道在1月3號的時候的點閱次數就可以算出EQ 這個EQ是Y減跟Y和Y hat之間的差距算出來是2.0K那同樣的方法你就可以算過這三年來每一天的預設的誤差假設我們今天的 function是Y等於0.5K加1倍的XY這是三年來每一天的誤差通通都可以算出來每一天的誤差都可以給我們一個小1好那接下來我們就把每一天的誤差通通加起來加起來然後取一個平均這個大根代表我們的訓練資料的次數的個數那我們訓練資料的個數就是三年來的訓練資料所以就365乘以3了 每年365.3年所以365.3好那我們算出一個L我們算出一個大L這個大L是每一筆訓練資料的誤差這個E啊相加以後的結果這個大L就是我們的Loss這個大L越大代表說我們現在這一組參數越不好這個大L越小代表我們現在這一組參數越好那這個E啊就是計算這個估測的時跟實際的時間的差距其實有不同的計算方法在我們剛才的例子裡面我們是算Y跟Yi結絕對值的差距這種計算差距的方法得到的這種大L啊得到的Loss 叫做 mean absolute error 所寫是 mae那在作業1裡面我們是算 y 跟 y hat 相減以後的平方如果你今天的1是用相減以後的平方算出來的這個叫 mean square error 叫 mse那 mse 跟 mae 他們其實有非常微妙的差別通常你要選擇用哪一種方法來衡量距離那是看你的需求 看你對這個任務的理解在這邊我們就不往下講反正我們就是選擇了 mae作為我們計算這個誤差的方式把所有的誤差加起來就得到 loss那你要選擇 mse 也是可以的 在作業裡面我們會用NAC那有一些任務有如果Y跟Y hat它都是機率分布的話在這個時候你可能會選擇Course Entropy這個我們都之後再說反正我們這邊就是選擇了NAD好的 這個是機器學習的第二步那我剛才舉的那些數字不是真正的例子但是在這門課裡面我在講課的時候就是要舉真正的例子給你看所以以下的數字是真實的例子是這個頻道真實的後台的數據所計算出來的結果 那我們可以調整不同的 W我們可以調整不同的 B窮取各種 W 窮取各種 B我們組合起來以後我們可以為不同的 W 跟 B 的組合都去計算它的 Loss然後就可以畫出以下這一個等高線圖在這個等高線圖上面越偏紅色系代表計算出來的 Loss 越大就代表說這一組 W 跟 B 越差如果越偏藍色系就代表 Loss 越小就代表這一組 W 跟 B 越好拿這一組 W 跟 B 換到 Function 裡面 放到我們的model裡面那我們的預測會越精準所以就可以所以就知道說假設W帶負0.25這個B帶負500就代表說這個W帶負0.25B帶負500就代表說這個頻道每天看的人越來越少而且Loss很大只能真實的狀況不太好如果W帶0.75B帶500那這個正確率會這個估測會比較精準那估測最精準的地方看起來應該是在這裡如果你今天W帶一個很接近1的值B帶一個小小的值比如說100多 這個時候估測是最精準的那這跟大家預期可能是比較接近的就是你拿前一天的點閱的總次數去預測隔天的點閱的總次數那可能前一天跟隔天的點閱的總次數其實差不多的所以大家估測1然後1呢設一個小一點的數值也許你的估測就會蠻精準的那像這樣子的一個等高線圖啊就是你試了不同的參數然後計算它的MODE畫出來的這個等高線圖啊叫做ERROR的SERVICE好那這種是機器學習的第二步好那接下來我們進入機器學習 第三步要做的事情其實是解一個最佳化的問題那如果你知道最佳化的問題是什麼的話也沒有關係我們今天要做的事情就是找一個 W 跟 B把未知的參數找一個數值出來看帶哪一個數值進去可以讓我們的 L 讓我們的 loss 值最小那個就是我們要找 W 跟 B那這個可以讓 loss 最小的 W 跟 B我們就叫做 W star 跟 B star代表說他們是最好的因素 W 跟 B可以讓 loss 值最小好那這個東西要怎麼做呢在這門課裡面 唯一会用到的 optimization 的方法叫做 gradient descent那 gradient descent 这个方法怎么做呢它是这样做的为了要简化曲线我们先假设我们位置的参数只有一个就是 w我们先假设没有 b 那个位置的参数只有 w 这个位置的参数那当我们 w 在不同的数值的时候我们就会得到不同的 loss那这条曲线就是 error surface只是刚才在前一个例子里面我们看到的 error surface 是二维的 二低的那这边只有一个参数所以我们看到的这个 error surface 是一低的好 那怎么样子 找一個W去讓這個Loss的值最小呢那首先呢你要隨機選取一個初始的點那這個初始的點我們叫做W0那這個初始的點往往真的就是隨機的就是隨便選一個真的都是隨機的那在往後的課程裡面我們其實會看到也許有些方法可以給我們一個比較好的W0的值那我們先不講這件事是我們先當作就是隨機的隨便指個骰子隨機決定說W0的值應該是多少那假設我們隨機決定的結果是在這個地方那接下來啊你這樣計算說這個 在 w 等於 860 的時候a w 這個參數對 loss 的微分是多少假設你知道微分是什麼你知道微分是什麼這對你來說不是個問題就計算 a w 對 loss 的微分是多少如果你不知道微分是什麼的話那沒有關係反正我們做的事情就是計算在這一個點在 a w 這個位置的這個 parallel surface 的斜線斜率也就是這一條藍色的虛線它的斜率但如果這一條虛線的斜率是負的那代表什麼意思呢代表說左邊比較高右邊比較低 這個位置附近左邊比較高右邊比較低那如果左邊比較高右邊比較低的話那我們要做什麼樣的事情呢如果左邊比較高右邊比較低的話那我們就把 w 的值變大那我們就可以讓 loss 變小如果算出來的斜率是正的就代表說左邊比較低右邊比較高是這個樣子左邊比較低右邊比較高如果左邊比較低右邊比較高的話那就代表左代表我們把 w 變小把 w 往左邊移我們可以讓 loss 的值變小那這個時候你就應該把 w 的值變小那假設你連斜率是什麼的話是什麼都不知道的話沒有關係你就想像說有一個人 站在這個地方然後他左右環視一下那這個算微分這件事就是左右環視他會知道說左邊比較高還是右邊比較高看哪邊比較低他就往比較低的地方跨出一步那這一步要跨多大呢這一步的步伐的大小取決於兩件事情第一件事情是這個地方的斜率有多大這個地方斜率大這個步伐就跨大一點斜率小步伐就跨小一點另外除了斜率以外就是除了這個微分這一項微分這一項我剛才說他就代表斜率除了微分這一項以外還有另外一個東西 影响步伐的大小这个东西我们这边用eta来表示这个eta叫做learning rate叫做学习数这个learning rate它是怎么来的呢它是你自己设定的你自己决定这个eta的大小如果eta设大一点那你每次参数update就会量很大可能学习可能就比较快如果eta设小一点那参数的update就很慢每次都会只会改变一点点参数的数值那这种你在做机器学习需要自己设定的东西叫做hyperparameter这个我们刚才讲说机器学习的第一步 就是定一個有未知參數的 function而這些參數這些未知的參數是機器自己找出來的但是有...欸你請說好你請說好這其實是一個好的問題我複述一下這個問題有同學問說為什麼 loss 可以是負的呢為什麼 loss 可以是負的呢loss 這個函數是你自己定義的所以在剛才我們的定義裡面我們說 loss 就是估測的值跟正確的值它的絕對值那如果根據剛才 loss 的定義那它不可能是負但是 loss 這個函數是你自己定義的 你可以說我今天要決定一個Loss Function就是絕對值在減一百那你可能就服了所以我這邊這個curve我這邊可能剛才忘了跟大家說明說這個curve並不是一個真實的Loss它是我隨便亂舉的一個例子因為在我今天想要舉一個比較General的case它並不是一個真實任務的Error Surface所以這個Loss的這個curve這個Error Surface它可以是任何形狀我們這邊沒有預測的立場說它一定要是什麼形狀但是確實在真實在剛才這個如果Loss的定義就跟我們剛才定的一樣是絕對值那它就不可能是不可能的 但是NO這個方向是你自己決定的所以它有可能失敗好 既然有同學問問題我們就在這邊停一下看大家有沒有問題想問好 然後住校以後會幫我看那個YouTube的直播來 有人在直播上問問題嗎?如果有的話,請幫我唸一下 by bwd6 你先看好以後再唸給我聽我們就先繼續講我們等一下講到一個段落再繼續回答大家的問題再問一下現場的同學有沒有想到問題呢沒有的話就請容我繼續講那剛才講到哪裡呢剛才講到Hyperparameter這個東西Hyperparameter是你自己設的所以在機器學習的這整個過程中你需要自己設定的這個東西就叫做Hyperparameter那我們說我們要把W0往右移一步這個新的位置就叫做W1這一步的步伐是add乘上每分的節度那如果你沒有這個步伐的話 你要數學式來表示它的話就是把 w0 減掉 eta乘上微分的結果得到 w1那接下來你就是反覆進行剛才的操作你就計算一下 w1這個微分的結果然後呢再決定現在要把 w1 移動多少然後再移動到 w2然後你再繼續反覆做同樣的操作不斷的把 w 移動位置最後你會停下來什麼時候會停下來呢往往有兩種狀況第一種狀況是你失去耐心了你一開始會設定說我今天在調整我的參數的時候我在計算我的微分的時候 我最多計算幾次你可能會設說嗯我的上限就是設定100萬次所以我的參數更新100萬次以後我就不再更新了那至於要更新幾次這個也是一個hyperparameter這個是你自己去如果說大概就是明天那你可能更新的次數就設少一點不要給它下放更新的次數就設多一點那還有另外一種理想上的評價的可能是今天當我們不斷調整參數調整到一個地方它的微分的值就是這一項算出來正好是0的時候如果這一項正好算出來是00乘上這個Learning Rate App還是0所以你的參數就不會上升 移動的位置好 那假設我們是一個理想的狀況我們把W0更新到W1再更新到W2最終更新到WT電腦有點卡更新到WT 卡住了也就是算出來這個微分的值是0了那就不會在參數的位置就不會再更新了那講到這邊你可能會馬上發現說Gradient Descent這個方法哇 有一個巨大的問題這個巨大的問題在這個例子裡面非常容易被看出來就是我們沒有找到真正最好的解決我們沒有找到那個可以讓Loss最小的那個比例 在這個例子裡面把w設定在這個地方你可以讓loss最小但是如果gradient descent是從這個地方當作隨機初始的位置的話你很有可能走到這裡你的序列就停住了你就沒有辦法再移動w的位置那這個位置這個真的可以讓這個loss最小的地方叫做global的minima而這個地方叫做local的minima它的左右兩邊都比這個地方的這個loss還要高一點但是它不是整個error free surface上面的最低點這個東西叫做local minima 你可能會聽到有人講到 gradient descent就會說 gradient descent 不是個好方法這個方法會有 local minima 的問題你沒有辦法真的找到 local minima但這一期教科書常常這樣講農場文常常這樣講但這個其實只是幻覺而已事實上假設你有做過深度學習相關的事情假設你有自己訓練內部自己做 gradient descent 的經驗的話其實 local minima 是一個假議題我們在做 gradient descent 的時候我們真正面對的難題不是 local minima到底是什麼 這個我們之後會再講到在這邊你就先接受先相信多數人的講法說 gradient descent 有 local minima 的問題在這個圖上 在這個例子裡面顯然有 local minima 的問題但之後會再告訴你說gradient descent 真正的透別到底是什麼好 那剛才舉的是只有一個參數的例子而已那我們實際上我們剛才模型有兩個參數有 w 跟 b那有兩個參數的情況下怎麼用 gradient descent 呢其實跟剛才一個參數沒有什麼不同如果一個參數你沒有問題的話你可以很快的推廣到兩個參數好 那謝謝大家 現在有兩個參數那我們給他兩個參數都給他隨機的出12隻就是w0跟b0然後接下來呢你要計算w對nose的微分你要計算b對nose的微分計算在哪計算是在w等於w0的位置b等於b0的位置在w等於w0的位置b等於b0的位置你要計算w對l的微分計算b對l的微分計算完以後就根據我們剛才一個參數的時候的做法去更新w跟b把w0減掉learning rate乘上微分的結果得到w1把b0減掉learning rate乘上微分的結果得到w1 到 B1那有同學可能會問說這個微分這個要怎麼算啊在這方面我們不會算微分的話不用緊張怎麼不用緊張呢在 Deep Learning 的 Framework 裡面或在我們作為議會用的 PyTorch 裡面這個算微分啊都是程式自動幫你算的你就Call 一行你就寫一行程式自動就把微分的值就算出來了你就算完全不知道自己在幹嘛你還是可以把微分的值算出來所以這邊如果你根本就不知道微分是什麼不用擔心這一步驟就是一行程式 這個等一下之後在作業日的時候大家可以自己體驗看看好 那就是反覆同樣的步驟就不斷的更新W跟B然後期待最後你可以找到一個最好的WW star 跟最好B B star好 那這邊呢就是舉一下例子跟大家看一下說如果在這個問題上它操作起來是什麼樣子那假設你隨便選一個初始的值在這個地方那就先計算一下這個W對L的微分計算一下B對L的微分然後接下來你要更新W跟B更新的方向就是W對L的微分乘以Eta再乘以一個負號 然後我的微分再乘以A 乘以一個負號你算出這個微分的值你就可以決定更新的方向你就可以決定W要怎麼更新 B要怎麼更新那把W跟B更新的方向結合起來它就是一個向量就是這個紅色的箭頭我們就從這個位置移到這個位置然後再計算一次微分然後你再決定要走什麼樣的方向把這個微分的值乘上Running Rate再乘上負號你就知道紅色的箭頭要指向哪裡你就知道怎麼動W跟B的位置一直移動 一直移動 一直移動 一直移動期待最後可以找出一組不錯的W跟B好 那實際上呢 真的用歸根比賽進行一番計算以後這個是真正的數據我們算出來的最好的W是0.97最好的B是0.1K跟我們猜測蠻接近的因為XY的值可能跟Y很接近所以這個W就設一個接近1的值B就設一個比較偏小的值那Loss多大呢Loss算一下是0.48K也就是在2017到2020年的資料上如果使用這個還是B帶0.1KR帶0.97那平均的誤差是0.4 也就是它的預測觀看的錯誤差大概是500人次左右好那講到目前為止我們就講了機器學習的三個步驟第一個步驟寫出一個函式這個函式裡面是有未知數的第二個步驟第一個叫做Loss Function第三個步驟JX Optimization Problem找到一組A與B讓Loss最小那A與B是你剛才找出來的那這組A與B可以讓Loss小到0.48K但是這樣是一個讓人滿意後 值得稱道的結果嗎也許不是為什麼因為這三個步驟合起來叫做訓練我們現在是在我們已經知道答案的資料上去計算囉2017到2020年的資料我們已經知道啦我們其實已經知道2017到2020年每天的觀看次數所以我們現在其實只是在自嗨而已我們就是假裝我們不知道隔天的觀看次數然後拿這個函數來進行預測發現誤差是0.48K但是我們真正要在意的是已經知道的觀看次數嗎 不是我們要真正在意的是我們不知道未來的觀看次數是多少好 所以我們接下來要做的事情是什麼呢就是拿這個函式來真的預測一下未來的觀看次數那這邊我們只有2017年到2020年的字我們在這個2020年的最後一天跨年夜的時候找出了這個函式接下來從2021年開始每一天我們都拿這個函式去預測隔天的觀看次數我們就拿2020年的12月31號的觀看次數去預測2021年的字 用2021年元旦的觀看人次預測一下2021年元旦隔天1月2號的觀看人次用1月2號觀看人次去預測1月3號的觀看人次每天都做這件事一直做到2月14號就做到情人節然後得到平均的指數平均的誤差值是多少呢這個是真實的數據的結果在2021年沒有看過的資料上這個誤差值是我們這邊用Lπ來表示它是0.58所以在有看過的資料上在訓練資料上誤差值是比較小在沒有看過的資料上在誤差值上 每個一年的資料上看起來誤差值是比較大的那我們每天的平均誤差有580人左右 600人左右能不能夠做得更好呢在做得更好之前我們先來分析一下結果這個圖怎麼看呢這個圖的橫軸代表的是時間所以0這個點啊最左邊的點代表的是2021年1月1號最右邊的點代表的是2021年2月14號然後這個縱軸啊就是觀看的人次啊這邊是用千人當作單位紅色的線是 红色的线是真实的观看人次蓝色的线是机器用这一个函数预测出来的观看人次你会发现很明显的这个蓝色的线没什么神奇的地方它几乎就是红色的线往右平一天而已它其实也没做什么特别厉害的预测就把红色的线往右平一天这很合理因为我们觉得F1也就是前一天的观看人次跟隔天观看人次的要怎么拿前一天观看人次去预测隔天观看人次呢前一天观看人次在0.97加上0.01加上第八就是隔天 整天的觀看等次所以你會發現說機器幾乎就是拿前一天的觀看等次來預測隔天的觀看等次但是如果你仔細觀察這個圖你就會發現這個真實的資料啊有一個很神奇的現象它是有週期性的啊它有神奇的週期性啊你知道這個週期是什麼嗎你知道這個你知道它每隔七天就會有兩天特別低兩天觀看的人特別少那兩天是什麼日子呢那我發現那兩天都固定是禮拜五跟禮拜六禮拜五跟禮拜六可以了解就禮拜五週末啊大家出去玩啊誰還要學機器學習啊禮拜六我們下禮拜見 誰還要學機器學習啊不知道為什麼禮拜天大家是願意學機器學習這個我還沒有我還沒有參透為什麼是這個樣子也許跟YouTube背後神奇的演算法有關係比如說YouTube在你知道YouTube都會推頻道的影片嘛也許YouTube在推頻道的影片的時候他都選擇禮拜五禮拜六不推只推禮拜天到禮拜四可是為什麼推禮拜天到禮拜四呢這個我也不了解但是反正看出來的結果我們看真實的數據就是這個樣子每隔七天一個循環禮拜五禮拜六看的人就是特別少所以既然我們已經知道每隔七天都是一個循環那這一個式子這一個Model 很爛 因為它只能夠看前一天如果說每個七天開個循環我們應該要看七天對不對我們如果我們一個模型它是參考前七天的資料把七天前的資料直接複製到拿來當作預測的結果也許預測的會更準也說不定所以我們就要修改一下我們的模型那通常對模型的修改往往來自於對這個問題的理解也就是 domain knowledge所以一開始我們對問題完全沒過理解的時候我們就胡亂寫個 y 等於 v 加 w x 半並沒有做得特別好接下來我們觀察了真實的數據以後得到一個結論是每個七天有一個循環所以我們 應該要把前七天的觀看人次都列入考慮所以我們寫了一個新的模型這個模型長什麼樣子呢這個模型就是Y等於B加SJSJ代表什麼這個下標J代表是幾天前然後這個J等於1到7也就是從一天前兩天前一直考慮到七天前那七天前的資料通通乘上不同的weight乘上不同的WJ加起來再加上IF得到預測的結果如果這個是我們的model那我們得到結果是怎麼樣呢我們在訓練資料上的Loss是0.38K那因為這 這邊只考慮一天嘛 這邊考慮七天嘛所以在訓練資料上你會得到比較低的Loss這邊考慮比較多的資訊在訓練資料上你應該要得到更好的更低的Loss這邊算出來是0.38K但是在他沒有看過的資料上面做不做得好呢在沒有看到的資料上有比較好是0.49K所以剛才只考慮一天是0.58K的誤差考慮七天是0.49K的誤差那這邊每一個W跟B啊我們都會用歸根底線算出他的最佳值他的最佳值長什麼樣子呢這邊秀出來給你看他的最佳值長這個樣子 這個邏輯我是沒有辦法了解我本來以為他會選七天前的數據七天前的這個觀看人數直接複製過來不過看來他沒有這樣選就是了他的邏輯是前一天跟你要預測的隔天的數值的關係很大所以W1是0.79那不知道為什麼他還考慮前三天前三天是0.12然後前六天是0.3前七天是0.18不過他知道說如果是前兩天前四天前五天他的值呢會跟未來然後預測的隔天的值是成反比的所以W2 W4跟W5他們最佳的值 讓Loss可以在訓練資料上是0.38K的值是復原但是W1 W3 W6跟W7是正我們考慮前7天的值那你可能會問說能不能夠考慮更多天呢可以那這個請你的感考慮更多天本來只考慮前7天好考慮28天會怎麼樣呢28天就一個月嘛考慮前一個月每一天的觀看人次去預設隔天的觀看人次預設出來結果怎樣呢訓練資料上是0.33K那在2021年的資料上在沒有看過的資料上是0.46K看起來又更好一點好 28天那接下來 在考慮56天會怎麼樣呢在訓練資料上是稍微再好一點是0.32K在沒看過資料上還是0.46K看起來考慮更多天沒有辦法再更進步了看來考慮天數這件事也許已經到了一個極限好那這邊這些模型他們都是把輸入的這個X這個X還記得它叫什麼嗎它叫做Feature把Feature乘上一個Weight再加上一個Piece就得到預測的結果這樣的模型有一個共同的名字叫做Linear Model好那我們接下來會看怎麼把Linear Model做得更好