---
share_link: https://share.note.sx/5giux0j7
share_updated: 2025-09-11T16:42:34+08:00
---
# 笔记总结
![[Pasted image 20250911153340.png]]
-  课程讲解每个作业的通关策略，各作业结构相似。
-  作业包含训练数据（X和对应的Y：X1-Y1, X2-Y2...XN-YN）和测试数据（只有X）。
-  作业二（语音识别）：X是小段声音信号，Y是预测该信号对应的“风铃”（类似音标）。
-  作业三（图像识别）：X是图片，Y是图片中包含的物体。
-  作业四（说话人识别）：X是声音信号，Y是说话人身份，应用于银行客服身份验证。
-  作业五（机器翻译）：X是一种语言的句子，Y是翻译后的句子。


![[Pasted image 20250911153359.png]]
- 训练模型的过程包含三个步骤：
    - 写出包含未知参数（θ）的函数 f(x)，其中 x 为特征。
    - 定义损失函数 (loss function)，用于评估参数 θ 的好坏。
    - 通过优化问题找到使损失函数值最小的参数 θ*。
- 将 θ* 代入模型，使用测试数据进行预测，并将结果上传到目标。
- 直接运行示例代码通常只能达到简单的基线结果。
-  为了获得更好的结果，需要进一步的优化策略。

![[Pasted image 20250911153424.png]]
- 提升作业表现的攻略，如同游戏中的开局策略。
- 首要步骤：==检查训练数据（training data）的损失（loss），而非直接关注测试数据（testing data）的损失。==
- 训练数据损失过大，表示模型在训练数据上学习效果不佳。
- 训练数据学习不佳的两个可能原因：模型偏差==（model bias）==。

![[Pasted image 20250911153436.png]]

- 模型偏差指的是模型过于简单，无法找到使损失函数降低的函数。
- 模型包含未知参数，不同的参数值会产生不同的函数，但这些函数的集合可能太小，不包含能够降低损失的函数。
- 即使找到集合中损失最低的函数，但由于该函数集合本身就无法降低损失，所以效果仍然不好，如同在没有针的海里捞针。
- 解决方法是==重新设计模型，增加模型的弹性，例如增加输入特征的数量（例如从使用前一天的信息改为使用56天前的信息）或使用深度学习。==

![[Pasted image 20250911153501.png]]
- 提升模型弹性方法：增加特征，使用更大模型，或运用深度学习。
- loss值大并不一定代表模型偏差，也可能是==优化方法问题==。
- 主要优化方法为梯度下降法，其缺点是可能陷入局部最小值，无法找到loss值更低的参数。
- 图像化解释：模型可表示函数的集合是一个蓝色集合，其中包含一些loss值低的函数，但梯度下降法无法找到这些函数。

![[Pasted image 20250911153513.png]]
- SEDA STAT 显示模型损失函数值不够低。
- 存在损失函数值低的函数，但梯度下降法未能找到它（大海捞针的比喻）。
- ==问题在于模型偏差（Model Bias）还是优化算法问题（Optimization）。==
- 讨论了模型弹性（Model Capacity）是否足够的问题：模型中是否存在低损失函数（海里是否有针），以及优化算法（梯度下降）是否有效（能否捞到针）。
- 建议通过比较不同模型来判断模型大小是否足够。

![[Pasted image 20250911153548.png]]
-  一个20层和一个56层的网络在测试集上的loss比较，56层网络的loss高于20层网络。
-  早期对该现象的误解：认为是过拟合(overfitting)，表明深层网络无效。
-  实际情况：训练集上56层网络的loss高于20层网络，说明56层网络的优化(optimization)过程存在问题，而非过拟合或模型偏差(Model Bias)。
-  56层网络的容量(弹性)大于20层网络，如果优化成功，其loss应该低于20层网络。  因此，问题在于优化算法的效率。
![[Pasted image 20250911153609.png]]

- 如何判断优化是否有效：尝试先使用简单的模型（例如==线性模型或支持向量机==），这些模型更容易优化，不易出现优化失败的情况。  先了解简单模型能达到的损失值，再训练复杂的深度模型。
- 深度模型的损失值若高于简单模型，则表明优化存在问题（梯度下降效果不佳）。
- 案例：观看人数预测中，四层网络损失为0.10k，但五层网络损失反而增高至0.34k，这并非模型偏差问题，而是优化问题。

![[Pasted image 20250911153635.png]]
- 训练损失大可能是模型偏差(model bias)或优化失败(optimization failure)。
-  下一节课将讲解优化失败的解决方法。
- 训练损失小，测试损失也小，则训练结束。
- 训练损失小，测试损失大，则可能出现过拟合(overfitting)。
- 判断过拟合需先确认训练损失小，再观察测试损失是否远大于训练损失。
-  许多同学忽视训练损失，应先检查训练损失以确定优化和模型大小是否合适。
-  训练损失小而测试损失大是过拟合的可能原因，后续将用极端例子解释。

![[Pasted image 20250911153710.png]]

- 训练数据导致机器学习模型找到一个无用的函数。
- 此函数的工作方式：若输入X存在于训练数据中，则输出对应的Y；否则，输出随机值。
- 此函数在训练数据上的损失为0，但在测试数据上的损失很大。
- 这是一个极端的例子，说明模型可能在训练数据上表现良好，但在测试数据上表现很差，因为它实际上没有学习到任何东西。

![[Pasted image 20250911153721.png]]
- 输入特征X和输出标签Y是一维的，两者关系为二次曲线。
- 该曲线无法直接观察，只能观察到从曲线上随机采样的训练数据点。
- 模型能力强，灵活性大，只根据少量训练数据点拟合曲线时，在未训练数据区域会产生各种结果（过拟合）。
- 测试数据与训练数据来自同一分布，但模型在训练数据上拟合良好的曲线，在测试数据上的表现不一定好。


![[Pasted image 20250911153804.png]]

- ==高自由度模型可能导致过拟合==（在训练数据上表现良好，但在测试数据上表现不佳）。
- 解决过拟合的两个方向：
    - 增加训练数据（最有效，但在作业中不允许）。
    - 数据增强（Data Augmentation）：利用对问题的理解创造新的数据，例如图像识别中的左右翻转或局部放大。  数据增强需谨慎，需符合数据特性和问题理解。  不合理的增强（例如图像上下颠倒）可能导致模型学习到错误信息。

![[Pasted image 20250911153817.png]]
- 解决模型过拟合的第二种方法：==限制模型的灵活性==。
- 通过限制模型为特定类型的函数（例如二次曲线），减少模型的复杂度。
- 这种方法限制了模型可以选择的函数空间，即使训练数据有限，也能更好地拟合真实数据分布，从而在测试数据上获得更好的结果。
- 模型的约束程度（constraint）取决于对问题的理解和模型设计。不同的模型设计会导致不同的结果。


![[Pasted image 20250911163344134.png]]

- ==过拟合与模型限制并非同一概念==。
- ==模型限制过大导致模型偏差（model bias），而非过拟合。==
- 限制过大的例子：假设模型必须是线性模型（y=a+bx），则无法拟合非线性数据，导致测试结果差。
- 模型复杂度与偏差之间存在矛盾关系：增加模型复杂度提升了模型灵活性，但过分限制模型也会产生偏差。


![[Pasted image 20250911163344145.png]]


- 本节课未对模型复杂度和弹性给出明确定义，仅作概念性描述。
- 下节课将讲解如何衡量模型复杂度和弹性。
- 直观理解：复杂模型包含更多函数和参数。
- 复杂模型的训练损失会随着模型复杂度增加而降低。
- 但测试损失会先下降后上升，超过一定复杂度后会暴增（过拟合）。
- 目标是选择一个适中复杂度的模型，在训练和测试数据上都能获得最低损失。


![[Pasted image 20250911163344153.png]]


- 直接根據 Kaggle 分數選擇模型可能導致過擬合，因為模型可能只是在公開測試集上表現良好，而在私有測試集上表現很差。
-  一個極端例子：即使使用一兆個隨機模型，總會有一個在公開測試集上表現出色，但這並不代表該模型有效。
-  僅根據公開測試集分數（Leaderboard）選擇模型，可能導致在私有測試集上表現極差，排名大幅下降。
-  公開和私有測試集的設置，可以避免僅根據公開測試集表現選擇模型所帶來的風險。
-  以往只使用私有測試集評分的學期，也發生過類似情況，導致參賽者排名大幅下降和情緒低落。

![[Pasted image 20250911163344162.png]]

- 使用Public和Private測試集的原因：若只有Public測試集，則模型可以通過隨機產生輸出或過擬合Public數據獲得高分，這沒有意義。

- Benchmark數據集的問題：公開的Benchmark數據集（例如Libris Speech）的測試結果是公開的，這導致即使很差的模型也能通過多次嘗試獲得好的結果，從而夸大模型的性能，例如2016年一些語音辨識模型聲稱超越人類的表現，實際上是在特定Benchmark數據集上達到的。

-  現實應用與Benchmark結果的差距：在Benchmark數據集上表現良好的模型，在實際應用中未必有同樣好的表現。

-  不當操作案例：一些公司通過操縱數據或使用外部API來達到KPI指標，例如清除測試集中的雜訊或秘密使用Google API。

-  總結：分割Public和Private測試集的重要性在於避免過擬合和確保模型在真實環境中的有效性，避免Benchmark數據集的誤導性結果。
![[Pasted image 20250911163344179.png]]


-  为了避免过度依赖Public Set结果，建议将训练数据分成training set (90%) 和 validation set (10%)。
-  根据validation set上的分数选择模型，再上传到Public Set测试。
-  选择validation set分数最高的模型，以此来减少在Public Set上表现好但在Private Set上表现差的风险。
-  限制模型上传次数，避免过度调整模型以适应Public Set。
-  不要过度关注Public Set的排名，因为排名靠前者容易掉落。
-  理想情况下，仅使用validation set选择模型，上传后不再调整。
-  虽然实际操作中很难完全忽略Public Set结果，但应尽量减少对它的依赖。



![[Pasted image 20250911163344189.png]]

- 如何划分训练集(Trending Set)和验证集(Validation Set)：建议使用N折交叉验证(N=3)。
- N折交叉验证过程：将数据分成N份，每次选择一份作为验证集，其余作为训练集，重复N次。
- 模型选择：将不同模型在N折交叉验证中运行，平均每个模型在不同设置下的结果，选择结果最好的模型。
- 最终训练：用选择的最佳模型在整个训练集上进行训练，然后在测试集上进行最终评估。
- 此方法适用于课程前期，用于解决模型训练中验证集划分不佳可能导致结果偏差的问题。

![[Pasted image 20250911163344199.png]]

![[Pasted image 20250911163344208.png]]

- 使用Model 1进行M4交叉验证，先在全部交易集上训练，再用测试集测试。
- 上周预测2月26日观看人数的结果很差，预测值与真实值相差2.58K。所有模型预测都失败，2月26日实际为峰值，而模型预测为低点。
- 模型失败的原因是数据分布不匹配(Mismatch)，这与过拟合(Overfitting)不同。过拟合可以通过收集更多数据解决，但数据分布不匹配则不能。
- 数据分布不匹配是指训练数据和测试数据分布不同，增加训练数据也无济于事。
- COVID-19作业中，使用2020年数据训练，2021年数据测试，结果很差，因为两年的数据分布不同。最终使用了不同的数据分割方法。


