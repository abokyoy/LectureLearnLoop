# 笔记总结

![[Pasted image 20250911171738697.png]]

- 本节课讨论优化失败的原因。
- 优化失败表现为训练损失不再下降，模型性能不佳。
- 过去常误认为是卡在了局部最小值(local minima)，但实际上梯度为零的点还有鞍点(saddle point)。
- 梯度为零的点统称为临界点(critical point)。
- 鉴别是卡在局部最小值还是鞍点很重要，因为鞍点附近仍有下降的空间。
- 课程结束。

![[Pasted image 20250911171738706.png]]
- 如何區分臨界點是局部最小值還是零點：需要用到微積分和線性代數知識。
- Loss function 的形狀複雜，無法完整得知，但可在特定參數 (θ) 周圍用近似式表示。
- 近似式利用Hessian Approximation，包含三項：
    - 第一項：L(θ')，表示 θ 與 θ' 接近時，L(θ) 與 L(θ') 接近。
    - 第二項： (θ - θ')<sup>T</sup>g，其中 g 為梯度向量 (gradient)，用以彌補 θ 與 θ' 之間的差距。
    - 第三項：(θ - θ')<sup>T</sup>H(θ - θ')，其中 H 為 Hessian 矩陣 (包含 L 的二階偏導數)，進一步彌補差距。
-  在臨界點，梯度為零。

![[Pasted image 20250911171738713.png]]

![[Pasted image 20250911171738721.png]]

- 当损失函数处于临界点时，可近似为L0(θ')加上一项（红色项）。
- 通过分析红色项，可以判断临界点是局部最小值、局部最大值还是鞍点。
- 红色项大于0，则为局部最小值 (local minimum)。
- 红色项小于0，则为局部最大值 (local maximum)。
- 红色项有时大于0，有时小于0，则为鞍点 (zero point/saddle point)。
-  为了判断上述条件，无需测试所有向量V，可直接计算Hessian矩阵 (H) 的特征值 (Eigenvalue)。
- Hessian矩阵所有特征值都为正，则为局部最小值。
- Hessian矩阵所有特征值都为负，则为局部最大值。
- Hessian矩阵特征值有正有负，则为鞍点。


![[Pasted image 20250911171738729.png]]

- 使用最簡單的神經網路(單神經元，無激活函數)示例，說明如何判斷是局部最小值(Local Minima)還是鞍點(Saddle Point)。
- 此神經網路的損失函數(Loss Function)只包含兩個參數 W1 和 W2。
- 透過窮舉所有 W1 和 W2 的值，繪製出損失曲面(Error Surface)。
- 損失曲面顯示出多個臨界點(Critical Point)，其中包含多個局部最小值和一個鞍點(零點)。
- 局部最小值位於損失曲面的“山溝”中。
- 鞍點位於曲面的中心(原點)。
-  說明如何通過觀察損失函數在臨界點附近的值變化來判斷臨界點的類型。


![[Pasted image 20250911171738739.png]]


- 使用损失函数 L = (1 - (W1 + W2))^2  计算Z-point。
-  计算W1和W2对L的偏导数。
- 当W1 = 0, W2 = 0时，偏导数均为0，这是一个临界点。
- 使用Hessian矩阵判断临界点类型。Hessian矩阵由L的二阶偏导数组成。
-  当W1 = 0, W2 = 0时，Hessian矩阵为 [[0, -2], [-2, 0]]。
-  Hessian矩阵的特征值一个为正，一个为负，因此该临界点为鞍点 (saddle point)。


![[Pasted image 20250911171738751.png]]

-  讲解如何从Hessian矩阵判断临界点是鞍点还是局部最小值。
-  在鞍点处，梯度为0，但Hessian矩阵可以指示参数更新的方向。
-  Hessian矩阵的特征值和特征向量用于参数更新。
-  如果特征值λ小于0，则沿对应特征向量u更新参数可以减小损失函数值 (L<sub>θ</sub> < L<sub>θ'</sub>)，因为λu<sup>T</sup>u < 0。
-  即使在鞍点处梯度为零，也可以通过找到负特征值及其对应的特征向量来更新参数，从而降低损失函数值。


![[Pasted image 20250911171738758.png]]

- 原点是临界点，其Hessian矩阵有一个负特征值（-2）。
- 对应该负特征值的特征向量有多个，取(1, 1)为例。
- 沿着特征向量(1, 1)的方向更新参数可以降低损失函数，逃离鞍点。
- 实际应用中很少计算Hessian矩阵及其特征值和特征向量，因为计算量巨大。
-  讨论Hessian矩阵的特征向量是为了说明，如果卡在鞍点，仍然有方法（沿负特征值对应的特征向量方向更新）可以继续优化。
-  后续将介绍其他计算量更小的逃离鞍点的方法。

![[Pasted image 20250911171738768.png]]

![[Pasted image 20250911171738777.png]]
- 高维空间中的误差曲面可能与低维空间不同，低维空间中看似众多的局部最小值，在高维空间中可能只是零点。
- 深度学习训练中的误差曲面维度很高（参数数量决定维度），这使得存在许多路径可以优化。
- 实验结果表明，训练过程中很少遇到真正的局部最小值。
- 训练通常卡在临界点（critical point），这些点大多是鞍点（saddle point），而非局部最小值。
- 即使在接近局部最小值的点上，也只有一半左右的特征值是正的，这意味着仍然存在可以降低损失的路径。
- 经验表明，局部最小值并不常见，模型训练通常卡在鞍点上。

---

# 原始语音转文字

好 那我們來上課吧那接下來我們要講什麼呢接下來我們要講如果optimization失敗的時候怎麼辦所以看一下我們剛才的攻略現在我們要講的是optimization的部分所以等一下我們要講的東西基本上跟比如說overfitting沒有什麼太大的關聯我們只是討論optimization的時候怎麼把gradient descent做得更好那為什麼optimization會失敗呢那你常常在做optimization的時候你會發現說隨著你的參數不斷的update你的training的loss不會再下降 但是你對這個Loss仍然不滿意就像我剛才說的你可以把Deep的Network跟Linear的Model或比較Shallow的Network比較發現說它沒有做得更好所以你覺得Deep Network沒有發揮它完整的力量所以Optimization顯然是有問題那有時候你會甚至發現說一開始你的Model就Trend不起來一開始你不管怎麼Update你的參數你的Loss通通都掉不下去那這個時候到底發生了什麼事情呢那過去常見的一個猜想是因為我們現在走到了一個地方這個地方參數對Loss的為分為零 當你的參數在Loss的微分為0的時候Gradient Descent就沒有辦法再Update參數了這個時候Trending就停下來了你的參數不再Update了Loss當然就不會再下降那講到Gradient為0的時候大家通常最先想到的腦海中最先浮現的可能就是Local Minima所以常有人說這個做Deep Learning你用Gradient Descent你會卡在Local Minima然後所以Gradient Descent不Work所以Deep Learning不Work等等但是如果有一天你要寫一個Deep Learning相關的Paper的時候你千萬不要講什麼卡在Local Minima 但是別人會覺得你非常沒有追準為什麼因為不是只有local minima的gradient是0還有其他可能會讓gradient是0比如說saddle point所謂的saddle point其實就是gradient是0但是不是local minima也不是local maxima的地方像在這個例子裡面紅色的這一個點它在左右這個方向是比較高的前後這個方向是比較低的它就像是一個馬鞍的形狀所以叫做saddle point中文就翻成鞍點像saddle point這種地方它也是gradient為0 它不是local minima那像這種gradient為0的點統稱為critical point所以你可以說你的loss沒有辦法再下降也許是因為卡在了critical point但你不能說是卡在local minima因為saddle point也是微分為0的點好 但是如果我們今天你發現你的gradient真的很靠近0卡在了某個critical point我們有沒有辦法知道到底是local minima還是saddle point呢其實是有辦法不是完全沒有辦法那為什麼我們會想要知道到底是卡在local minima還是saddle point呢那我們今天就來講到這裡謝謝大家收看我們下次節目再見 還是卡在settle point呢因為如果是卡在local minima那可能就沒有路可以走了因為四周都比較高現在所在的位置已經是最低的點loss最低的點了往四周走 loss都比較高你會不知道怎麼走到其他的地方去但settle point就比較沒有這個問題如果你今天是卡在settle point的話你今天是走到一個settle point的話settle point旁邊還是有路可以走的還是有路可以讓你的loss更低的你只要逃離settle point你就有可能讓你的loss更低所以鑑別今天我們走到primal point的時候到底是local minima還是settle point是一個值得考慮的 而去探討的問題那怎麼知道今天一個critical point到底是屬於local minima還是zero point呢這邊需要用到一點數學以下這一段如果其實沒有很難的數學啦就只是微積分跟線性代數但如果你沒有聽懂的話以下這一段skip掉是沒有關係的好那怎麼知道說一個點到底是local minima還是zero point呢那你要知道我們loss function的形狀可是我們怎麼知道loss function的形狀呢Nemo本身很複雜又複雜Nemo算出來loss function顯然也很複雜我們怎麼知道loss function長什麼樣子呢 雖然我們沒有辦法完整知道整個Loss Function的樣子但是如果給定某一組參數比如說藍色這個θ塊在θ塊附近的Loss Function是有辦法被寫出來的它寫出來就像是這個樣子所以這個L of θ完整的樣子寫不出來但是它在θ塊附近你可以用這個式子來標示它比如說這個是什麼這個還有好多種顏色這個東西到底是什麼這個東西是Helicerous Approximation這個假設你在微積分的時候其實已經學過了所以你就不會 我不會細講說這一串是怎麼來的但我們就只講一下它的概念這一串裡面包含什麼東西呢第一項是L of theta pi就告訴我們說當theta跟theta pi很近的時候L of theta應該跟L of theta pi還蠻靠近的那第二項是什麼呢第二項是theta減theta pi的全pose乘上gg是什麼g是一個向量這個g就是我們的gradient我們在上週已經講過gradient這個東西了我們用綠色的這個g來代表gradient它是一個向量那這個gradient告訴我們什麼呢這個gradient會來彌補 θ'跟θ之間的差距我們雖然剛才說θ跟θ'它們應該很接近但是中間還是有些差距的這個差距第一項我們用這個gradient來表示它們之間的差距這個G就是有時候gradient會寫成倒三角形的L它是由θ'這個地方的G然後G是一個向量它的第一個component就是θ的第一個component對L的微分好 光是看G還是沒有辦法完整的描述L的θ你還要看第三項第三項是什麼第三項 跟Hashen有關這邊用一個H這個H叫做Hashen它是一個矩陣這個第三項是Seda-SedaPi的全後乘上H才乘上Seda-SedaPi所以這個第三項呢會在補足這個再加上Gradient以後跟真正的L到Seda之間的差距那這個H裡面放的是什麼東西呢這個H裡面放的是L的二次微分這個H它的第I個row第J個column的值是什麼呢它第I個row 第J個column的值就是把Seda的第I個column的對L做成 把θ的第j個component對L做為分再把θ的第i個component對L做為分做兩次為分後的結果就是這個hij好 如果這邊你覺得有點聽不太懂的話沒有關係 反正你就記得說這個L of θ這個Loss function 這個Error surface在θ塊附近可以寫成這個樣子這個式子跟兩個東西有關係跟Gradient有關係 跟Hesitant有關係Gradient就是一次為分Hesitant就是裡面有二次為分的項好 那如果我們今天走到了一個Credible point那意味著什麼 意味著 gradient為0 也就是綠色的這項完全都不見了G是一個zero vector綠色的這項完全都不見了只剩下紅色的這項所以當在critical point的時候這個Loss function它就是L0它可以被近似為L0 theta prime加上紅色的這項我們可以根據紅色的這項來判斷說在theta prime附近的arrow surface到底長什麼樣子知道arrow surface長什麼樣子我們就可以判斷說theta prime它是一個local minima是一個local maxima還是一個zero point我們可以靠這項來瞭解這個arrow surface的地貌大概長什麼樣子知道它地貌長什麼樣子 我們就可以知道說現在是在什麼樣的狀態這個是Hatien好 那我們就來看一下怎麼根據Hatien怎麼根據紅色的這項來判斷θ 和 π附近的我們現在為了等一下符號方便起見我們把θ減θπ用V這個向量來表示θ是一個向量θπ也是一個向量這兩個向量相減我們用V這個向量來表示它如果今天對任何可能的VV的control H乘上V都大於0也就是說現在θ不管帶任何值 V可以是任何的V就代表說θ減θπ可以是任何值也就是θ可以是任何值不管θ帶任何值紅色框框裡面通通都大於0那意味著什麼意味著說紅色框框裡面都大於0意味著說Lnθ大於Lnθπ也就是θ不管帶多少只要在θπ附近θLnθ都大於Lnθπ那代表什麼代表Lnθπ是附近的一個最低點所以它是local minimum如果今天反過來說對所有的V而言Vtranspose H乘上V都小於0也就是紅色框框裡面永遠都小於0也就是θ不管帶什麼 紅色框框裡面都小於0那意味著什麼意味著紅色框框裡面都小於0意味著說L0θ'都是大於L0θL0θ小於L0θ'代表說L0θ'是附近最高的一個點所以它是local maxima第三個可能是假設V transpose HV有時候大於0有時候小於0你帶不同的V進去也就帶不同的θ進去紅色這個框框裡面有時候大於0有時候小於0意味著說在θ'附近有時候有些Loss比L0有時候L0θ大於L0θ'有時候L0θ小於L0θ' 小於L0θ5在L0θ5附近有些地方高有些地方低這意味著什麼這意味著這是一個Zero point那這邊你會有的接下來一個問題是我們看這項看紅色這項沒有辦法判斷我們現在可以判斷它是Local minimum還是Zero point還是Local maximum但是你這邊是說我們要帶所有的V去看VxPose HB是大於還是小於V我們怎麼有可能把所有的V都拿來試試看所以有一個更簡便的方法去確認說這一個條件或這個條件會不會發生這個就直接告訴你結論先進代數理論上是有教過這件事情的 如果今天對所有的B而言B全乘以後是HB都大於0那這種矩陣叫做positive definite正positive definite的矩陣有什麼特性呢它所有的Eigenvalue都是正的所以如果你今天算出一個Hash你不需要把它跟所有的B都乘看看你只要去直接看這個H的Eigenvalue如果你發現說所有Eigenvalue都是正的那就代表說這個條件成立就B全乘以後乘上H乘上B會大於0也就代表說是一個Local minimum所以你從Hash matrix可以看出它是不是Local minimum你只要算出Hash matrix算完以後看它的Eigenvalue發現都是正的 它就是local minima就結束了那反過來說也是一樣如果今天在這個狀況對所有的v而言v transpose h乘以v小於0那h是negative definite那就代表所有的Eigenvalue都是負的所有的Eigenvalue都是負的就保證它是local maxima那如果Eigenvalue有正有負那就代表是saddle point那假設假設你沒有聽得很懂的話你就可以記得結論你只要算出一個東西這個東西的名字叫做action它是一個矩陣這個矩陣如果它所有的Eigenvalue都是正的那就代表我們現在的local minima 如果它有證有過就代表是Saturn Core所以我們是有辦法判斷說是Locominima還是Saturn Core好 那如果剛才講的你覺得你沒有聽得很懂的話我們這邊舉一個例子我們現在有一個史上最廢的Neural它廢到什麼程度呢輸入一個X它只有一個Neuron乘上W1而且這個Neuron還沒有Activation Function所以X乘上W1以後之後就通過輸出然後再乘上W2然後就再輸出就得到最終的輸出就是Y總之這一個Function非常的簡單單Y等於W1乘以W2乘以X這是一個史上最廢的Neural Neural 因為我們有一個史上最廢的training dataset這個dataset說我們只有一筆data這筆data是X是1的時候它的label是1所以輸入1進去你希望最終的輸出跟1越接近越好這個史上最廢的training而這個史上最廢的training它的error surface你是有辦法直接畫出來的因為反正只有兩個參數W1 W2連bias都沒有假設沒有bias只有W1跟W2兩個參數這個network只有兩個參數W1跟W2那我們可以窮取所有W1跟W2的數值算出所有W1 W2數值所帶來的loss 然後就畫出Aero Surface 長這個樣子所以這邊是高的這邊Loss是高的這邊Loss是高的這邊Loss也是高的四個角落Loss是高的好 那這個圖上你可以看出來說有一些Critical Point哪些地方Critical Point呢這個黑點點的地方 這個0.0圓點的地方是Critical Point然後事實上這一條線這邊也是一排Critical Point也是一排Critical Point如果你更進一步要分析它們是Settled Point還是Loss Minimum的話那圓心這個地方圓點這個地方它是Settled Point為什麼它是Settled Point呢你往這個方向走 往這個方向走 Loss 會變大往這個方向走 Loss 會變小往這個方向走 Loss 會變小它是一個 Zero Point而這兩群 Critical Point它們都是 Local Minima所以這個山溝裡面有一排 Local Minima這一排山溝裡面有一排 Local Minima然後在原點的地方有一個 Zero Point這個是我們把 Arrow Surface 報收以後報收所有的參數得到的 Loss Function 以後得到的 Loss 的值以後發出 Arrow Surface 可以得到這樣的結論那現在假設 Google 搜尋報收所有可能的 Loss那如果要直接算出一個點是 Local Minima 還是Z-point的話 怎麼算呢好 我們可以把Loss的function寫出來這個Loss的function這個L是正確答案Y hat減掉model的輸出也就是W1 W2乘以X那這邊取這個square arrow這邊只有一筆data所以就不會submention over所有的全球data因為反正只有一筆dataX代1 Y hat代1我剛才說過只有一筆訓練資料最廢的對 只有一筆訓練資料所以Loss function就是1減W1 W2的平方那你可以把這個Loss function它的gradient求出來那這邊細節我就不講了 我微積分都還記得的話算一下就會知道說這個是對的反正W1對L的微分顯示出來是這個樣子W2對L的微分顯示出來是這個樣子那這個東西就是所謂的矩所謂的歸點什麼時候歸點會0呢什麼時候會到一個critical point呢舉例來說如果W1等於0W2等於0就在原心這個地方如果你W1代0W2代0這個歸點算出來就都是0了如果W1代0W2代0W1對L的微分W2對L的微分算出來就都是0就都是0這個時候我們就知道說原點是一個critical point但是它是一個critical point但它是local maximum它是local maximum Local Minimum還是Settled Point呢那你就要看Hatien才能夠知道我們剛才已經報收說可能W1 W2了所以你已經知道說它顯然是一個Settled Point但是現在假設還沒有報收所有可能的Loss所以我們要看看能不能夠用H用Hatien看出它是什麼樣的Critical Point那怎麼算出這個H呢H它是一個矩陣這個矩陣裡面就是收集了L的各次為分所以這個矩陣裡面第一個Row第一個Column的位置就是該位置 W1對L微分兩次第一個落地的他人的位置就是先用W2對L做微分然後W1對L做微分然後這邊就是W1對L做微分W2對L做微分然後W2對L微分兩次這四個詞組合起來就是我們的Hessian而這個Hessian的值是多少呢這個Hessian的式子我都已經把它寫出來了你只要把W1等於0 W2等於0代進去代進去就得到在原點的地方Hessian是0-2-2-0這樣子的一個矩陣好 那這個矩陣告訴我們這個Hessian告訴我們它是local minima還是single point呢那就要看這個矩陣的I跟value了算一下吧 你會發現這個矩陣有兩個I跟value二跟負二I跟value有正有負代表saddle point對不對 我們剛才講I跟value有正有負就代表的是saddle point好 那所以我們現在呢就是用一個例子跟你操作一下告訴你說你怎麼從Hesian看出一個點它一個critical point它是saddle point還是local minimum好 如果今天呢你卡的地方是saddle point也許你就不用那麼害怕了為什麼因為如果你今天你發現你停下來的時候是因為saddle point停下來的那你可以放心的你其實就可以 你其實就有機會可以放心了為什麼因為H他不只可以幫助我們判斷現在是不是在一個set of point他還指出了我們參數可以update的方向就之前我們參數update的時候都是看radian看G但是我們走到某個地方又發現G變成0了不能再看G了G不見了歸零沒有了但如果是一個set of point的話還可以再看H怎麼再看H呢H怎麼告訴我們怎麼update參數呢我們這邊假設U是H的Eigenvalue然後λ是U的EigenvalueH有一個Eigenvalue叫λ它的對比是 所以我們的Eigenvector叫做U如果我們把這邊的V換成U的話會發生什麼事呢如果我們把U乘在H的左邊跟H的右邊也就是U乘O乘上H乘U會得到什麼呢H乘以U會得到λU為什麼 因為U是一個Eigenvector所以H乘上Eigenvector會得到λEigenvalue乘上Eigenvector所以我們這邊得到UT乘於λU然後再整理一下把UT跟U乘起來得到U的那種平方所以得到λU的那種平方所以這一項假設我們這邊V帶的是一個Eigenvector 我們這邊θ-θ'放的是一個Eigenvector的話會發現說我們這個紅色的項裡面其實就是λ乘上u的null平方那今天如果λ小於0Eigenvalue小於0的話會發生什麼事呢如果Eigenvalue小於0的話那λ乘上u的null平方就會小於0因為u的null平方一定是正的嘛所以Eigenvalue是負的那這一整項就會是負的也就是u的全後乘上h乘上u它是負的也就是紅色這個位置 框框裡面是小於0的 是負的所以這個意思是說假設θ減θ'等於u那這一項就是負的也就是L0θ會小於L0θ'也就是說假設θ減θ'等於u也就是θ等於θ'加u你把θ'本來參數在θ'的位置加上u沿著u的方向做update得到θ你就可以讓Loss變小因為你根據這個式子你知道θ減θ'等於u Loss 就會變小所以你今天只要讓SEDA等於SEDA換一加U你就可以讓Loss變小你只要沿著U也就是Eigenvector的方向去更新你的參數去改變你的參數你就可以讓Loss變小了所以雖然 在 critical point 沒有 gradient如果我們今天是在一個 saddle point你也不一定要 金花你只要找出正你只要找出負的Eigenvalue找出負的Eigenvalue再找出它對應的Eigenvector用這個Eigenvector去加θ'就可以找到一個新的點這個點的Loss比原來還要低如果這樣你聽得不是很清楚的話我們就舉具體的例子剛才我們已經發現說原點是個critical point它的Hesitant長這個樣子好那我們現在發現說呢這個Hesitant有一個負的這個Eigenvalue這個Eigenvalue 這個 eigenvalue 等於負2好,那這個 eigenvalue 等於負2那它對應的 eigenvector 長什麼樣子呢它有很多個,其實是無窮多個對應的 eigenvector我們就取一個出來我們取 1 1是它對應的一個 eigenvector那我們其實只要順著這個 u 的方向順著 1 1 這個 vector 的方向去更新我們的參數你就可以找到一個比 saddle point 的 loss還要 loss 更低的 如果以今天這個例子來看的話呢你的settle point在00這個地方那你在這個地方因為沒有規點所以規點不會告訴你說你要什麼更新參數那Cation告訴我們說Cation的Eigenvector告訴我們說只要往1的方向更新你就可以讓Loss變得更小只要往Q這個Eigenvector的方向更新1的方向更新你就可以讓你的Loss變小也就是說你可以逃離你的settle point然後讓你的Loss變小所以從這個角度來看從這個角度來看似乎 Settle Point 並沒有那麼可怕如果你今天在 training 的時候你的訓練停下來你的規定變成零你的訓練停下來是因為 H 是因為 Settle Point 的話那似乎還有解但是當然實際上啊在實際的 implementation 裡面你幾乎不會真的把 H 算出來為什麼這個要是二次微分哦要計算這個矩陣的computation 需要的運算量非常非常的大更遑論你還要把它的 Eigenvalue跟Eigenvector找出來所以在實作上你幾乎沒有看到有人用這一個方法來逃離saddle point等一下我們會講其他也有機會逃離saddle point的方法他們的運算量都比要算這個h還要小很多那今天之所以我們把這個saddle point跟Eigenvector跟Hessian的Eigenvector拿出來講是想要告訴你說如果是卡在saddle point也許沒有那麼可怕最糟的狀況下你還有這一招可以告訴你要往哪一個方向走 好 那在講到這邊你就會有一個問題了這個問題是那到底Sedopoin跟Local Minima誰比較常見呢我們說Sedopoin其實並沒有很可怕那如果我們今天常遇到的是Sedopoin比較少遇到Local Minima那就太好了那到底Sedopoin跟Local Minima哪一個比較常見呢那這邊呢我們要講一個不相干的故事我們先講一個故事這個故事是什麼呢這個故事發生在1543年 1543年發生了什麼事呢那一年這個君士坦丁堡淪陷這個是君士坦丁堡淪陷圖君士坦丁堡本來是東羅馬帝國的領土然後被鄂圖曼土耳其帝國佔領了然後東羅馬帝國就滅亡了而當時在鄂圖曼土耳其人進攻君士坦丁堡的時候那時候東羅馬帝國的國王是君士坦丁是一世他不知道要怎麼對抗土耳其人有人就獻醜了 上了一層找來了一個魔法師叫做迪奧倫納因為這是真實的故事知道嗎出自三體的故事這個迪奧倫納這樣說迪奧倫納是誰呢他有一個能力跟張飛一樣張飛不是可以萬軍從中取上將首級探囊取物嗎迪奧倫納也是一樣他可以直接取得蘇丹的頭他可以從萬軍中取得蘇丹的頭 大家想說 欸 迪奧倫娜怎麼這麼厲害她真的有這麼強大的魔法嗎所以大家就要迪奧倫娜先展示一下她的力量這時候迪奧倫娜就拿出了一個聖杯大家看到這個聖杯就大吃一驚為什麼大家看到這個聖杯要大吃一驚呢因為這個聖杯啊本來是放在聖索菲亞大教堂的地下室而且它是被放在一個石棺裡面這個石棺是密封的沒有人可以打開它它是迪奧倫娜 他從裡面取得了聖杯而且還放了一串葡萄進去那君士坦丁11世未來要驗證迪奧倫娜是不是真的有這個能力就帶了一堆人真的去撬開了這個石棺發現聖杯真的被拿走了裡面真的有一串新鮮的葡萄就知道迪奧倫娜真的有這個萬軍叢中去上將首級的能力那為什麼迪奧倫娜可以做到這件事呢那是因為這個石棺你覺得它是封閉的那是因為你是從範圍的空間來做 從三圍的空間來看這個石棺是封閉的沒有任何路可以進去但是迪奧倫納可以進入四圍的空間從高圍的空間中這個石棺是有路可以進去的它並不是封閉的那至於迪奧倫納有沒有成功刺殺蘇丹呢你可以想像一定是沒有嘛所以金氏產品寶才淪陷了那至於為什麼沒有大家請見三體這樣就不雷大家了那總之這個從三圍的空間來看是沒有路可以走的東西在高圍的空間中 二維空間中是有路可以走error surface會不會也一樣呢所以你在一維的空間中一維的一個參數的error surface你會覺得好像到處都是local minima但是會不會在二維的空間來看他就只是一個zero point呢常常會有人畫類似這樣的圖告訴你說deep learning的訓練是非常的複雜如果我們移動某兩個參數error surface的變化 非常的複雜是這個樣子的那他顯然有非常多的local minima我在這邊顯然有一個local minima但是會不會這個local minima只是在二維的空間中看起來是一個local minima在更高維的空間中他看起來就是zero point在二維的空間中我們沒有路可以走那會不會在更高的維度上面更高維度我們沒有辦法visualize他我們沒辦法真的拿出來看會不會在更高維的空間中其實有路可以走 如果維度越高是不是可以走的路就越多了呢所以而今天我們在訓練一個Network的時候我們的參數往往動則百萬千萬上所以我們的Error Surface其實是在一個非常高的維度中對不對我們參數有多少就代表我們的Error Surface的維度有多少參數是1000萬就代表Error Surface它的維度是1000萬這樣維度這麼高會不會其實根本就有非常多的路可以走呢那其實有非常多的路可以走 會不會其實local minima根本就很少而經驗上,如果你自己做一些實驗的話也支持這個假說這邊是訓練某一個network的結果每一個點代表訓練那個network訓練完之後把他的hesion拿出來進行計算所以這邊的每一個點都代表一個network我們訓練某一個network然後把它訓練訓練訓練到規定很小,卡在critical point 把這兩組參數拿出來分析看看它比較像是Settle Point還是比較像是Local Minimum那這邊的縱軸跟橫軸是什麼意思呢縱軸代表Trending的時候的Loss就是我們今天卡住了那個Loss沒辦法再下降了 那個Loss是多少那很多時候 你的Loss在還很高的時候訓練就不動了 就卡在Critical Point那很多時候 Loss可以降得很低才卡在Critical Point 這是縱軸的部分橫軸的部分是什麼呢是Minimum Ratio Minimum Ratio呢Minimum Ratio是Eigenvalue的數目分之正的Eigenvalue的數目就如果所有的Eigenvalue都是正的代表我們今天的critical point是local minimum如果有正有負代表saddle point那在實作上你會發現說你幾乎找不到完全所有Eigenvalue都是正的critical point你看這邊這個例子裡面這個minimum ratio代表Eigenvalue Eigenvalue的數目分之正的Eigenvalue的數目最大也不過0.5到0.6間而已代表說只有一半的Eigenvalue是正的還有一半的Eigenvalue是負的所以今天雖然在這個圖上越往右代表我們的Critical Point越向Local Minima但是他們都沒有真的變成Local Minima就算是在最極端的狀況我們仍然有一半的case我們的Eigenvalue是負的這一半的case Eigenvalue是正的 代表說在所有的維度裡面有一半的路其實都還可以讓Loss都還可以讓Loss下降這一半的路會讓Loss上升還有一半的路可以讓Loss下降所以從經驗上看起來其實Local Minima並沒有那麼常見多數的時候你覺得你圈到一個地方你規定真的很小然後所以你的參數不再Update了往往是因為你卡在了一個Zero Point 好 那接下來我們就是要講說假設你Trend啊Trend啊也許你卡在Local Minima雖然我剛才講說Local Minima出現的狀況也許沒有那麼多或卡在Settle Point或甚至是在Settle Point附近也就是非常平坦的地方那麼有什麼樣可能的解決方法好 那在這邊呢我們還是停一下看看大家有沒有問題要問的 Laundry can be really... Ugh! Especially if it sits in the washer too long.