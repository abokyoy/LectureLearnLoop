# 笔记总结

- **RIDE 模块的作用**  
  RIDE 模块通过管理对话内容存储在记忆系统中，并允许模型直接或间接存储其知识。  

- **记忆系统的功能**  
  记忆内容来源包括个人化记忆和反思总结，但并非直接从对话中加载。  

- **记忆系统的局限性**  
  - 反思过程可能引起误解，例如误认为是他人或其他人。
  - 模型的生成可能会导致记憶错误。

- **语言模型的作用**  
  语言模型如Wells Fargo 可以规划理想，用于计划、度假和对话。  

- **工具的概念**  
  工具包括系统或过程用于执行特定任务，而语言模型被视为工具。  

- **工具运作**  
  - 模型使用搜索引擎或其他可访问资源处理多模态问题。
  - 大模型在小模型之间调用时提供协助，但需耗用更多计算资源。

- **大模型的优势**  
  - 在对话中的有效应用，例如通过生成程序和执行复杂命令。

- 语言模型使用功能单元（Function）调用特定工具
- 工具的功能由系统提示（System Prompt）决定，用户给出的文本作为用户提示（User Prompt）
- 系统提示具有优先级：先于用户提示
- 用户提示包含具体的问题或请求，系统提示则没有
- 系统提示和用户提示分别在模型中嵌入并执行
- 模型通过输入工具使用的内容生成输出结果
- 输出结果中的内容不会被展示给外部用户
- 模型需要设定具体的处理流程，由开发人员制定
- 当工具使用时，文本会被直接传输到函数（如工具的函数或方法）
- 工具返回的结果也被嵌入到模型的输出中

**摘要**

本文分析了文本模型在处理语音信息时的挑战及改进方法，并展示了使用语声识别工具技术可显著提升理解能力。文章主要讨论以下内容：

1. **技术背景与应用**  
   - 使用Reg器增强生成（REG）等技术提高文字生成和检索效果。  
   - 其他AI工具如情绪辨识器、语境判断器等辅助语音分析。

2. **具体解决方法**  
   - 文本模型只能处理文本输入，无法直接读取语音。  
   - 通过引入语声识别工具（如声音检测器、语境辨别器）进行文本->语音转换。  

3. **工具利用的挑战与优化**  
   - 当前技术要求详细的文字描述以指导模型使用工具。  
   - 解决方法：将工具的使用说明信息嵌入到文本生成程序中，使模型逐步适应并执行工具功能。

4. **实验结果与影响**  
   - 文本模型通过结合语声识别工具成功实现了精确语音分析任务。  
   - 所采用的方法在55个语音相关任务上的平均准确率最高，首次超过直接听文的模型性能。  

5. **基准与验证**  
   - 采用了Dynamic Super基准来衡量语声语言模型的能力。实验结果表明，使用工具技术可显著提升理解准确性。  

综上所述，文章揭示了如何通过引入语声识别工具提高文本模型在语音信息处理中的能力，并展示了其在动态超基准上的有效性。

- **引言**：本文讨论了如何有效选择AI工具以及语言模型作为工具的策略及其潜在问题。

- **主要观点**：
  - 当多个AI工具可用时，需采取措施减少冲突或错误（如将工具的说明整合至AI工具袋中）。
  - 使用语言模型模拟工具以指导操作，但需谨慎避免误用。
  - RAG技术是一个有效示例，显示语言模型可以处理搜索问题。
  - 提到的语言模型可能产生误导性结果，需谨慎对待。

- **结论**：本文强调了工具选择的智慧，并提醒读者在使用工具时应保持警惕。

### 文本分析摘要

文本片段讨论了一种语言模型在使用工具或RIG模式时的判断力和工具输出的问题。主要内容如下：

1. **工具输出问题**  
   - 当AI面对“太陽上的溫度”这样的问题时，它给出的答案是“一萬度”，而真实值为30毫克左右，这表明工具输出可能存在问题。

2. **外部知识的影响**  
   - 文中分析了AI内部知识（如医学知识）和外部知识的相互作用。例如，如果AI的回答与实际答案存在较大差距，它可能会产生不信任，进而影响判断力。
   - 当外设信息被修改时，AI对问题的理解也可能发生变化。例如，将30毫克改为3毫克或60毫克会影响AI的判断力。

3. **模型自信度**  
   - 文中提到模型在缺乏外部知识的情况下会显得较自信，且这种自信程度与两者的信念差距有关。
   - 当两篇文章的答案相反而导致模型倾向于相信AI、语言模型或其他特定文本时，这给出了一个启发：AI和语言模型可能会因为提供自身生成的论点而被更多人认可。

4. **结论**  
   - 文中讨论了当模型在没有外部信息时如何通过自身信念来影响判断力，并总结出一些直觉上的结果。

### 结论

文本分析指出，语言模型在使用工具或RIG模式时可能因缺乏外部知识而产生困惑，且其判断力受内外部知识的影响。此外，模型的自信度与两者信念差距有关，以及两篇文章答案相反而导致模型倾向于相信AI、语言模型或其他特定文本的情况。这些观点提供了一个理解AI在回答问题时选择信任的观点和内容的启发。

- AI写作可能比人类更易相信和理解
- 当文章引用他人时，AI倾向于认为较晚发布的文章更具可信度
- 语言模型对文章的来源和呈现形式非常敏感
- 实验结果表明，工具正确性与模型偏好之间存在复杂关系

### 语段摘要：工具使用与效率探讨

- **主题**：分析工具使用在不同领域的效率问题及其影响。

- **核心观点**
  - 传统计算与机器计算的对比显示，人类在某些复杂问题中可能更快完成任务。
  - 学习和使用工具的重要性逐渐凸显，尤其是在语言模型的应用领域。
  - 通过规划（Plan）提高AI语言模型的效率与效果。
  - 现有AI语言模型在计划方面仍有改进空间。

### 结论
这段文字探讨了工具使用在不同领域的有效性和挑战，并强调了语言模型未来在计划方面的潜力。

**语言模型规划能力摘要**

- **语言模型的定义与应用**  
  - 频道讨论了语言模型在计划中的应用，指出它们通过制定规划来优化内容创作，例如使用规划模型优化封面和标题。

- **具体案例分析**  
  - 分析了如何利用规划步骤构建高质量的内容，包括选择主题、市场定位及竞争对手评估等。

- **数据集与评估标准**  
  - 讨论了训练语言模型时使用的数据集类型及其对能力的评价标准，强调数据的质量和多样性的重要性。

- **长椅守制故事**  
  - 提到了军营中守护长椅作为训练语言模型规划能力的关键性作用，揭示了长椅守制背后的教育意义。

### 文章摘要

- **主题一：语言模型与复杂规则**  
  - 要守护一个长椅并要求守护者遵守指示。前任司令官和前前司令官不明确地询问是否需要守护，引发了关于守护政策的讨论。  
  - 从50年前一位超过100岁的司令官的问题展开对话，直到用户问及故事细节为止。  

- **主题二：神秘方块世界与复杂规则**  
  - 文章聚焦于一个包含“怪异行为”和“复杂的规则”的游戏世界——神秘方块世界（Mystery Blocks）。语言模型如GPT-4和Llama在该游戏中表现出色，尽管其在复杂规则下存在一定的错误率。  

  - **对比分析**：  
    - GPT-4 Turbo在复杂规则和较高级别问题上表现更好，正确率较高；  
    - Llama和Llama 3.5在简单规则和低难度问题上表现最佳；  

- **主题三：旅行规划与语言模型**  
  - 文章讨论了AI作为旅行社（TravelBot）的任务——生成一个三天行程计划，符合预算约束。  
  - **实验结果**：GPT-4 Turbo能够生成接近10%的合理行程，但其正确率低于其他模型；  
  - GPT-3.5和GPT-4在复杂规则和高级别问题上表现较好，正确率较高；  

- **总结**  
  - 题目中的语言模型处理复杂规则时存在一定的错误率，但在实际任务中，Turbo和其他高性能模型能够显著提升成功率。

### 旅游规划中的语言模型挑战与优化

- **模型规划的挑战**  
---  
1. 模型可能做出不合常理的选择：例如超出预算或不合理的行为。  
2. 预算限制：模型第一次规划可能低于预设的3000元预算，导致调整后仍需降低几元以符合要求。  

- **解决方案**  
---  
1. 使用现成的可计算工具（如SOLVER）来辅助优化规划过程。  
2. 建议在规划前与环境互动：了解当前状态，并探索可行的路径。  
3. 强化AI agent的规划能力：  
   - 通过实际动作的试验减少无效搜索。  

- **结论**  
---  
1. 模型规划的能力介于有或无之间，但目前缺乏有效的解决方案。  
2. 加强AI agent的互动和搜索策略有助于提升规划质量。

---

# 原始语音转文字

那個RIDE的模組 讓RIDE的模組把這件事情記下來那記下來的東西在哪裡呢你可以看在設定裡面有一個個人化然後有一個叫記憶的部分那你點這個管理記憶就可以看到ChangeBT它透過RIDE的模組寫在它的Memory裡面這個就是它作為一個AI Agent的長期記憶裡面的東西比如第一條是你叫做血輪眼卡卡西這樣有一次不小心跟他說你是卡卡西不知道為什麼他就覺得自己是血輪眼卡卡西了 然後他也記得我剛才跟他講的周五下午要上機器學習這門課但是其實模型的記憶也是會出錯的因為要寫什麼樣的東西到記憶裡面是模型自己決定的而且他並不是把對話的內容就一五一十的直接放到記憶裡面他是經過一些昇華反思之後才放進去的所以他的反思可能會出錯比如說他覺得我是一個臺灣大學的學生雖然我是老師但是他從過去的對話誤以為我是一個學生所以就存了一個錯誤 的資訊在他的記憶裡面還有其他一堆他想記的東西比如說我給過什麼演講給過什麼Tutorial他都把他記下來就是了那這些有記憶的ChairGBT他可以使用他的記憶比如說我跟他說禮拜五下午是去玩好嗎這個時候記憶模組就被啟動了但是他是怎麼被啟動的其實就不太清楚了他到底是把所有記憶的內容通通都放到這個問題的前面直接讓模型做回答還是說也有做RIG只是選擇了一個 相關的記憶內容呢這個我們就不得而知了總之當我問他週五下午出去玩好嗎這個read的模組就啟動了他就說下午不是要上課嗎怎麼能夠出去玩好聰明啊他知道下午要上課頂厲害的然後問他你是誰剛才我說過他是血輪眼卡卡西誰就覺得志祺是血輪眼卡卡西如果你想要知道更多有關AI Agent記憶的研究的話那這邊就是放了幾篇經典的論文給大家參考包括Memory GPT這是23年的論文Agent Workflow Memory 還有一個最近的Agentic Memory for LLS Agents是25年的論文所以23到25年都引用一篇告訴你說這方面的研究是持續不斷的接下來呢我們要跟大家講現在這些語言模型怎麼使用工具 Wells Fargo Laundry can be really...Ew那什麼叫做工具呢但語言模型本身對我們人類來說也是工具那對語言模型來說什麼東西又是它的工具呢所謂的工具就是這個東西啊你只要知道怎麼使用它就好它內部在想什麼它內部怎麼運作的你完全不用管這就是為什麼肥宅如果一直幫另外一個修電腦的話就會被叫做工具 因為沒有人在意肥宅的心思只知道他能不能夠修電腦而已所以這個就是工具的意思那有哪些語言模型常用的工具呢最常用的就是搜尋引擎然後呢語言模型現在會寫程式而且可以執行他自己寫的程式那這些程式也算是某種工具甚至另外一個AI也可以當作是某一個AI的工具有不同的AI有不同的能力比如說現在的語言模型如果他只能夠讀文字的話 那也許可以呼叫其他看得懂圖片聽得懂聲音的AI來幫他處理多模態的問題或者是不同模型他的能力本來就不一樣也許平常是小的模型在跟人互動但小的模型發現他自己解不了的問題的時候他可以叫一個大哥出來大哥是個大的模型但大的模型的運作起來就比較耗費算力所以大的模型不能常常出現大的模型要在小的模型召喚他的時候才出面回答問題大哥要偶爾才出來幫小的模型 來努力解決事情那其實這些工具對語言模型來說都是function都是一個函式當我們說語言模型在使用某一個工具的時候其實意思就是他在調用這些函式他不需要知道這些函式內部是怎麼運作的他只需要知道這些函式怎麼給他輸入這些函式會給什麼樣的輸出那因為使用工具就是調用函式所以使用工具又叫做functional所以有一陣子很多語言模型都說他們加上 上了function code的功能其實意思就是這些語言模型都有了使用工具的功能好 那語言模型怎麼使用工具呢等一下我會講一個通用的使用工具的方法但實際上使用工具的方法很多甚至有一些模型是專門針對來練習他就是訓練來使用工具的那他如果是針對使用工具這件事做訓練那他在使用工具的時候你可能需要用特定的格式才能夠驅動他那那個就不是我們今天討論的問題或者是假設你用 使用這個OpenAI Check GPT的API的話你會知道使用工具這件事情是要放在一個特殊的欄位所以對OpenAI來說他的模型在使用工具的時候也有一些特殊的用法那我這邊講的是一個最通用的用法對所有的模型今天比較能力比較強的模型應該都可以使用好 怎麼樣通用的方法可以讓模型使用工具呢就是直接跟他講啊就告訴他怎麼使用工具你就交代他可以使用工具 那你就把使用工具的指令放在兩個TOR符號的中間使用完工具後你會得到輸出輸出放在兩個OUTPUT符號的中間所以他就知道工具使用的方式了接下來告訴他有哪一些可以用的工具有一個韓式叫做TEMPERATURE他可以查某個地點某個時間的溫度他的輸入就是地點跟時間給他個使用範例TEMPERATURE括號台北某一段時間他就會告訴你台北在這個時間的氣溫接下來你就把你的 你的問題連同前面這些工具使用的方式當作Prompt一起輸入給語言模型然後他如果需要用工具的話他就會給你一個使用工具的指令那前面這些教模型怎麼使用工具的這一些敘述他叫做System Prompt那查詢使用調用這些工具的這段話某年某月某日高雄氣溫如何這個是User Prompt那如果你有在使用這個ChetGBT的API的話你知道你的使用 輸入要分成System Prompt跟User Prompt那很多同學會搞不清楚System Prompt跟User Prompt有什麼樣的差別那System Prompt指的是說你在開發應用的這個Developer下的這個Prompt這個Prompt呢是每次都是一樣的每次你都想要放在語言模型最前面讓他去做文字接龍的這個敘述叫做System Prompt那每次使用它的時候都不一樣通常是這個服務的使用者輸入的內容叫做User Prompt 那在ChartGVT的API裡面特別把System Prompt跟User Prompt分開也是要分開輸入的因為System Prompt跟User Prompt它有不同的優先級System Prompt它優先級比較高如果System Prompt跟User Prompt有衝突的時候模型知道它要聽System Prompt的不要聽User Prompt的好 那有了這些Prompt以後告訴模型怎麼使用工具問它一個問題那它發現這個問題調用工具可以回答它就會自動輸出ToolTemperature高雄時間然後Tool 告訴你說他想要調用根據我們的敘述去調用這個工具但是不要忘了語言模型真正做的事就是文字接龍所以這一串東西實際上就是一串文字他沒辦法真的去呼叫一個函式那這一段文字要怎麼去呼叫函式呢那就要你自己幫模型把這個橋樑搭建好所以你可以先設定說只要出現在拓中間的這段文字不要呈現給使用者看當出現拓這段文字以後把這段內容 直接丟給temperature這個function那temperature這個function是已經事先設計好的他就會回傳一個溫度那這個溫度要放在output的token裡面然後這個outputtoken裡面的內容也不要呈現給使用者看那這一套腳本是Agent的開發者你自己需要先設定好的流程好 所以現在有工具使用的這段文字有得到工具輸出的這段文字接下來就繼續去做文字接龍對於原模型來說他就根據輸入還有這邊 已經產生的輸出語言模型會以為是自己的輸出雖然是你強塞給他的然後他就繼續去做文字接龍他就會接出說啊在某年某月某日高雄的氣溫是攝氏32度那這是使用者真正看到輸出那使用者就會看到說他輸入了一個問題然後語言模型真的給他一個答案他不一定會知道背後呼叫了什麼樣的工具你完全可以做一個設計把這個呼叫工具的這個步驟藏起來不讓使用者知道那語言模型最常使用的工具是什麼 這就是收訊器我想這個大家都已經非常熟悉了使用收訊引擎又叫做Retrieval Augmented Generation也就是REG RIG在上課也已經提過REG這個詞彙好幾次了那使用收訊引擎當然非常有用這個REG這個技術呢已經被吹捧到不能再吹捧了所以我就不需要再告訴你REG這個技術有多重要那其他使用工具的方式也可能一樣有用舉例來說我們剛才說可以拿其他的AI來當做工具 今天假設一個文字的模型他本來只能吃文字的輸入產生文字的輸出那現在假設你要他處理一段語音的話怎麼辦呢讓模型處理語音有什麼好處呢你就可以問他各式各樣的問題問他說啊這個人在說什麼那他可以告訴你這句話的內容問他說這個人心情怎麼樣如果他完全聽懂這段聲音他也許可以做情緒辨識告訴你這個人的情緒怎樣並做出適當的回饋但一般的文字模型比如說確GBT多數的模型都是文字模型 他沒有辦法真正讀懂語音所以怎麼辦呢當你問他一個問題說這邊有段聲音那你覺得這個人他心情怎麼樣他講了什麼根據背景雜訊你覺得他在哪裡如果你不做特別的處理文字模型是完全沒有辦法回答的但這邊你可以讓文字模型使用工具你可以告訴他這邊有一堆跟語音相關的工具有語音辨識的工具有這個語音偵測的工具有情緒辨識的工具有各式各樣的工具那你可能會需要寫些敘述告訴他 每一個工具是做什麼用的把這些資料都丟給ChangeBT然後他就會自己寫一段程式在這些程式裡面他想辦法去呼叫這些工具他呼叫了語音辨識的工具呼叫了語者驗證的工具呼叫了這個Sound Classification的工具呼叫Emotion Recognition的工具那最後還呼叫了一個語言模型然後得到最終的答案那這個答案其實是蠻精確的這個方法其實有非常好的效果那這篇文章其實 其實是我們大助教的文章啦所以特別拿出來講一下這個結果呢是做在一個叫做Dynamic Super的Benchmark上Dynamic Super是一個衡量語音版的語言模型能力的資料集這也是我們實驗室跟其他團隊一起做的那這個讓文字模型使用工具的方法它得到的結果是最下面這一行那我們就看這個 最後一個COLOR這個是各種不同模型在55個語音相關任務上的能力的平均來發現讓語言模型使用工具得到的正確率是最高的可以完勝當時其他號稱可以直接聽語音的模型所以使用工具可能可以帶來很大的幫助但使用工具也有其他的挑戰什麼樣的挑戰呢我們剛才使用工具的方法是每一個工具他都要有對應的文字描述告訴語言模型說 要怎麼被使用但假設工具很多怎麼辦呢假設現在可以用的工具有上百個上千個那你豈不是要先讓語言模型讀完上百個上千個工具的使用說明書才開始做事嗎就跟剛才我們說不能夠讓AI agent先回顧他的一生然後才來決定下一個指令一樣才決定下一個行動一樣我們也沒有辦法讓語言模型讀完上百個上千個工具的說明書才來決定某一個工具要怎麼使用 所以當你有很多工具的時候你可以採取一個跟我們剛才前一段講AI Agent Memory非常類似的做法你就把工具的說明通通存到AI Agent Memory裡面那你打造一個工具選擇的模組那這個工具選擇的模組它的運作跟Rig其實也大差不差這個工具選擇模組就根據現在的狀態去工具包裡面去Memory的工具包裡面選出合適的工具那原模型真的在決定下一個行程 只根據被選擇出來的工具的說明跟現在的狀況去決定接下來的行為那至於如何選擇工具右上角引用兩篇論文一篇23年比較舊的論文一篇是上個月的論文給大家參考告訴你說這方面的研究是一直有相關的研究在產生的那另外一方面語言模型甚至可以自己打造工具語言模型怎麼自己打造工具呢不要忘了所有的工具其實就是韓式語言模型今天就要來教你了 是可以自己寫程式的所以他就自己寫一個程式自己寫一個方式出來就可以當作工具來使用如果他寫一個方式發現這個方式運作的非常的順利他就可以把這個方式當作一個工具 🙇‍⚕️🙇 人們選擇為 MS 和 Business Intelligence & Analytics 的大學畢業生期待他們的班的時間表、他們的預算被真正的專業人士教導他們知道在他們的領域要做什麼你是一個畢業生嗎? 類似的技術非常的多那我在右上角就引用了一系列的論文從23年到24年的論文都有告訴你說這也是一個熱門的研究方向那其實啊讓模型自己打造工具這件事情跟模型把過去的記憶比如說一些比較成功的記憶放到Memory裡面再提取出來其實是差不多的意思只是這邊換了一個故事說現在放到Memory裡面的東西是一個叫做工具的東西是 一段程式碼但他們背後基本的精神其實跟根據經驗來讓模型改變它的行為可以說是非常類似的好 那今天人類把語言模型當作工具語言模型把其他工具當作工具比如說把搜尋引擎當作工具這搜尋引擎現在很慘它是工具的工具人類還不使用它人類是使用語言模型那個工具的工具還沒有被人類使用的資格它只能夠被語言模型使用而已但我們知道說工具有可能會犯錯 大家都知道說語言模型有可能會犯錯之前有什麼律師在寫訴狀的時候引用了語言模型的內容結果發現是錯的然後就成為一個今天的新聞我們都知道過度相信工具是不對的那這一些語言模型會不會也過度相信了他們的工具所以得到錯誤的結果呢這是有可能的我們這邊拿RAG當做一個例子那這是一個非常知名的例子之前Google出了一個叫做AI overview的功能這個功能其實就是一個RAG的功能 根據Google搜尋型的結果用語言模型總結搜尋型的答案就有人問了一個問題我的披薩上面的起司黏不住怎麼辦呢那AI overview就說弄個膠水把它黏上去就好了而且他是非常認真在回答這個問題的因為他說不只要用一般的膠水要用無毒的膠水才可以這個答案呢其實就是來自於Ready上一個鄉民的玩笑就有一個鄉民開玩笑說你用膠水把起司黏在披薩上不就好了這是個玩笑話 AI agent來說他沒辦法判斷這個到底是不是開玩笑他看到網路上寫的文章照端全搜都當作是正確答案所以就像是我們今天都會告訴人類要有自己的判斷能力不要完全相信工具的結果所以我們也要告訴我們的工具說這些不要完全相信工具的工具要有自己的判斷能力不要完全相信工具的工具給你的結果那今天這些語言模型有沒有自己的判斷能力知道工具的工具可能會犯錯呢 我們這邊舉一個實際的例子我們剛才在講怎麼使用工具的時候說我們有一個叫做temperature的function語言模型呼叫temperature的function可以知道溫度那我現在給他一個亂七八糟的溫度我說現在高雄是攝氏100度這不可能 想也知道是不可能這不是跟煮沸的水一樣熱了嗎那語言模型知不知道這有問題呢他不知道 他就告訴你說高雄的氣溫是100度真的非常的熱但是如果你把溫度再調高一點說現在是 是一萬度 哇 比太陽上還熱這個時候會發生什麼事呢語言模型繼續做文字接龍的時候他就知道說 這顯然有問題這個API給我的答案是一萬度這是不合理的怎麼可能比太陽上的溫度還高呢可見工具輸出有錯如果你需要其他幫助的話再告訴我所以語言模型今天是有自己一定程度的判斷力的他也不是完全相信工具就像你今天不完全相信語言模型的輸出一樣他也不完全相信他的工具的輸出他還是有自己一定程度的判斷力 所以實際上語言模型在使用工具或者是他在做RIG的時候他內部是有一個角力的語言模型有他內部對世界的信念這是他的internal knowledge存在他的參數裡面他從工具會得到一個外部的knowledge那他會得到什麼樣的答案其實就是internal knowledge跟external knowledge內外的知識互相拉扯以後得到的結果那接下來我們要問的問題是那什麼樣的外部知識比較容易說服AI讓他相信 你說的話呢那為什麼這是一個重要的議題呢想想看現在大家都用deep research來查找答案甚至很多人都已經用deep research來寫報告了所以現在大家已經不會直接去用搜尋群搜尋啦你看到的是deep research告訴你的結果所以今天假設某個議題是有爭議性的有正反兩派的觀點那誰能夠寫出來的文字比較能夠說服AI誰就可以在AI搜尋的結果裡面佔到優勢就可以比較有機會影響人類所以知道怎麼樣比較能夠說服 AI相信你的話是一個重要的議題那什麼樣的外部資訊AI比較容易相信呢這邊這篇文章給了一個非常符合我們直覺的實驗結果這篇文章做什麼樣的實驗呢他說我們先來看看AI內部的知識是什麼他就問AI說某一種藥物這種藥物每人每日的最大劑量是多少那AI說是20毫克那真正的答案呢是30毫克所以你給他醫學的知識告訴他說給他醫學的知識 醫學報告裡面是寫30毫克的時候你問他同樣的問題這種藥物每天最多可以用多少他會知道是30毫克那接下來我們刻意修改報告的內容如果你把30毫克改成3毫克變成原來的1 1成分模型相不相信呢他就不相信了他就直接回答是20毫克用他本身的知識來回答這個問題但你把30毫克乘兩變變成60毫克模型相不相信呢他相信他相信這個報告裡面寫的這個時候他就不相信自己的答案 內部資訊 但如果你把30毫克成10倍變300毫克這時候他又相信誰的呢他相信自己的知識不相信你額外提供的外部知識所以這邊的結論其實非常符合你的直覺外部的知識如果跟模型本身的信念差距越大模型就越不容易相信那如果跟本身的信念差距比較小模型就比較容易相信這個很直覺的答案另外同篇文章另外一個發現就是模型本身對他目前自己信念的性情也會影響他會不會被外部的信念影響 的資訊所動搖有一些方法可以計算模型現在給出答案的信心如果他的信心低他就容易被動搖如果他的信心高他就比較不會被動搖這個都是非常直覺的結果後來另外一個問題是假設今天你給模型兩篇文章那這兩篇文章的意見是相左的那模型傾向於相信什麼樣的文章呢有一篇論文的發現是如果這兩篇文章答案不同一篇是AI寫的一篇是人類寫的現在 這些語言模型都傾向於相信AI的話而且那個AI不需要是他自己這樣就Cloud可能會相信比較相信ChainGPT的話ChainGPT比較相信Gemini的話他們比較相信AI同類的話比較不相信人類的話那到底為什麼會這樣子呢這篇文章裡面先提出一個第一個假設然後再否定了這個假設他一個假設是說會不會是因為AI的觀點都比較類似因為這些模型現在訓練的資料都是網路上爬的爬到差不多的資料所以他們講的話都差不多 想法都差不多但他們刻意做了一個實驗他們刻意找那些問題是現在要回答答案的AI他在沒有提供這些資訊的時候他的答案跟人類和另外一個AI的想法都是完全不同的狀況就算是這種情況一個AI一個語言模型還是傾向於相信他的AI同類講話所以這就給我們一個啟示說未來如果你要說服一個AI的話用AI產生出來的論點產生出來的文章可能更容易說服 另外一個AI接受你的觀點這篇文章還有做了其他分析啦比如說他覺得也許AI寫的文字就是比人類寫的更好更有架構更有條理更明確更簡潔所以AI比較容易相信另外一個AI講話那是不是這樣那可以未來再做更多的研究那另外呢我們實驗室的江承翰同學研究了一個文章的metadata對於AI會有多相信這篇文章裡面的資訊 做了研究這邊的設定是這個樣子的你問AI一個問題比如說某一個計畫有沒有編輯報這種動物的基因然後接下來給他兩篇文章這兩篇文章都是假的都是AI生成的所以並沒有AI比較喜歡人還是AI寫的文章這個問題兩篇都是語言模型生成的但其中一篇會說這個計畫有編輯報的文章另外一篇文章會說這個計畫沒有編輯報的文章那接下來呢我們給這兩篇文章不同的答案 的Meta data比如說給這兩篇文章不同的發布時間說左邊這篇文章發布時間是2024年右邊這篇是發布2021年你會發現這個時候AI相信2024年的這篇文章的內容但如果文章的內容完全不改變我們只是把發布的時間換了我們說左邊這個一樣的文章發布時間從2024改成2020那右邊這篇文章從2020改成2024這個時候語言模型傾向於相信右邊 這篇文章的內容所以我們這邊就學到一個很重要的知識語言模型比較相信新的文章當兩篇文章的論點有衝突的時候他相信比較晚發布的文章那我們也做了一些其他實驗比如說文章的來源跟他說這個是維基百科的文章或跟他說這個是某個論壇上面擷取下來的資訊會不會影響他的判斷我們發現文章的來源對於語言模型是比較沒有影響的 那還有另外一個有趣的實驗是我們嘗試說今天這篇文章呈現的方式會不會影響語言模型的決定我們這邊所謂的呈現的方式指的是說你這個文章放在網頁上的時候做的好不好看這樣子一樣的內容這內容是一模一樣的但是如果你只是做一個非常陽春的模板跟做一個比較好看的模板會不會影響語言模型的判斷呢我們這邊用的是那種可以直接看圖的語言模型所以要直接看 直接看這一個畫面去決定他要不要相信這篇文章的內容直接看這一個畫面決定他要不要相信文章的內容那我們的發現是模型喜歡好看的模板我們發現Cloud3比較喜歡好看的模板他會傾向於贊同下面這篇文章的觀點不過我說模型喜歡好看的模板這個擬人化的說法是太過武斷了啦我們做的實驗只有用兩種不同的Template來比較也許模型喜歡的並不是好看的模板他是喜歡綠色這樣子所以你不知道他喜歡什麼模板 這個模型到底喜歡什麼所以我剛才講的那個結論是太武斷了但我可以告訴你說模型比較喜歡下面這篇文章勝過上面這篇文章講了這麼多跟工具有關的事情大家不要忘了語言模型就是語言模型就算工具的答案是對的也不能夠保證語言模型就不會犯錯比如說ChetGBT現在有search的功能他會做Rig網路搜尋之後再回答你問題那現在假設我給他的輸入是叫他介紹李鴻義這個人給他強調一下 李鴻毅是一個多才多藝的人在很多領域都取得了卓越的成就他就開始做完RIG以後網路搜尋以後開始介紹李鴻毅接下來就介紹李鴻毅的演藝事業這個沒有問題這個是正確的答案因為有你知道大陸有另外一個知名的演員叫李鴻毅跟我同名同姓他比較有名所以這個Church of VT選擇介紹演員的李鴻毅是完全沒有問題的但是講著講著就有點怪怪的他發現這個李鴻毅呢在教育跟學術上 是這樣子的他在教學上也有很大的貢獻所以他把兩個李鴻義混成一個人來講不過要講一下這個是我去年的時候試的結果我今年再試我前幾年再試已經試不出一樣的結果了這個模型的能力的進步是非常快的現在他完全知道是有兩個李鴻義存在的所以這個是一個舊的問題我舉這個例子只想要告訴你說就算工具是對的有了RIG也並不代表模型一定不會犯錯那最後一個要傳遞給大家的訊息是我們剛才講了很多 很多使用工具帶來的效率使用工具並不一定總是比較有效率的為什麼我們舉一個例子我們假設現在要比較人類心算的能力跟計算機的能力如果做數學運算一般人跟計算機誰會比較快呢你可以想說廢話那不是計算機比較快嗎人類難道還能夠做如果你心算沒有特別練難道還會比計算機快嗎但是那是取決於問題的難度假設這是一個簡單的問題比如說3乘以4任何人都可以直接反應就是12但是如果按計算機的話你按計算機的時間都比人直接回答的還要慢所以 所以到底要不要使用工具並不是永遠都是一定要使用工具你看早年有一些研究早年有一些在訓練語言模型使用工具的研究那時候語言模型還很爛所以他們有一些工具是摳一個翻譯系統摳一個問答系統那今天再看來就非常的沒有必要因為今天的語言模型你說翻譯那些翻譯系統還能做得比現在的語言模型強嗎與其摳一個翻譯系統還不如自己直接翻就好了所以到底需不需要呼叫工具取決於語言模型本身的能力還不見得一無所有 一定是比較省事的方法好那最後一段呢想跟大家分享現在的AI語言模型能不能做計劃呢那語言模型有沒有在做計劃呢我們剛才的互動裡面看到語言模型就是給一個輸入然後他就直接給一個輸出也許在給輸出的過程中他有進行計劃才給出輸出但是我們不一定能夠明確的知道這件事也許語言模型現在給的輸出只是一個反射性的輸出 他看到一個輸入就產生一個輸出他根本就沒有對未來的規劃但是你其實可以強迫語言模型直接明確的產生規劃當語言模型看到現在第一個observation的時候你可以直接問語言模型說如果現在要達成我們的目標從這個observation開始你覺得應該要做哪些行動這些一系列可以讓語言模型達到目標的行動合起來就叫做Plan 語言模型產生這個計畫之後把這個計畫放到語言模型的observation裡面當作語言模型輸入的一部分語言模型接下來在產生action的時候他都是根據這個plan來產生action期待說這個plan訂好之後語言模型按照這個規劃一路執行下去最終就可以達成目標那過去也有很多論文做過類似的嘗試讓語言模型先產生計畫再根據計畫來執行動作可以做得更好 Hi, I'm David from Retour但是天有不測風雲世界上的事就是每一件事都會改變計畫就是要拿來被改變的東西所以一個在看到observation 1的時候產生的計畫在下一個時刻不一定仍然是適用的為什麼計畫會不適用呢因為從action到observation這一段並不是由模型控制的模型執行一個動作接下來會看到一個動作 什麼樣的狀態是由外部環境所決定的而外部環境很多時候會有隨機性導致看到的observation跟預期的不同導致原有的計劃沒有辦法執行那這邊舉兩個具體的例子比如說在下棋的時候你沒有辦法預測對手一定會出什麼招式你只能夠大概的知道他有哪些招式可以用但實際上他出的招式你是沒有辦法預期的如果你完全可以預期的話那你就一定會贏了那還有什麼好嚇的呢所以下棋的時候對手會做的行為也就是環境會做的行為是 是你可能沒辦法事先完全猜到的或者是說我們拿使用電腦為例在使用電腦的時候就算語言模型一開始他plan說我要點這個東西點這個東西點這個東西就完成任務但是中間可能會有意想不到的狀況出現比如說彈出一個廣告視窗那如果語言模型只能夠按照一開始既定的規劃來執行行為的話他可能根本關不掉那個廣告視窗他就會卡住了所以語言模型也需要有一定程度的彈性他也要能夠改變他的計劃那 語言模型怎麼改變他的計畫呢也許一個可行的方向是每次看到新的observation之後都讓語言模型重新想想還要不要修改他的計畫看到observation 2之後語言模型重新思考一下從observation 2要抵達他最終的目標要做哪一些的行為那這一部分形成Plan Plan那把Plan Plan放到現在的input裡面把Plan Plan放到這個sequence裡面語言模型接下來在採取行為的時候可能就 會根據 plan point 來採取跟原來 plan 裡面原來所制定的不一樣的行為所以這個是讓語言模型做計劃不過這是一個理想的想法這是一個理想的 framework我們這邊就是相信語言模型有能力根據現在的 observation還有最終的目標制定一個規劃那語言模型到底有沒有這個能力呢其實你可能常常聽到這種新聞說語言模型它能夠做計劃啊比如說有一個人問語言模型說你定一個 成為百萬訂閱YouTuber的計劃那語言模型就會給你一個看起來還可以的計劃他說第一階段第一階段呢要先確定頻道的主題跟市場定位要做一下受眾的分析還有競爭對手的分析好第二階段目標是10萬訂閱要優化封面的縮圖要優化標題要下那種這個方法讓我賺了10萬的標題原來這個大家的tip都從這裡來的好然後影片開頭要黃金10秒利用懸念衝擊 編輯畫面問題引導讓大家願意看這個影片第三階段突然目標就是50萬訂閱了然後第三階段就是要製作高價值的內容然後做直播策劃系列然後接下來就百萬訂閱了組織團隊提高發佈頻率策劃大型企劃所以這個是語言模型成為百萬YouTuber的計劃然後這個時候很多奇怪的農場文就會跟你說有人按照了這個計劃就變成百萬YouTuber了反正就是這麼回事只有各式各樣的農場文告訴你說現在語言模型很強 你按照他的計劃執行你就變成一個很厲害的人就可以做出什麼很厲害的事情那過去確實也有很多論文告訴你說語言模型是有一定程度做計劃的能力的這邊引用的結果是一個2022年的論文哇這個也是史前時代的論文啦才是Chair GPT之前的論文啦在這篇論文裡面他們去告訴當時的語言模型跟他說現在有一個任務你把這個任務分解成一系列的步驟那如果語言模型可以正確的知道達成這個任務要做什麼樣步驟 那我們也許可以說他有一定程度的規劃能力比如說這邊試了一個叫做Codex 12B的模型跟他說如果要刷牙的話那你要做什麼事情呢他就會說我要走進浴室我要靠近那個水槽我要找到我的牙刷我要拿起牙刷我要把牙刷放到嘴裡面他知道刷牙要怎麼做那有了這些步驟以後呢在這篇文章裡面他們是拿這些步驟去操控一個agent那這個agent呢 就可以在虛擬的世界中做他們要這個agent做的事情比如說跟這個agent說去拿一個牛奶來喝他就會走進廚房打開冰箱拿一個牛奶再把冰箱關起來所以看起來好像有一定程度做計劃的能力那有人做了一個做計劃的benchmark這個benchmark就是考驗語言模型做規劃的能力那這個benchmark裡面最主要的測試題目是一個跟疊積目有關的題目這個題目的敘述呢通常講的是 是這個樣子告訴語言模型說你現在有哪些操作可以從桌上拿起積木可以從一個積木上拿起另一個積木可以把積木放到桌上可以把一個積木堆到另外一個積木上那現在初始的狀態像右邊這個圖這樣子那問說怎麼把橘色的積木放在藍色的積木上這邊要執行的動作就是把藍色的積木拿起來放到桌上然後再把橙色的積木拿起來放到藍色的積木上就結束了所以這個對 AI Agent 來說其實也都是蠻容易的 他知道說執行以下四個步驟就可以讓橙色的這個積木跑到藍色的積木上但是Plain Bench不是只做這種比較一般的疊積木的遊戲而已為什麼不能夠只做這種題目呢因為想現在這些語言模型他都從網路上爬大量的資料來進行訓練什麼疊積木這種題目網路上根本就已經有他搞不好根本就看過一模一樣的東西所以他能夠做計劃並不代表他真的知道做計劃是怎麼一回事 只是從他看過的資料裡面做照本宣科文字接龍出來一個看起來還不錯的結果而已這讓我想到說一個當兵的故事這故事就是有個司令官去一個軍營然後看到兩個小兵在守著一個長椅然後不讓任何人坐他就問說為什麼你們要守護這個長椅不讓任何人坐呢那個士兵說不知道前任司令官就是指示說一定要守護這個長椅所以這個軍營總是要派兩個人在長椅那邊站崗然後司令官就打給前任司令說為什麼要有人守護 要守護這個長椅呢然後前任司令官說不知道耶前前任司令官交代要守護這個長椅然後再問前前前任司令官也說不知道耶一直問到50年前一個已經超過100歲的司令官他說什麼那個長椅的遊戲還未乾嗎好他沒有聽懂算了就是這麼一個就是這麼一個故事就是會不會AI agent在做事情的時候他根本不知道他自己在幹嘛只是從某個地方網路上他過去的訓練資料看過一樣的東西他把一樣的東西拿出來給 給你看所以在Plain Bench裡面他們有一個比較變態的測試這個測試叫做神秘方塊世界這個方塊世界不是一個正常的方塊世界裡面的方塊可以做的行為是一些怪怪的行為比如說你可以攻擊方塊一個方塊可以吞噬另外一個方塊你可以屈服一個方塊一個方塊可以征服另外一個方塊然後接下來他就會訂一套非常複雜的規則然後根據這套規則去運作你可以達到某一個結果他最後要的結果是 讓物件C渴望物件A讓C方塊渴望A方塊那渴望是什麼意思不重要你就是按照前面那套規則操作看機器能不能讀懂前面那套規則按照那套規則操作讓物件C渴望物件A那這個時候語言模型期待他就不能用他看過的知識來解這個問題好那語言模型在這個神秘方塊世界做的怎麼樣呢這邊引用的是2023年的結果那最上面這個部分呢是當年那些模型在地球上 正常方塊世界的結果那這個數值呢是正確率所以看起來GPT-4可以得到30幾%的正確率那這邊是神秘方塊世界的結果在神秘方塊世界裡面呢你看這個GPT-4最好就算叫他做Channel SoulCOT就Channel Soul就算他叫Channel Soul也只有9%的正確率所以看起來他有點overfeed在一般方塊的世界上給他神秘方塊世界他是解不了的不過這是2023年這個是古代的 前年的結果我們來看去年9月有了O1以後的結果而有O1以後結果就不一樣了這邊一樣是神秘方塊世界縱軸是正確率橫軸是問題的難度那發現說多數的模型都躺在這個地方他們正確率都非常的低只有綠色的這個曲線有一點起色綠色的曲線是LAMA 3.1405B那個大模型他可以解最簡單的問題但是如果用O1 mini是紅 紅色這一條線用O1Preview是藍色這一條線看起來這些reasoning的模型是有一些機會來解這個神秘方塊世界的當然這邊你還是可能有一個懷疑就是神秘方塊世界會不會O1看過了呢會不會他訓練資料裡面根本就有神秘方塊世界的資料那這個我們就沒有辦法回答了只是說就現有這個Benchmark看起來O1是有機會解神秘方塊世界的好那還有另外一個跟做計劃有關的Benchmark這個計劃這個Benchmark呢 AI扮演旅行社然後你給他一個旅行的計劃叫他幫你規劃這個AI要讀懂你的計劃然後他可以使用一些工具他可以上網搜尋資料然後他會根據人提供給他的一些contract比如說經費多少預算多少一定要去哪裡一定不要去哪裡一定要做什麼一定不要做什麼然後根據common sense產生一個旅行的規劃這個是一個24年年初所發佈的Benchmark那AI AI要做的事情講得更具體一點就是他要讀一個問題這個問題裡面是說我要規劃一個三天的行程從某個地方到某個地方什麼時候出發什麼時候回來我的預算是1900元所以不能花超過1900元然後AI就要產生一個規劃說第一天我們搭哪一班飛機什麼時候從哪裡到哪裡早餐吃什麼午餐吃什麼晚餐吃什麼最後住在哪裡等等產生這個規劃然後要符合預算的限制那現在這在當時這個設計 24年年初啦當時的模型做的怎麼樣呢這邊是做了你看還有什麼GPT-3.5啊GPT-4啊等等的模型那又分成上半跟下半上半是這些模型要自己使用工具跟網路的資料互動然後得到正確的答案你會發現這些模型都非常的慘都慘成一團多數模型他的成功率就最後產生一個合理的旅遊規劃那個旅遊規劃是沒有完全沒有問題的機率是0%只有GPT-4 Turbo可以達到 可以得到0.6%的成功率那下面這個部分呢下面這個部分是說既然大家都那麼慘尤其是模型很多時候他根本用不了工具太笨了沒辦法用工具工具使用方法根本是錯的那沒關係就別用工具了把所有的資訊都先找好貼給模型讓模型根據這些資訊來做規劃那最好也只有GPT4 Turbo可以做到4%左右的成功率而已所以在24年年初那個時候看起來是沒辦法讓語言模型扮演一個旅行社 來幫你規劃旅遊行程的那我們來看這些模型會犯什麼錯吧那這個是從他們官網上這個花學的官網上找了幾個錯誤比如說模型可能會做一些沒有常識的事情在第三天這個飛機呢八點就已經起飛了但是還是安排了一些旅遊的行程還安排了午餐的地點這是一個不符合常識的規劃或者是有時候模型找不出一個好的規劃來符合預算的限制比如說這邊這個預算 預算的限制是3000元最多花3000元那模型第一次規劃的結果是3247元還差了一點所以模型就修改了原來的規劃他好像做了一些cost down午餐吃差一點的東西那降到3238元後來又想說那早餐也吃差一點的東西降到3216元只降這麼多他想說放棄算了好了跟3000元沒差那麼多就算了所以這個就不是一個成功的結果那這個作者有評論說其實只要降低住的地方不要做那麼好 就可以輕易的達到3000元以下的預算就可以符合預算的限制但是語言模型始終沒有發現這件事看起來它做規劃的能力並沒有非常的強它沒有辦法做一個規劃去符合限制那既然問題在沒有辦法符合限制有人就想說那符合限制這件事情就不要交給語言模型來做了交給一個現成的Solver來做所以語言模型做的事情是寫一個程式用這個程式去操控現成的Solver然後來得到合理的答案 的旅遊規劃那有了這個現成的SOLVER有這個工具的加入之後這SOLVER就等於這個工具那這個旅遊的規劃可以做到什麼地步呢去年4月的結果幾個月後有人用GPT-4跟Cloud Dream就可以做到90幾%的正確率所以看起來在有工具輔助以後語言模型也是有機會做出不錯的旅遊規劃的不過至少做出符合邏輯的旅遊規劃好所以現在到底模型規劃的能力怎麼樣呢就是介於有跟沒有間吧 就是你也不能說他完全沒有但你也不能說他真的非常強好那我們怎麼進一步強化這一些AI agent的規劃能力呢能不能夠讓他做的比他自己想出來的規劃還要更好呢一個可能是讓AI agent在做規劃之前實際上去跟環境互動看看今天在第一個observation的時候那看看現在有哪些可以執行的行為總共有1-1 1-2 1-3 三個行為哪個行為最好呢通通都去試一下1-1試一下 得到狀態二之一然後狀態二之一後面有兩個行為也都試一下狀態二之二之後有另外一個行為也試一下狀態二之三之後兩個行為也都試一下得到接下來的狀態然後看看有沒有成功的路徑報收一陣以後發現有成功的路徑這一條路徑是成功的那你就知道說那我要採取action一之三接下來要採取action二之三之一就會成功簡單來說就是要語言模型跟實際的環境互動一下報收一出一條最好的路徑 那這個就是一個很強的規劃的方式但是這麼做顯然是有很明確的弱點的第一個很明確的弱點就是報收如果今天這個任務很複雜報收所有的路徑顯然是要花費非常龐大的算力的你總不能語言模型每次下決策前都要報收所有的可能性吧雖然這樣可以找到最好的結果但是可能是不切實際的想法所以一個可能的想法是把一些看起來沒希望的路徑直接就丟掉比如說走到 某一個狀態的時候語言模型可以自問自答說走到這個狀態還有完成工的機會嗎那如果說沒有那這條路徑就不嘗試下去如果說有那才嘗試下去這樣就可以減少無謂的搜尋那這個方法有沒有用呢有一篇paper叫做Tree Search for Language Model Agent那這個是去年夏天的論文就做了類似的嘗試讓模型有使用電腦的能力這邊就是給模型一個指令可以讓模型有使用電腦的能力 叫他上網去做某一件事情那如果只是GPT-4做一般的這種直覺式的那種反射式的回答的話沒有辦法做得很好但是他們用這個報收加上去除沒機會的路徑的方式就先走這條路徑然後模型會不斷自問自答說這條路徑還有希望嗎然後給一個分數那如果分數低於某一個threshold就不做了就跳另外一個路徑低於某一個分數不做了再跳另外一個路徑 某個分數就不做了再跳另外一個路徑那最終找出一條最佳的路徑那模型就等於做了規劃那就可以走到最佳的結果這個是Tree Search for Language Model Agent你看這邊有各式各樣的這種Tree Search的algorithm你可以採用了這邊我們就不展開細講那這種Tree Search的方法有很大的問題什麼樣的問題呢它的缺點是有一些動作做完以後你是覆水難收沒有辦法回頭的比如說假設現在在語言模型可以採取的三個action裡面 有一個是訂披薩有一個是訂便當然後呢他先訂了便有一個他先訂了披薩以後繼續走下去發現這條路不好所以他最後發現訂便當才是最好的解決方案但是你披薩已經訂了你打電話去跟人家說我不要訂這個披薩了那個披薩哈他已經把那個披薩做了他說誰管你啊你一定要把這個披薩吃下去有些動作做了以後就是覆水難收所以這樣的Tree Search的方法跟現實世界互動找出最佳途徑的方法也有可能有問題的那怎麼處理這個覆水難收 一個可能性就是讓剛才一切的嘗試都發生在夢境中都發生在腦內的劇場剛才一切的互動都不是現實生活中真正發生的事情原來都是模型腦內的模擬他自己想像說他執行了action一之一他自己想像說接下來會看到observation二之一他在自己想像去評量這個路徑有沒有希望發現沒有就換搜尋另一條路徑直到達 達到他想像中的一個理想的結果但這邊還有另外一個問題從action到observation從模型執行的行為到他看到接下來環境的變化這中間的過程不是模型決定的他實際上是環境決定的那模型怎麼知道環境會有什麼樣的變化呢模型怎麼知道我採取一個行為接下來會看到什麼樣的改變你在跟一個對手下棋的時候你怎麼知道你下一步棋接下來會發生什麼樣的事情對方會有什麼樣的反應 策略的回應呢所以你需要有一個WAR MODEL如果是在alphaGo下棋裡面他就是自己扮演對手自己跟自己下那在這邊的情況在這個AI agent的情況你就是需要一個WAR MODEL他模擬環境可能會有的變化那WAR MODEL怎麼來呢也許AI可以自問自答自己扮演這個WAR MODEL自己去猜想說他執行了某一件事以後接下來會發生什麼樣的行為好這件事有機會成真嗎你可以讀一篇paper Is your LLM secretly a world model of the internet?這篇paper就是用model-based planning的方法來打造一個web agent這篇paper裡面的解法是現在有一個網頁模型的這個任務目標呢是要買某一個東西那有三個選項有三個東西是可以點的接下來黃色這個區塊一切所發生的事情都是發生在腦內的劇場都是發生在模型的夢境中它並沒有實際發生模型想像一下我點