# 笔记总结

-  现有Key-Value编码模式的局限性在于其可数性，无法应对超越初始规定范围的情况。
-  理解语言的关键在于模型能否在超越经验的情况下生成合理内容，这需要找到将真实语言token投射到潜空间的方法（降维嵌入矩阵）。
-  Word2Vec模型的目标是得到嵌入矩阵，而非完成特定任务，其训练方法与传统机器学习模型不同。
-  Word2Vec的编码和解码原理：输入token经矩阵编码成词向量，再解码回token。训练的关键在于损失函数，通过比较解码结果与原始token的差异修正参数。
-  CBOW方法：输入相邻的几个token，将其词向量相加，解码得到中间缺失的token。通过损失函数比较解码结果与缺失token，修正参数。此方法原理类似于物理学中的力的合成与分解。
-  模型训练的目标是获得体现语义的嵌入矩阵，而非预测特定答案。即使训练数据包含多种情况（如红苹果、绿苹果、甜苹果），模型也不一定能预测出“这是一个什么苹果”的答案，但可以保证“红”、“绿”、“甜”在潜空间中语义接近。

- Word2Vec 类似于词典，通过其他词项量合成目标词项量，生成词向量，客观表达词义，不依赖作者主观意图。
- Skip-gram 是 CBOW 的逆过程，两者均可进行自监督学习。
- Word2Vec 的实现通常使用一层隐藏层的神经网络，其权重矩阵W和W'可能独立学习，而非解析求解，这可能与矩阵求逆的计算复杂度有关。
- Word2Vec 主要训练词向量，不足以理解词与词组合后的语义。
- Transformer 模型包含编码器和解码器，以机器翻译为例，输入需先转化为词向量。
- Transformer 的核心是注意力机制，它处理词与词组合后的语义。

---

# 原始语音转文字

所以说这就没法应对超越首次规定之外的情况了即便是key value这个表是无穷的那它也有一个问题它是可数级它的式是阿列复0和自然数是等式的而不是实数的阿列复1所以说我觉得这一点才是决定编码模式是否真正理解了语言的有力证据因为只有理解了才能在超越经验的情况下生成出合理的内容明白这一点之后那我们接下来考虑的就是一些具体的问题了就是如何才能找到把真实语言里的token投放到潜空间的方法也就是怎么才能找到实现降维的嵌入矩阵这肯定是用机器学习的方法了不可能人工去设定关键是这个token的语言 具体的机器学习的方法应该怎样做呢谷歌在2013年提出的word2vec就是一个其中的方法这个方法比较特别的是它的目标和我们平时了解机器学习模型的目标不太一样通常我们了解的机器学习它们的模型是什么呢就是在训练好了之后是希望模型可以完成某个任务就比如说我说一个照片你得能知道它到底是狗还是猫而word2vec这个模型的目标它希望得到的是嵌入矩阵也就是说它的目标不是模型的结果而是模型的参数 就是要把作家給培養出來是希望他能按照你的要求去寫出相應的文章的而word2vec它的目標更像是編詞演編好了給作家去用這兩個目的是不一樣的這兩個目的的不同帶來的最直接的差別就是word2vec它裏面不需要記錄函數它會計算起來更簡單具體什麽意思呢我們一步一步來看編碼和解碼的原理是這樣的你輸入一個token經過一個矩陣編碼成了詞項量然後詞項量你又可以解碼回去再變成token如果你的參數沒問題你解碼回去這個詞和之前應該是沒有差別的用這個思路去訓練模型行不行的看起來是可以但其實是沒有辦法去訓練因為訓練它一定是要有一個 经过模型计算出来一个结果这个结果和正确答案是有差异的然后才能把这个差异去反向传播回去去修正参数如果是我刚才说的那种情况那根本就不能训练因为不论输入的向量是什么只要前面和后面两个矩阵它们是一个违逆关系你要是相乘之后它们是一个单位矩阵那么输入和输出它就一定是相等的所以说要想真正的完成训练还需要在这个思路上做一些调整具体的方法就是谷歌论文里提到的有两种一个是COBO一个是SKIPGRAND先说COBO它的原理是这样的 你负责的所以你可以准备你自己喜欢的比如说你可以添加自行构图并选择你喜欢的工作不需要人帮助输入的不再是一个token而是准备一组基数个的token假如说是五个token然后把中间的这个给拿掉剩下四个分别与同一个嵌入句子相成把它们变成前空间里的词项量之后再把这四个项量加在一起合成一个项量然后再对这个合项量进行解码这个时候损失函数就会定量的去看这个合项量解码后得到的那个token挖掉的那个中间token 是不是一样的如果不一样那就需要去修改参数但是这里有些细节需要注意我们后面还会再去细说我们先来解释一下这里这个原理为什么把上下四个单词的项量加一起就应该得到中间那个词的项量呢其实只需要大家去调动一下高中物理学受力分析的基因把这个问题和力的合成力的分解对应起来就可以比较容易的理解了如果只能从文本中去理解一个token的语义那就能且只能根据这个上下文去进行判断去理解反过来有了上下文也应该可以推断出缺失了那个token的语义 把已知的磁相量看作是分粒中间缺的那个token所对应的磁相量看作是已知分粒的合力这应该挺合理的对吧缺失的那个token它既然能被上下文决定那么这个token所对应的磁相量那它应该就是已知的磁相量的合力反过来也是一样中间的这个token磁相量如果做相量分解的话分解出来的分量也应该可以对应到上下文的磁相量上当然你也可能会说这里是不是有点问题就比如说有这样一段话这是一个空格苹果那这个空格到底是甜什么呢可以是红可以是绿可以是甜可以是便宜这都是正确的它预测出来到底是什么首先这是因为上下文 训练的数据不够多再者这里训练的目的不是为了让模型具备完形填空的预测能力而是让它能够训练出体现语意的嵌入矩阵即便是训练的时候训练的数据里面同时有这是一个红苹果这是一个绿苹果这是一个甜苹果下次你再输入这是一个什么苹果的时候它仍然没有办法给出一个你希望的答案但是也没关系因为给出正确答案这个根本就不是这个模型的目的重要是经过训练了之后在这个模型的潜空间里面红绿甜这几个token的语意一定是一种比较接近的关系至少从我们来说它们都是形容词我们前面不是比喻What to act 更像是编词典吗你想想词典里面的一个词被注解出来有好几个解释这很正常对吧这是一样的道理当然 world2vec只是提供了一个对语意的最初的理解它训练完成之后体现的是单个token之间的联系也就是前面说的它就起到一个词典的作用词典会用其他词去解释目标词world2vec它生成的潜空间就是在用其他的词项量去合成目标词项量得到的词项量从这里我想大家也能看出来了world2vec这种形式这个潜空间里面词项量对应的词意它是不依赖于作者主观意图的它是一种客观的表达这个客观性是和整个目标有关的 语言环境绑定在一起而一个作者根据自己的主观意图把许多词汇组成在一起这个时候才具有了主观性才能体现出不同人想表达的不同内容而这个主观性就体现在作者选择了不同的词按照不同的顺序进行的组合是这个不同词和不同顺序在体现主观性而要想让模型理解这部分体现作者主观性的语意那就不是well2vec的责任了这其实就是后面注意力机制需要做的事情这是Kobold原理知道了他skipgram也就好理解了他其实就是把Kobold原理反过来用以至于一个token根据它的词项量去求出上下文对应的token的分量来看看是否能够 是不是和训练数据一致其实大家也能看出来这两种方法都是可以自监督学习的不需要人文的去打标签只要给一个文本就可以用程序自己挖掉一些空自己去训练这是他们的原理啊具体实现的时候还是有些细节的我在这虽然用的是矩阵和向量这种数学方式来表示的但是在真正建立模型的时候往往还是会把它看作是一个只有一层隐藏层的神经网络去操作的如果这个神经网络里面没有偏执系数B也没有激活函数如果单纯从我刚才解释的那个原理去看 这样的话隐藏层两边其实是一个逆过程也就是说W和W'它们相乘之后应该是得到一个单位矩阵因为这是一个降维和升维的过程所以它们行列数肯定不一样也就是说它们是一个美逆的关系它们不是一个标准的逆矩阵总之就是两个矩阵只要知道了一个另一个是可以直接通过解析解求出来的不需要去进行学习不过但是据我了解在具体实现这个模型的时候模型里面W和W'还就是两个独立的矩阵它们分别去进行学习和训练也就是说它们会在反向传播进行训练的时候各学各的我猜测这里可能和求矩阵的逆这个计算的复杂度太高了有可能 如果单纯的是把T2向后传播这个计算复杂度大概是大欧N这样一个量级的但是矩阵求逆呢这就是一个大欧N的三次方这样一个量级的复杂度了所以不严格的话那还是通过T2下降法去进行训练会来的更简单一些还有就是隐藏层和输出层的这个神经元它是没有计和海数的因为根据我们上面的解释它其实这里做的就是一个典型的向量求和向量分解所以这里根本就没有分线性的需求因为变换前和变换后它的空间都是相同的嘛这是标准的Word2Vec的方法这种方法重点就是训练出一本词典也就是说这里训练出来的嵌入矩阵W只是针对单个词典 如果想把一个一个的词组成有准确含义的一句话那靠这种简单的模型就不够了包括前面的介绍我们现在对编解码和词项量有了一个基本的了解了这些都算是铺垫现在我们终于可以正式开始介绍Transformer了还是这张图按照现在大家公认的理解这个图上左边那部分就是编码部分右边的就是解码部分前面也提到了现在的各大模型都是为了适应某个特定的需求对这个结构进行了一些变化和优化而得来的现在我们如果就是针对这个结构去介绍的话接下来为了大家理解方便还是用机器翻译作为例子去讲会更容易一点在这个图上最下面有输入和输出就相当于 左边输入中文 它会自动翻译成英文从右边输出当然这是已经训练好进行推理的情况如果是为了训练那么左边和右边都是一种输入分别把中文和英文与相同的一些语料输入到模型里面最后输出的是这个图上最上面那部分也就是损失函数在上面通过损失函数的值再进行反向传播去修改里面的参数然后再细看一下还会发现下面这两个输入都是需要先转化成词降量然后再去进行后续的操作的也就是说这一层其实已经有一个嵌入矩阵了这个嵌入矩阵也不是写死的它也是需要在训练的时候进行调整参数的 前面不是说过吗这部分就相当于准备好词典把单个token它这个词译都查出来词和词组合之后的语义是需要在现在的基础之上再去进一步分析和理解的我也不卖关子了对词和词组合后的语义进行理解靠的就是注意力机制就是图上橙色的那一部分这就是transformer的核心了剩下的其他部分虽然也重要不过不是核心所以我们接下来就先介绍什么是注意力怎么理解注意力对注意力有了一个基本的了解之后再理解其他部分就会容易了说到什么是注意力机制最直接的方法就是看这张图注意主词项量然后经过三个句子相乘之后分别会得到QQ 然后他们再进行一顿运算最后还是会输出一组词项量值得注意的是在注意力机制这里如果每次只是输入一个词你要计算也的确是可以但是这样的话就体现不出注意力机制它的价值前面说了词嵌入已经解决了单个词单个token语意的问题了注意力机制要解决的就是许多词组合在一起之后整体体现出来的那个语意你只有把一句话里多个词同时输入到模型里面前面说的那一点才能体现出来所以接下来讲解输入部分就不考虑只输入一个词的情况了而是考虑输入一组词的情况这个时候这组词项量就组成了一个数据矩阵 输入的是一个T行的矩阵输出它也是一个T行的矩阵至于输出的列数也就是一个磁相量它的维度的个数我们把token变成磁相量就像是把大象变成石头方便后面的各种操作对石头来说比较方便的操作就是切割搬运称量磁相量方面的操作那是和空间变换有关的各种矩阵和向量运算了所以说经过注意力机制之后把原来的磁相量进行一些升维和降维这些操作就再正常不过了WQ WK WV这三个矩阵按照注意力机制的要求输入的磁相量矩阵都需要先和这三个矩阵相乘之后才会得到QKV先不管它们具体的功能是什么只要是完成这样一个相乘 它们至少是可以起到空间变换作用的输出的矩阵是T行D音力WQ WK WV三个矩阵是D音行Dout力这个Dout就决定了输出的磁相量是多少力量也就是有多少个维度具体D音是要大于Dout还是小于就需要根据具体的情况而定了在模型里面不做具体的要求磁相量矩阵和这些W矩阵相乘之后得到的QKV这其实没有什么特别的作业力机制里面最值得关注的其实是得到QKV之后后续的操作我先说一下这里的计算规则然后再慢慢解释其中的含义得到QKV之后先把K进行转制然后让Q和K的转制相乘当然这部分也可以是