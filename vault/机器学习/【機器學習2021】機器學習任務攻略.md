---
share_link: https://share.note.sx/5giux0j7
share_updated: 2025-09-11T16:42:34+08:00
---
# 笔记总结
![[Pasted image 20250911153340.png]]
-  课程讲解每个作业的通关策略，各作业结构相似。
-  作业包含训练数据（X和对应的Y：X1-Y1, X2-Y2...XN-YN）和测试数据（只有X）。
-  作业二（语音识别）：X是小段声音信号，Y是预测该信号对应的“风铃”（类似音标）。
-  作业三（图像识别）：X是图片，Y是图片中包含的物体。
-  作业四（说话人识别）：X是声音信号，Y是说话人身份，应用于银行客服身份验证。
-  作业五（机器翻译）：X是一种语言的句子，Y是翻译后的句子。


![[Pasted image 20250911153359.png]]
- 训练模型的过程包含三个步骤：
    - 写出包含未知参数（θ）的函数 f(x)，其中 x 为特征。
    - 定义损失函数 (loss function)，用于评估参数 θ 的好坏。
    - 通过优化问题找到使损失函数值最小的参数 θ*。
- 将 θ* 代入模型，使用测试数据进行预测，并将结果上传到目标。
- 直接运行示例代码通常只能达到简单的基线结果。
-  为了获得更好的结果，需要进一步的优化策略。

![[Pasted image 20250911153424.png]]
- 提升作业表现的攻略，如同游戏中的开局策略。
- 首要步骤：==检查训练数据（training data）的损失（loss），而非直接关注测试数据（testing data）的损失。==
- 训练数据损失过大，表示模型在训练数据上学习效果不佳。
- 训练数据学习不佳的两个可能原因：模型偏差==（model bias）==。

![[Pasted image 20250911153436.png]]

- 模型偏差指的是模型过于简单，无法找到使损失函数降低的函数。
- 模型包含未知参数，不同的参数值会产生不同的函数，但这些函数的集合可能太小，不包含能够降低损失的函数。
- 即使找到集合中损失最低的函数，但由于该函数集合本身就无法降低损失，所以效果仍然不好，如同在没有针的海里捞针。
- 解决方法是==重新设计模型，增加模型的弹性，例如增加输入特征的数量（例如从使用前一天的信息改为使用56天前的信息）或使用深度学习。==

![[Pasted image 20250911153501.png]]
- 提升模型弹性方法：增加特征，使用更大模型，或运用深度学习。
- loss值大并不一定代表模型偏差，也可能是==优化方法问题==。
- 主要优化方法为梯度下降法，其缺点是可能陷入局部最小值，无法找到loss值更低的参数。
- 图像化解释：模型可表示函数的集合是一个蓝色集合，其中包含一些loss值低的函数，但梯度下降法无法找到这些函数。

![[Pasted image 20250911153513.png]]
- SEDA STAT 显示模型损失函数值不够低。
- 存在损失函数值低的函数，但梯度下降法未能找到它（大海捞针的比喻）。
- ==问题在于模型偏差（Model Bias）还是优化算法问题（Optimization）。==
- 讨论了模型弹性（Model Capacity）是否足够的问题：模型中是否存在低损失函数（海里是否有针），以及优化算法（梯度下降）是否有效（能否捞到针）。
- 建议通过比较不同模型来判断模型大小是否足够。

![[Pasted image 20250911153548.png]]
-  一个20层和一个56层的网络在测试集上的loss比较，56层网络的loss高于20层网络。
-  早期对该现象的误解：认为是过拟合(overfitting)，表明深层网络无效。
-  实际情况：训练集上56层网络的loss高于20层网络，说明56层网络的优化(optimization)过程存在问题，而非过拟合或模型偏差(Model Bias)。
-  56层网络的容量(弹性)大于20层网络，如果优化成功，其loss应该低于20层网络。  因此，问题在于优化算法的效率。
![[Pasted image 20250911153609.png]]

- 如何判断优化是否有效：尝试先使用简单的模型（例如==线性模型或支持向量机==），这些模型更容易优化，不易出现优化失败的情况。  先了解简单模型能达到的损失值，再训练复杂的深度模型。
- 深度模型的损失值若高于简单模型，则表明优化存在问题（梯度下降效果不佳）。
- 案例：观看人数预测中，四层网络损失为0.10k，但五层网络损失反而增高至0.34k，这并非模型偏差问题，而是优化问题。

![[Pasted image 20250911153635.png]]
- 训练损失大可能是模型偏差(model bias)或优化失败(optimization failure)。
-  下一节课将讲解优化失败的解决方法。
- 训练损失小，测试损失也小，则训练结束。
- 训练损失小，测试损失大，则可能出现过拟合(overfitting)。
- 判断过拟合需先确认训练损失小，再观察测试损失是否远大于训练损失。
-  许多同学忽视训练损失，应先检查训练损失以确定优化和模型大小是否合适。
-  训练损失小而测试损失大是过拟合的可能原因，后续将用极端例子解释。

![[Pasted image 20250911153710.png]]

- 训练数据导致机器学习模型找到一个无用的函数。
- 此函数的工作方式：若输入X存在于训练数据中，则输出对应的Y；否则，输出随机值。
- 此函数在训练数据上的损失为0，但在测试数据上的损失很大。
- 这是一个极端的例子，说明模型可能在训练数据上表现良好，但在测试数据上表现很差，因为它实际上没有学习到任何东西。

![[Pasted image 20250911153721.png]]
- 输入特征X和输出标签Y是一维的，两者关系为二次曲线。
- 该曲线无法直接观察，只能观察到从曲线上随机采样的训练数据点。
- 模型能力强，灵活性大，只根据少量训练数据点拟合曲线时，在未训练数据区域会产生各种结果（过拟合）。
- 测试数据与训练数据来自同一分布，但模型在训练数据上拟合良好的曲线，在测试数据上的表现不一定好。


![[Pasted image 20250911153804.png]]

- ==高自由度模型可能导致过拟合==（在训练数据上表现良好，但在测试数据上表现不佳）。
- 解决过拟合的两个方向：
    - 增加训练数据（最有效，但在作业中不允许）。
    - 数据增强（Data Augmentation）：利用对问题的理解创造新的数据，例如图像识别中的左右翻转或局部放大。  数据增强需谨慎，需符合数据特性和问题理解。  不合理的增强（例如图像上下颠倒）可能导致模型学习到错误信息。

![[Pasted image 20250911153817.png]]
- 解决模型过拟合的第二种方法：==限制模型的灵活性==。
- 通过限制模型为特定类型的函数（例如二次曲线），减少模型的复杂度。
- 这种方法限制了模型可以选择的函数空间，即使训练数据有限，也能更好地拟合真实数据分布，从而在测试数据上获得更好的结果。
- 模型的约束程度（constraint）取决于对问题的理解和模型设计。不同的模型设计会导致不同的结果。


![[Pasted image 20250911163344134.png]]

- ==过拟合与模型限制并非同一概念==。
- ==模型限制过大导致模型偏差（model bias），而非过拟合。==
- 限制过大的例子：假设模型必须是线性模型（y=a+bx），则无法拟合非线性数据，导致测试结果差。
- 模型复杂度与偏差之间存在矛盾关系：增加模型复杂度提升了模型灵活性，但过分限制模型也会产生偏差。


![[Pasted image 20250911163344145.png]]


- 本节课未对模型复杂度和弹性给出明确定义，仅作概念性描述。
- 下节课将讲解如何衡量模型复杂度和弹性。
- 直观理解：复杂模型包含更多函数和参数。
- 复杂模型的训练损失会随着模型复杂度增加而降低。
- 但测试损失会先下降后上升，超过一定复杂度后会暴增（过拟合）。
- 目标是选择一个适中复杂度的模型，在训练和测试数据上都能获得最低损失。


![[Pasted image 20250911163344153.png]]


- 直接根據 Kaggle 分數選擇模型可能導致過擬合，因為模型可能只是在公開測試集上表現良好，而在私有測試集上表現很差。
-  一個極端例子：即使使用一兆個隨機模型，總會有一個在公開測試集上表現出色，但這並不代表該模型有效。
-  僅根據公開測試集分數（Leaderboard）選擇模型，可能導致在私有測試集上表現極差，排名大幅下降。
-  公開和私有測試集的設置，可以避免僅根據公開測試集表現選擇模型所帶來的風險。
-  以往只使用私有測試集評分的學期，也發生過類似情況，導致參賽者排名大幅下降和情緒低落。

![[Pasted image 20250911163344162.png]]

- 使用Public和Private測試集的原因：若只有Public測試集，則模型可以通過隨機產生輸出或過擬合Public數據獲得高分，這沒有意義。

- Benchmark數據集的問題：公開的Benchmark數據集（例如Libris Speech）的測試結果是公開的，這導致即使很差的模型也能通過多次嘗試獲得好的結果，從而夸大模型的性能，例如2016年一些語音辨識模型聲稱超越人類的表現，實際上是在特定Benchmark數據集上達到的。

-  現實應用與Benchmark結果的差距：在Benchmark數據集上表現良好的模型，在實際應用中未必有同樣好的表現。

-  不當操作案例：一些公司通過操縱數據或使用外部API來達到KPI指標，例如清除測試集中的雜訊或秘密使用Google API。

-  總結：分割Public和Private測試集的重要性在於避免過擬合和確保模型在真實環境中的有效性，避免Benchmark數據集的誤導性結果。
![[Pasted image 20250911163344179.png]]


-  为了避免过度依赖Public Set结果，建议将训练数据分成training set (90%) 和 validation set (10%)。
-  根据validation set上的分数选择模型，再上传到Public Set测试。
-  选择validation set分数最高的模型，以此来减少在Public Set上表现好但在Private Set上表现差的风险。
-  限制模型上传次数，避免过度调整模型以适应Public Set。
-  不要过度关注Public Set的排名，因为排名靠前者容易掉落。
-  理想情况下，仅使用validation set选择模型，上传后不再调整。
-  虽然实际操作中很难完全忽略Public Set结果，但应尽量减少对它的依赖。



![[Pasted image 20250911163344189.png]]

- 如何划分训练集(Trending Set)和验证集(Validation Set)：建议使用N折交叉验证(N=3)。
- N折交叉验证过程：将数据分成N份，每次选择一份作为验证集，其余作为训练集，重复N次。
- 模型选择：将不同模型在N折交叉验证中运行，平均每个模型在不同设置下的结果，选择结果最好的模型。
- 最终训练：用选择的最佳模型在整个训练集上进行训练，然后在测试集上进行最终评估。
- 此方法适用于课程前期，用于解决模型训练中验证集划分不佳可能导致结果偏差的问题。

![[Pasted image 20250911163344199.png]]

![[Pasted image 20250911163344208.png]]

- 使用Model 1进行M4交叉验证，先在全部交易集上训练，再用测试集测试。
- 上周预测2月26日观看人数的结果很差，预测值与真实值相差2.58K。所有模型预测都失败，2月26日实际为峰值，而模型预测为低点。
- 模型失败的原因是数据分布不匹配(Mismatch)，这与过拟合(Overfitting)不同。过拟合可以通过收集更多数据解决，但数据分布不匹配则不能。
- 数据分布不匹配是指训练数据和测试数据分布不同，增加训练数据也无济于事。
- COVID-19作业中，使用2020年数据训练，2021年数据测试，结果很差，因为两年的数据分布不同。最终使用了不同的数据分割方法。


---

# 原始语音转文字

好 那我們就繼續來上課吧那接下來的課程要講什麼樣的內容呢接下來要告訴你每一個作業通關的大戰略通關的攻略長什麼樣子那我們已經看了作業1了那其實之後好幾個作業它看起來的樣子基本上都是大同小異就是你會有一堆訓練的資料那這些訓練資料裡面會包含了X跟Y的片你會有X1跟它對應的Y1X2跟它對應的Y2然後XN還有它對應的YN然後測試資料呢測試資料就是你只有X沒有Y那剛才大家已經看了作業1了其實在之後 每幾個作業看起來都是非常類似的格式比如說作業二其實是做語音辨識那我們的X就是非常小的一段聲音訊號那其實這個不是真正的完整的語音辨識系統它是語音辨識系統的一個閹割版X是一小段訊號那Y是要去預測 需要去判斷說這一小段聲音訊號它對應到哪一個風鈴那你不知道風鈴是什麼沒有關係你就把它想成是黑黑音標就可以了那作業三是要做影像辨識那這個時候我們的X是一張圖片那Y是機器要判斷說這張圖片裡面有什麼樣的東西而作業四是語者 那語者辨識是要做什麼事情呢語者辨識要做的事情是這個X也是一段聲音訊號那Y現在不是封鎖Y是現在是哪一個人在說話可以想像說這樣的系統現在其實非常的有用如果你打電話去銀行的客服那現在都有自動的語者辨識系統那會聽說現在打電話進來的人是不是客戶本人那就少了這個客服人員問你身份驗證的時間那中業5是做機器翻譯X就是某一個語言比如說這是我文藝會的一句日文欸 伊塔米諾西勒那他的Y呢就是另外一句話 現在你在留言區裡面就可以洗一些諸葛春夫之類的那訓練資料拿來做什麼呢訓練資料就是要拿來訓練我們的model訓練model的過程上週已經講過了訓練的過程就是三個步驟第一個步驟你要先寫出一個有未知數的function那這個未知數以後我們都用theta來代表一個model裡面所有的未知參數所以ftheta的意思就是說我現在有一個function叫f of x那它裡面有一些未知的參數這些未知的參數表示成theta 它的input叫做x這個input叫做feature那接下來你要訂一個東西叫做lossloss是一個function這個loss的輸入就是一組參數然後去判斷說這組參數是好還是不好那接下來你要解一個optimization problem你要去找一個theta那這個theta可以讓loss的值越小越好可以讓loss的值最小的那個theta我們就寫做theta star那有了theta star以後那你就把它拿來用在測試資料上也就是你把theta star代入這些未知的參數本來是theta的theta star 裡面有些未知的參數現在這個SELA呢用SELA STAR來取代用SELA STAR來取代那它的輸入呢就是你現在的測試資料那輸出的結果你就把它存起來然後上傳到Target就結束了但接下來你就會遇到一個問題那直接執行註調的Sample Code往往只能夠給你過Simple Baseline的結果而已如果你想要做得更好那應該要怎麼辦以下就是如何讓你做得更好的攻略它是只能夠給你過Simple Baseline的結果而已如果你想要做得更好那應該要怎麼辦 以下就是如何讓你做得更好的攻略他適用於前期所有的作業這個就跟魔關羽一樣你知道嗎開局救獸可以幫助你打贏前期所有的副本好那這個攻略是怎麼走的呢從最上面開始走起第一個是你今天如果你覺得你在cargo上的結果不滿意的話第一件事情你要做的事情是什麼檢查你的training data的loss有的人說欸我在意的不是應該是testing data的loss嗎因為cargo上面的結果呈現的是testing data的結果啊但是你要想 要先檢查你的training data看看你的model在training data上面有沒有學起來再去談testing的結果所以你要先檢查一下training data的漏失如果你發現你的training data的漏失很大顯然他在訓練資料上面也沒有學好那接下來就要分析一下在訓練資料上面沒有學好是什麼樣的原因那這邊有兩個可能第一個可能是model的bias那model的bias這件事情呢我們在上週已經跟大家講過了所謂model bias的意思是說假設你的model太過簡單舉例來說我們現在寫了一個 一個有未知parameter的function這個未知的parameter我們可以帶各種不同的數值你帶set1得到一個function我把那個function用這個一個點來表示你帶set2得到另外一個function你把所有的function集合起來得到一個function的set但是這個function的set它太小了這個function的set裡面沒有包含任何一個function可以讓我們的loss變低可以讓loss變低的function不在你的model可以描述的範圍內你的model裡面有未知的參數未知參數可以帶任何的數值把這些數值帶進去以後你得到了一個function的set加入不同的數值 你只得到不同的function把所有function集合起來你得到一個function的set那這個set裡面沒有任何一個function可以讓你的loss變低那在這個情況下就算你找出了一個set a star他是這些藍色的function裡面最好的那一個他是那個藍色的function裡面可以讓loss最低的那一個也無濟於事這些都是魯蛇他只是魯蛇裡面的霸主就還是一個魯蛇那個loss還是不夠低那這個狀況就是哇這個你想要在大海裡面撈針這個針指的是一個loss低的function結果針呢根本就不在海裡所以白忙一場你怎麼撈都撈不出針你怎麼撈都撈不出針 根本就不在你的這個function set裡面不在你的這個大海裡面所以怎麼辦這個時候重新設計你的model怎麼重新設計給你的model更大的彈性我們上週已經示範過舉例來說你可以增加你輸入的feature我們上週說本來我們輸入的feature持有前一天的資訊假設我們要預測接下來的這個觀看人數的話那我們用前一天的資訊不夠多那用56天前的資訊那model的這個彈性就比較大了那你也可以用deep learning增加更多的彈性所以如果你覺得你的model的彈性 彈性不夠大可以增加更多feature可以設一個更大的model可以用deep learning來增加model的彈性這是第一個可以的解法但是並不是training的時候loss大就代表一定是model bias你可能會遇到另外一個問題這個問題是什麼這個問題是optimization做得不好什麼意思呢我們知道說我們今天用的optimization在這門課裡面我們其實都只會用到gradient descent這種optimization的方法這種optimization的方法有很多的問題舉例來說我們上週也講過說你可能會卡在local mean 你沒有辦法找到一個真的可以讓Loss低的參數那如果要圖具像化的方式來表示的話就像是這個樣這個是你的model它可以表示的函式所形成的結合你可以把Seda代入不同的數值形成不同的function把所有的function通通集合在一起得到這個藍色的set這個藍色的set裡面確實包含了一些function這些function它的Loss是低的但問題是Gradient Descent這一個演算法沒辦法幫我們找出這個Loss低的functionGradient Descent說你要我幫你解Optimization的problem我給你這個Seda上 然後就結束了但這個SEDA STAT給我們Loss不夠低這個Model裡面存在著某一個Function它的Loss是夠低的但歸點Descent沒有給我們這一個Function好 這就好像是說我們想大海撈針針確實在海裡但是我們卻沒有辦法把針撈起來但這邊問題就來了我們今天看到Trending Data的Loss不夠低的時候到底是Model Bias還是Optimization的問題呢今天我們發現說我們找不到一個Loss低的Function到底是因為我們的Model的彈性不夠我們的海裡面沒有針還是說我們的Model的彈性不夠我們今天就來講到這裡謝謝大家我們下期再見 我們的model彈性已經夠了只是Optimization gradient descent不給力他沒辦法把針撈出來到底是哪一個呢到底我們的model已經夠大了還是他不夠大了怎麼判斷這件事呢這邊一個建議的判斷的方法就是你可以透過比較不同的模型來得知說你的model現在到底夠不夠大怎麼說呢我們這邊舉個例子那這個實驗是從Residual Network那篇paper裡面揭露出來的我們把paper連結放在右上角這篇paper一開頭就跟你講了一個故事 我想去兩個network一個network有20層一個network有56層那我們把它們測試在測試資料上那這個橫軸是指的是training的過程啦就是你參數update的過程那隨著參數的update當然你的loss會越來越低但是結果20層的loss比較低56層的loss還比較高那這個residual network是比較早期的paper2015年的paper啦如果你現在大學生的話那個時候你都還是高中生而已所以那個時候大家對deep learning我覺得了解還沒有那麼透徹大家對deep learning有各種奇怪的誤解很多人看到這個 這張圖就會說這個代表什麼這個代表over fitting就告訴你deep learning不work知道嗎56層太深了不work根本就不需要那麼深那個時候大家也不是每個人都覺得deep learning是好的那時候還有很多對deep learning的質疑所以看到這個實驗有人就會說所以深沒有比較好這個叫做over fitting但是這個是over fitting嗎這個不是over fitting等一下會告訴你over fitting是什麼並不是所有的結果不好都叫做over fitting你要檢查一下訓練資料上的結果你檢查訓練資料結果發現說現在20層的內窩跟56層的內窩比起來在訓練資料上20層的內窩的loss其實是比較大的 你50層的Naver的Loss是比較高的這代表什麼這代表56層的Naver他的Optimization沒有做好他的Optimization不給力你可能問說你怎麼知道是56層的Optimization不給力搞不好是Model Bias搞不好是56層的Naver他的Model的彈性還不夠大要156層才好56層也許彈性還不夠他但是你比較56層跟20層20層的Loss都已經可以做到這樣了56層的彈性一定比20層更大對不對如果今天56層的Naver要做到20層的Naver可以做到的事情對他來說是輕而易舉的他找前20層的參數 跟這個20層的Naver一樣剩下36層就什麼事都不做identity copy前一層的輸出就好那56層的Naver一定可以做到20層的Naver可以做到的事情所以20層的Naver都已經可以走到這麼低的Loss了56層的Naver他比20層的Naver的彈性還要更大所以沒有道理20層的Naver可以做到的事情56層的Naver做不到啊所以56層的Naver如果你Optimization成功的話他應該要比20層的Naver可以得到更低的Loss但結果在訓練資料上面沒有這個不是Open fitting這個也不是Model Bias因為56層的Naver彈性 是夠的這個問題是你的這個optimization不給力optimization做得不夠好所以剛才那個例子就告訴我們說你怎麼知道你的optimization有沒有做好呢這邊給大家加的建議是看到一個你從來沒有做過的問題也許你可以先跑一些比較小的比較淺的內窩或甚至用一些不是deep learning的方法比如說linear的model比如說support vector machine有一些方法比如說support vector machine那不知道是什麼也沒有關係啦那他們可能是比較容易做optimize他們比較不會有optimization失敗的問題 也就是這些model他會竭盡全力的在他們的能力範圍之內找出一組最好的參數他們比較不會有失敗的問題所以你可以先train一些比較淺的model或者是一些比較簡單的model先知道先有個概念說這些簡單的model到底可以得到什麼樣的loss接下來才train一個深的model如果你發現你深的model跟淺的model比起來深的model明明彈性比較大但loss卻沒有辦法比淺的model壓得更低那就代表說你的optimization有問題你的gradient descent不給力那你要用一些其他的方法來把Optimization 這件事情做得更好舉例來說我們上次看到的觀看人數預測的例子我們說在訓練資料上面2017年到2020年的資料是訓練資料一層的network他的loss是0.28k兩層就降到0.18k三層就降到0.14k四層就降到0.10k但是我去五層的時候結果變成0.34k這是什麼問題我們現在loss很大這個是什麼問題這是model bias的問題嗎顯然不是因為四層都可以做到0.10k了五層應該可以做得更低 optimization的problem這個是optimization的時候做得不好才造成這樣子的問題好那如果optimization做得不好的話怎麼辦呢這個我們下一節課就會告訴大家要怎麼辦現在就知道說有這個問題知道怎麼判斷說現在如果你的training loss大到底是model bias還是optimization如果model bias那就把model變大如果是optimization失敗了那就看下等一下的課程怎麼解決這個問題好那假設你現在經過一番的努力你已經可以讓你的training data的loss變小了那今天我們就講到這裡謝謝大家 接下來你就可以來看Testing Data Loss看Testing Data Loss做得怎麼樣那如果Testing Data Loss也小比如比這個Strong Baseline還要小那就結束了沒什麼好做的就結束了好嗎結束了好那但是如果你覺得還不夠小呢如果Trending Data上面的Loss小Testing Data上的Loss大那你可能就是真的遇到Overfitting的問題那你要注意是Trending的Loss小Testing的Loss大才叫做Overfitting很多同學每次一看到結果不好在Testing上的結果不好就說這個是Overfitting不一定是Overfitting你拿一個結果來問我說老師這個結果要怎麼變做得更好 所以我第一個問題都會問你說你在training data上的loss到底做得怎麼樣那我發現10個同學有8個都說要看training data的loss嗎我沒有把training data loss記下來你要把training data loss記下來先確定說你的optimization沒有問題你的model夠大了然後接下來才看看是不是testing的問題好 那如果是那個training的loss小testing的loss大這個有可能是overfitting那為什麼會有overfitting這樣的狀況呢為什麼有可能training的loss大testing的為什麼有可能training的loss小testing的loss大呢那這邊就舉一個極端的例子來告訴你說 為什麼會發生這樣子的狀況那這是我們的訓練資料假設根據這些訓練資料某一個很廢的machine learning的方法他找出了一個一無是處的function這個一無是處的function是什麼樣的function呢這個一無是處的function說如果今天X當作輸入的時候我們就去比對這個X有沒有出現在訓練資料裡面如果X有出現在訓練資料裡面就把它對應的Y當作輸出如果X沒有出現在訓練資料裡面那怎麼辦就輸出一個隨機的值那你可以想像說這個function 啥事也沒有幹他是一個一無是處的function但雖然他是一個一無是處的function他在training的data上他的loss可是0呢你把training的data通通丟進這個function裡面他的輸出跟你的訓練資料的level是一模一樣的所以在training data上面這個一無是處的function他的loss可是0呢只是在testing data上面他的loss會變得很大因為他其實什麼都沒有學這是一個比較極端的例子那在一般的狀況下也有可能 發生類似的事情舉例來說假設我們的輸入的feature叫做X我們輸出的label叫做Y那X跟Y都是一維的X跟Y之間的關係是這一個二次的曲線那這個曲線我們可以用虛線來表示因為我們通常沒有辦法直接觀察到這條曲線我們真正可以觀察到的是什麼我們真正可以觀察到的是我們的訓練資料那訓練資料你可以想像成就是從這條線 一條曲線上面隨機sample出來的幾個點你今天的模型呢他的能力非常的強他的flexibility很大他的彈性很大的話你只給他這三個點他會知道說在這三個點上面我們要讓low speed所以你今天你的model他的這個曲線會通過這三個點但是其他沒有訓練資料作為限制的地方他就會有freestyle因為他的這個自由彈性 它的flexibility很大嘛 它的彈性很大嘛所以你的model可以變成各式各樣的function你沒有給它資料作為訓練它就有freestyle可以產生各式各樣奇怪的結果那這個時候如果你在丟進你的testing data那testing data跟training data當然不會一模一樣它們可能是從同個distribution sample出來的testing data是橙色的這些點training data是藍色的這些點用藍色的這些點找出一個function以後你測試在橘色的這些點上不一定會好 如果你的model自由度很大的話他可以產生非常奇怪的曲線導致訓練資料上的結果好但是測試資料上的漏失不大那至於更詳細的背後的數學原理為什麼比較有彈性的model他就比較會overfitting背後的數學原理我們留待下下週吳佩媛老師會跟大家更詳細的說明好 那我們今天就是講一下他的概念就好好 那怎麼解決 剛才那個over fitting的問題呢有兩個可能的方向第一個方向是也許這個方向往往是最有效的方向是增加你的訓練資料所以今天假設你自己想要做一個application你發現有over fitting的問題其實我覺得最簡單解決over fitting的方法就是增加你的訓練資料所以今天如果訓練資料藍色的點變多了那雖然你的model他的彈性可能很大 但是因為這邊的點非常非常的多他就會被限制住他看起來的形狀還是會很像是在產生這些資料背後的二次曲線但是你在作業裡面你是不能夠使用這一招的因為我們並不希望大家浪費時間來就收集資料啊等等那這個不是機器學習技術最核心的部分我們希望大家多focus在機器學習核心的技術上而不是花太多力氣去網路上收集資料看看怎麼把作業做好所以這個不是我們要大家做的在作業裡面不能夠自己收集資料 那你可以做什麼呢?你可以做Data Augmentation這個方法並不算是使用了額外的資料Data Augmentation是什麼意思呢?Data Augmentation就是你用一些你對於這個問題的理解自己創造出新的資料舉例來說在做影像辨識的時候非常常做的一個招式是假設你的訓練資料裡面有某一張圖片把它左右翻轉或者是把它其中一塊截出來放大等等你做左右翻轉 你的資料就變成兩倍那這個就是Data Augmentation但是你要注意一下Data Augmentation不能夠隨便亂做這個Data Augmentation這個Augment要Augment的有道理舉例來說在影像辨識裡面就很少看到有人把影像上下顛倒當作Augmentation為什麼?因為這些圖片都是合理的圖片你把一張照片左右翻轉並不會影響到裡面什麼樣的東西那你把它顛倒那就很奇怪了可能不是一個訓練資料裡面可能會有的 可能不是真實世界會出現的影像那如果你給機器看這種奇怪的影像的話他可能就會學到奇怪的東西所以Data Augmentation要根據你對資料的特性對你現在要處理的問題的理解來選擇合適的Data Augmentation的方式 這邊是增加資料的部分那還有什麼解法呢另外一個解法就是不要讓你的模型有那麼大的彈性給他一些限制舉例來說假設我們直接限制說現在我們的model一定是一條二次曲線我們3號通靈出知道說X跟Y背後的關係其實就是一條二次曲線只是我們不明確的知道這二次曲線裡面的每一個參數 你想什麼樣你怎麼會通靈出這樣子的結果你怎麼會知道說要用多contraint的model才會好呢那這就取決於你對這個問題的理解因為這種model是你自己設計的到底model要多contraint、多flexible結果才會好那這個要問你自己那要看這個設計出不同的模型你就會得出不同的結果那現在假設我們已經知道說模型就是二次曲線那你就會給你你就會在選擇function的這個模型 有很大的限制因為二次曲線要嘛就是這樣子要嘛就是這樣子來來去去就是那幾個形狀而已所以當我們的訓練資料有限的時候因為我們來來去去只能夠選那幾個function所以你可能雖然說只給了三個點但是因為我們能選擇的function有限你可能就會正好選到跟真正的distribution比較接近的function然後在測試資料上得到比較好的結果所以這是第二個方法解決overfitting的問題你要給你的model一些 一些限制最好你的model正好跟背後產生資料的過程的process是一樣的那你可能就會那你就有機會得到好的結果但是如果你給你的有哪些方法可以給model製造限制呢舉例來說給他比較少的參數如果是deep learning的話就給他比較少的神經元的數目本來每成1000個神經元改成100個神經元之類或者是你可以讓model製造限制 它都共用參數你可以讓一些參數有一樣的數值那這個部分如果你沒有很清楚的話也沒有關係我們之後在講CNN的時候會講到這個部分所以這邊先前情先這個預告一下就是我們之前講的network的架構叫做Fully Connected Network那Fully Connected Network其實是一個比較有彈性的架構而CNN是一個比較有限制的架構那我們針對比較有耐用性的組體至少在晚上 Ondoそん中1點 小起點閃失或自動失靈這個部分我們就特別車完工作再開那電視49票而且我不是分這麼多可以小irring 大家會說CNN不是比較厲害嗎大家都說做影像就是要用CNN那比較厲害的model難道它比較沒有彈性嗎沒錯它是一個比較沒有彈性的model它厲害的地方就是它是針對影像的特性來限制模型的彈性所以你今天Fully Connected的network可以找出來的function所形成的集合器是比較大的CNN這個model所找出來的function它形成的集合器是比較小的其實包含在Fully Connected的network裡面的但是就是因為CNN給了比較大的 他的限制所以CNN在影像上反而會做得比較好這個之後都還會再提到還有哪些其他的方法呢一個就是用比較少的feature啦那剛才朱教授就示範過本來給三天的資料改成用給兩天的資料其實結果就好了一些那這個是一個招數還有一個招數呢叫做Early StoppingEarly Stopping, Regularization跟Dropout都是之後課程還會講到的東西這三件事情在作業一的程式裡面

這個是over fitting嗎這個不是over fitting因為你又回到了deep learning裡面常用來限制模型的方法那這個之後還會再提到 但是我們也不要給太多的限制為什麼不能給模型太多的限制呢假設我們現在給模型更大的限制說我們假設我們的模型一定是linear的model一定是寫成y等於a加bf那你的model呢它能夠產生的function就一定是一條直線今天給三個點沒有任何一條直線可以同時通過這三個點但是你只能找到一條直線這條直線跟這些點比起來他們的距離是比較近的 但是你沒有辦法找到任何一條直線同時通過這三個點這個時候你的模型的限制就太大了你在測試資料上就不會得到好的結果但是這個是over fitting嗎這個不是over fitting因為你又回到了model bias的問題所以你現在這樣在這個情況下在這個投影片的case上面你結果不好並不是因為over fitting了而是因為你給你的模型太大的限制達到你有了model bias的問題所以你會發現說這邊 產生了一個有點矛盾這邊產生了一個矛盾的狀況今天你讓你的模型的複雜的程度這樣讓你的模型的彈性越來越大但什麼叫做複雜的程度什麼叫做這樣讓你的產生了一個bias的問題所以你會發現說這邊產生了一個有點矛盾這邊產生了一個矛盾的狀況今天你讓你的模型的複雜的程度這樣讓你的模型的彈性越來越大但什麼叫做 複雜的程度什麼叫做彈性在今天這堂課裡面我們其實都沒有給明確的定義只給你一個概念上的敘述那在下一下這個課程裡面你會真的認識到什麼叫做一個模型很複雜什麼叫做一個模型有彈性怎麼真的衡量一個模型的彈性複雜的程度有多大那今天我們先用直觀的來瞭解所謂比較複雜就是它可以包含的 function 比較多它的參數比較多這個就是一個比較複雜的 model那一個比較複雜的 比較複雜的模特兒如果你看他的training的loss你會發現說隨著模特兒越來越複雜training的loss可以越來越低但是testing的時候呢當模特兒越來越複雜的時候剛開始啊你的testing的loss會跟著下降但是當複雜的程度超過某一個程度以後testing的loss就會突然暴增那這就是因為說當你的模特兒越來越複雜的時候複雜到某一個程度overfeeding 你的狀況就會出現所以你在Training的Loss上面可以得到比較好的結果但在Testing的Loss上面你會得到比較大的Loss那我們當然期待說我們可以選一個中庸的模型不是太複雜的也不是太簡單的剛剛好可以在訓練資料上給我們最好的結果給我們最低的Loss給我們最低的Testing Loss怎麼選出這樣的Model呢 一個很直覺的你很有可能沒有人告訴你要怎麼做的話你可能很直覺就會這麼做的做法是說誒這個Cardinal不是立刻上傳就可以知道答案了嗎所以假設我們有三個模型他們的複雜的程度不太一樣我不知道選哪一個模型才會剛剛好在測試資料上得到最好的結果因為你選太複雜的就Over-fitted太簡單的有Model Bias的問題那怎麼選一個不偏不倚的不知道那怎麼辦這三個模型的結果都不一樣 然後上傳到Kargo上面你即時的知道了你的分數看看哪個分數最低那個模型顯然就是最好的模型但是並不建議你這麼做為什麼不建議你這麼做呢我們再舉一個極端的例子我們再把剛才那個極端的例子拿出來假設現在有一群model這一群model不知道為什麼都非常廢他們每一個model產生出來的都是 是一無是處的function我們有1到這個0有多少個我不知道隨便打一兆好了我們有1到1兆個model這一到一兆個model不知道為什麼認出來的function都是一無是處的function他們會做的事情就是訓練資料裡面有的資料就把它記下來訓練資料沒看過的就直接open隨機的結果那你現在有一兆個模型那你再把這一兆個模型的結果通通上傳到kaggle上面你就得到兆的分數 然後看這一兆的分數裡面哪一個結果最好你就覺得那個模型是最好那雖然說每一個模型他們在這個Testing Data上面Testing Data他都沒有看過啊所以他輸出的結果都是隨機的但雖然在Testing Data上面輸出的結果都是隨機的但是你不斷的隨機你總是會找到一個好的結果對不對所以也許編號56789的那一個模型啊他找出來的function正好在Testing Data上面就給你一個好的結果謝謝 你就會很高興覺得說這個model鞭炮56789是個好model這個好model得到一個好function雖然它其實是隨機的但你不知道但是好function這個好function在這個testing data上面給我們好的結果所以你就覺得說嗯這個結果不錯就這樣我就選這個model這個function當作我們最後上傳的結果當作我最後要用在Private Testing Set上的結果但是如果你這樣做往往就會得到非常高的結果因為這個model畢竟是隨機的 隨機的 它恰好在Public的Testing Data上面它Public的Testing Set上得到一個好結果但是它在Private的Testing Set上可能仍然是隨機的所以假設你今天在選Model的時候你都用Public的就我們這個Testing Set分成Public的Set跟Private的Set你在看分數的時候你只看得到Public的分數 Private的分數要Data以後才知道但假設你在挑模型的時候你完全看你在Public的Set上面的分數 也就是leaderboard上的分數來選擇你的模型的話你可能就會這個樣子你在Public的leaderboard上面排前10但是Dayline一結束你就心態就崩了這樣就掉到300名之外而且我們這休克人這麼多你搞不好會掉到1000名之外也說不定而且這件事情並不是傳說並沒有誇飾每年都會有這樣子的狀況發生因為今年我們會看Public就是說我們在算分數的時候你在Public上面的結果好還是會給你 一點分數,我們不是只看Private的分數而已是Public跟Private的分數看啦那過去有些學期是只看Private的分數的時候發生這種狀況你心態就會整個崩掉了你就會非常非常的鬱悶好,那這個時候有同學就會說那為什麼我們要把Testing的Set分成Public跟Private呢?為什麼我們不能就 學期是只看Private的分數的時候發生這種狀況你心態就會整個崩掉你就會非常非常的鬱悶好那那呃呃呃那這個時候有同學就會說那為什麼我們要把Testing的Set分成Public跟Private呢為什麼我們不能就通通都分Public就好呢為什麼要為難大家呢為什麼要讓大家疑神疑鬼不知道自己Private上的結果是什麼你仔細想想看假設所有的Data都是Public那我剛才說 就算是一個一無是處的model得到的一無是處的function他也有可能在public的data上面得到好的結果如果我們今天只有public的testing set沒有private的testing set那你就回去寫一個程式不斷random產生輸出就好然後不斷把random的輸出上傳到puddle然後看你什麼時候可以random出一個好的結果那這個作業就結束了那這個顯然沒有意義顯然不是我們要的而且因為如果今天 然後這邊有另外一個有趣的事情就是你知道因為如果今天Public的Testing Data是公開的你可以知道Public的Testing Data的結果那你就算是一個很廢的模型產生了很廢的Function也可能得到非常好的結果那這就印證了說為什麼在機器學習的領域在那些Benchmark的Codec上面往往機器可以得到一如尋常的好的結果往往都超越人類所以Benchmark 意思就是有一些 dataset 是公開的然後舉例來說這個 Libris Speech 是一個公開的用來訓練語音辨識的資料集那如果你想要測試自己的語音辨識的模型好不好的話那就訓練在 Libris Speech 上面那 Libris Speech 有 Testing Set所有人都共用一模一樣的 Testing Set那我們就可以比較不同模型的好壞但是問題是這些 Testing Set 的結果都是 Public 的所以就算是一個很廢的模型它只能產生很廢的 function你只要做得夠多你還是可以在 Public 的設計上得到結果得到好的結果那這就解釋為什麼說這些 Benchmark Components最終往往機器可以得到超乎人類的結果那這個最有名的就是 就是這個2016年的時候Microsoft跟那個IPM都不約而同的說欸他們的Machine在語音辨識上面得到超越人類的結果專業的聽打員做的這個語音辨識的錯誤率還要低那這個是怎麼來的那個其實就是做在Benchmark的Corpus上面啦那個其實是做在一個叫做Switch4的Benchmark的Corpus上面那你說那在Benchmark Corpus上面得到一個非常好的超越人類結果在現實生活中他真的有超越人類嗎我想你不會相信對不對就算你不是做語音辨識的研究人員你光是有用過你今天語音辨識系統無所不在嘛每個手機拿出來都會語音辨識你其實不會相信說 机器在语音辨识的能力已经超越人类所以这个就是在那些Benchmark Composite上Benchmark Composite的Testing Set就是Priving的Testing Set但是你真的训练出一个语音辨识系统上线给人用的时候那这个是Priving的Testing Set你有可能在Priving的Testing Set上面得到什么超越人类的结果但并不代表它在Priving的Testing Set上已经是好的你在那些Benchmark Composite上面今天机器都说超越人类的语音辨识正确率了并不代表在日常生活中它的语音辨识的正确率超越了人类所以知道说那些说在Benchmark Composite上得到什么超越人类的结果可能都比较像是骗骗麻瓜的商业的辞令不过我觉得说用Benchmark Composite做出结果来还算是已经很好的 是很有品的了我聽過更沒有品的是怎樣就是有一個不知道哪來的新創去接了一個政府的計劃然後說要做語音辨識然後就拿那個Data Set然後KPI就訂說我們這個要做到90%以上的正確率然後做完哇沒有得到90%怎麼做都做不到90%人家要來驗收怎麼辦呢他們就說跟驗收的人說你這個Testing Set不好你再等下這個Testing Set裡面雜訊很多我幫你把它清乾淨我把有雜訊的那個句子拿掉這樣然後KPI就達到了正確率90%以上就起飛了就過了那個KPI了而且還有更更沒品的就是有人會有怪怪的新創會拿出一個東西拿出他自己的AirDrop 我自己做了一個語音辨識系統你知道嗎?跟Google辨識出來的結果都一樣好喔為什麼呢?而且因為他偷偷的Google的API這樣子好所以有各種各樣奇奇怪怪的東西啊所以網路上的東西網路上奇奇怪怪的追蹤大家有時候看看就好好所以講了這麼多只是想要告訴大家說欸我們為什麼要切Public的Testing Set我們為什麼要切Private的Testing Set然後你即使不要花不要用你Public的Testing Set去調你的模型因為你可能會在Private的Testing Set上面得到很差的結果那不過因為今年呢 你在Public Set上面的好的結果也有算分數所以怎麼辦呢?為了避免你為了就你可能會說好那我放棄Public Set的結果我就只拿Public Set的結果然後不斷的產生隨機的結果去上傳到Cargo然後看看說能不能夠正好生產出一個好的結果為了避免浪費時間做這件事情所以有每次上傳的限制讓你不會說我拿很廢的模型只產生隨機的結果不斷的測試Public Testing的Score好那到底要怎麼做才選擇Model才是比較合理的呢?那建議的方法是這個樣子的那其實助教程式裡面也都幫大家做好了你亮 � lira training的資料分成兩半一部分叫做training set一部分是validation set剛才助教程式裡面已經看到說有90%的資料放在training set裡面有10%的資料會被拿來做validation set那你在training set上訓練出來的模型你在validation set上面去衡量他們的分數你根據validation set上面的分數去挑選結果再把這個結果上傳到parkour上面去看看你得到的parkour的分數那因為你在挑分數的時候 來挑你的model所以你的public testing set的分數就可以反映你的private testing set的分數就比較不會得到說在public上面結果很好但是在private上面結果很差這樣子的狀況當我知道說其實你看到public的結果以後你就會去想要調他就你看到你現在弄了一堆模型然後用validation set檢查一下找了一個模型放到public set以上以後發現結果不好你其實不太可能不根據這一個結果去調整你的模型但是假設這一個loop做太多次 根據你的Public Testing Set上的結果去調整你的Model太多次你就又有可能Fit在你的Public Testing Set上面然後在Private Testing Set上面得到差的結果不過還好反正我們有限制上傳的次數所以這個Loop呢你也沒有辦法走太多次可以避免你太多Fit在Public Testing Set上面的結果好 那我知道說今天因為Public Testing Set上面的結果是大家都可以看到然後很多人都會然後名字你又可以隨便亂取所以假設有一個人洗到第一名的話他就會非常的得意他就會把自己的名字改成一些什麼第一次試就第一名了 不是 我其實只是個旁聽但其實他不是旁聽的他感覺說我其實只是個旁聽的隨便做就第一名了那這個時候你就會覺得很緊張尤其他如果是更認識的隔壁小毛得到第一名到時候耀武揚威的時候你就會開始有點緊張你就會說等一下你不要得意我等一下就去把你刷下來這樣那這個時候你要不要理他呢你不要理他根據過去的經驗就在Puffin leaderboard上排前幾名的往往Puffin是很容易慘掉這樣子所以在Puffin的Testing上面得到太好的結果也不用高興的太早 其實最好的做法就是用validation loss最小的值間條就好了就是你不要去管你的public testing set的結果但我知道在實作上你不太可能這麼做因為public set的結果你又看到所以你會對它對你的邏輯的選擇可能還是有一些影響但是你要越少去看那個public testing set的結果越好這樣我回答到你的問題嗎 好,那其他問題我等一下再回答好,那這個是剛才忘了那個附屬那個同學的問題啦線上直播的同學,我附屬一下那個同學的問題他問題是說所以我們不能去看Public Testing Set的結果嗎?理想上是,就理想上你就用Validation Set挑就好然後上傳以後,怎樣就是怎樣有過那個雙Base以後就不要再去動它了那這樣子你比較不會了那這樣子可以避免你Overfit在Testing Set上面好,那但是這邊會有一個問題就是怎麼分Trending Set跟Validation Set呢?那如果在重要程式裡面 就是隨機分的但是你可能會說搞不好我這個分分的不好啊搞不好我分到很奇怪的validation theta會導致我的結果很差啊如果你有這個擔心的話那你可以用N4的Process Validation那N4的Process Validation是怎麼做的呢就你先把你的訓練資料切成 當你的嚴重症狀說你無法做Tespire說你可以做Tespire是一個加強治療N等份在這個例子裡面我們切成三等份切完以後你拿其中一份當做Validation Set另外兩份當Trending Set然後這件事情要重複三次也就是說你先第一份第二份當Trend第三份當Validation然後第一份第三份當Trend第二份當Validation第一份當Validation第二份第三份當Trend然後接下來你有三個模型你不知道哪一個是好的你就把這三個模型在這三個Setting下 三個Trending跟Validation的Data Set上面通通跑過一次然後把這三個模型在這三種狀況的結果都平均起來把每一個模型在這三種狀況的結果都平均起來再看看誰的結果最好再看看誰的結果最好假設現在Model 1的結果最好你用這三個Fold得出來的結果是這個Model 1最好然後你再把Model 1用在全部的Trending Set上然後訓練出來的模型再用在Testing Set上面這個是跟Fold的Process 那這個就是這門課前期的攻略他可以帶你打贏前期所有的部分那接下來也許你要問的一個問題是上週結束的時候不是講到預測2月26號也就是跑過一次然後把這三個模型在這三種狀況的結果都平均起來把每一個模型在這三種狀況的結果都平均起來再看看誰的結果最好再看看誰的結果最好假設現在model 1的結果最好你用這三個forge得出來的結果是 這個Model 1最好然後你再把Model 1用在全部的Trading Set上然後訓練出來的模型再用在Testing Set上面這個是M4的Cross-Valuation好 那這個就是這門課前期的攻略它可以帶你打贏前期所有的部門那接下來也許你要問的一個問題是上週結束的時候不是講到預測2月26號也就是上週的觀看人數嗎到底結果做的怎麼樣好 那這個就是我們要做的結果上週比較多人選了單程的內容 所以我們就把三成的Network拿來測試一下以下是測試的結果我們就沒有再調參數了大家決定用三成的就是下好離手了就直接用上去了好得到的結果是這個樣子的這個圖上這個橫軸就是從2021年的1月1號開始一直往下然後紅色的線是真實的數字藍色的線是預測的結果2月26號在這邊這個是今年2021年觀看人數最高的一天 77的預設怎樣呢哇非常的慘差距非常的大差距有2.58K這麼多感謝大家為了讓這個模型不準這個下午我花了很多力氣去點了這個video所以這一天是今年觀看人數最多的一天那你可能開始想說那別的模型怎麼樣呢其實我也跑了一層二層跟四層的看看啦所有的模型都會慘掉兩層跟三層的錯誤率都是2點多K其實四層跟一層比較好都是1.8K左右但是這四個模型不約而同的 第二個 2月26號應該是個低點但實際上 2月26號是一個峰值那模型其實會覺得他是個低點也不能怪他因為根據過去的資料禮拜五就是沒有人要學機器學習禮拜五晚上大家出去玩了對不對禮拜五觀看的人數是最少的但是2月26號出現了反常的狀況好那這個呢就不能怪模型了那我覺得出現這種狀況啊應該算是另外一種錯誤的形式這種錯誤的形式呢我們這邊叫做Mismatch那也有人會說Mismatch也算是一種Overfitting這樣也可以啦這都只是名詞定義的問題那我覺得要想 我想表達的事情是Mismatch它的原因跟Overfitting其實不一樣一般的Overfitting你可以用收集更多的資料來克服但是Mismatch意思是說你今天的訓練資料跟測試資料它們的分佈是不一樣的在訓練資料跟測試資料分佈是不一樣的時候你訓練資料再增加其實也沒有幫助那其實在多數的作業裡面我們不會遇到這種Mismatch的問題那我們都用把這個題目設計好了訓練資料跟測試資料它的分佈差很多舉例來說以剛才作業一的COVID-19為例的話假如 假設我們今天資料在分訓練資料跟測試資料的時候我們說2020年的資料是訓練資料2021年的資料是測試資料那mismatch的問題可能就很嚴重了我們其實有試過了試了一下如果今天用2020年當訓練資料2020年當測試資料你就怎麼做都是慘啊就做不起來你訓練什麼模型都會慘掉因為2020年的資料跟2021年的資料他們背後的分佈其實就是不一樣的所以拿2020年的資料來訓練那2021年的作業日的資料上你根本就預測不準所以後來資料是用了別的方式來分割訓練資料跟測試資料 所以我們多數的作業都不會有這種Mismatch的問題那除了作業11因為作業11就是針對Mismatch的問題來設計的作業11也是一個加分類的問題這是他的訓練資料看起來蠻正常的但他測試資料就是長這樣子所以你知道這個時候這個時候增加資料哪有什麼用呢增加資料你也沒有辦法讓你的模型做得更好所以這種問題要怎麼解決那留待作業11的時候再講那你可能會問說我怎麼知道現在到底是不是Mismatch呢那我覺得要知道是不是Mismatch 那就要看你對這個資料本身的理解你可能要對你的訓練資料跟測試資料的產生方式有一些理解才能判斷說他是不是遇到了Mismatch的狀況好 那這個就是我們作業的攻略那我在這邊停下來看看大家有沒有問題要問的