# 笔记总结

![[Pasted image 20250911161222988.png]]
- 过拟合与模型限制并非同一概念。
- 模型限制过大导致模型偏差（model bias），而非过拟合。
- 限制过大的例子：假设模型必须是线性模型（y=a+bx），则无法拟合非线性数据，导致测试结果差。
- 模型复杂度与偏差之间存在矛盾关系：增加模型复杂度提升了模型灵活性，但过分限制模型也会产生偏差。


![[Pasted image 20250911161222997.png]]

- 本节课未对模型复杂度和弹性给出明确定义，仅作概念性描述。
- 下节课将讲解如何衡量模型复杂度和弹性。
- 直观理解：复杂模型包含更多函数和参数。
- 复杂模型的训练损失会随着模型复杂度增加而降低。
- 但测试损失会先下降后上升，超过一定复杂度后会暴增（过拟合）。
- 目标是选择一个适中复杂度的模型，在训练和测试数据上都能获得最低损失。


![[Pasted image 20250911161223004.png]]

- 直接根據 Kaggle 分數選擇模型可能導致過擬合，因為模型可能只是在公開測試集上表現良好，而在私有測試集上表現很差。
-  一個極端例子：即使使用一兆個隨機模型，總會有一個在公開測試集上表現出色，但這並不代表該模型有效。
-  僅根據公開測試集分數（Leaderboard）選擇模型，可能導致在私有測試集上表現極差，排名大幅下降。
-  公開和私有測試集的設置，可以避免僅根據公開測試集表現選擇模型所帶來的風險。
-  以往只使用私有測試集評分的學期，也發生過類似情況，導致參賽者排名大幅下降和情緒低落。

![[Pasted image 20250911161223013.png]]
- 使用Public和Private測試集的原因：若只有Public測試集，則模型可以通過隨機產生輸出或過擬合Public數據獲得高分，這沒有意義。

- Benchmark數據集的問題：公開的Benchmark數據集（例如Libris Speech）的測試結果是公開的，這導致即使很差的模型也能通過多次嘗試獲得好的結果，從而夸大模型的性能，例如2016年一些語音辨識模型聲稱超越人類的表現，實際上是在特定Benchmark數據集上達到的。

-  現實應用與Benchmark結果的差距：在Benchmark數據集上表現良好的模型，在實際應用中未必有同樣好的表現。

-  不當操作案例：一些公司通過操縱數據或使用外部API來達到KPI指標，例如清除測試集中的雜訊或秘密使用Google API。

-  總結：分割Public和Private測試集的重要性在於避免過擬合和確保模型在真實環境中的有效性，避免Benchmark數據集的誤導性結果。
![[Pasted image 20250911161223029.png]]

-  为了避免过度依赖Public Set结果，建议将训练数据分成training set (90%) 和 validation set (10%)。
-  根据validation set上的分数选择模型，再上传到Public Set测试。
-  选择validation set分数最高的模型，以此来减少在Public Set上表现好但在Private Set上表现差的风险。
-  限制模型上传次数，避免过度调整模型以适应Public Set。
-  不要过度关注Public Set的排名，因为排名靠前者容易掉落。
-  理想情况下，仅使用validation set选择模型，上传后不再调整。
-  虽然实际操作中很难完全忽略Public Set结果，但应尽量减少对它的依赖。



![[Pasted image 20250911161223039.png]]
- 如何划分训练集(Trending Set)和验证集(Validation Set)：建议使用N折交叉验证(N=3)。
- N折交叉验证过程：将数据分成N份，每次选择一份作为验证集，其余作为训练集，重复N次。
- 模型选择：将不同模型在N折交叉验证中运行，平均每个模型在不同设置下的结果，选择结果最好的模型。
- 最终训练：用选择的最佳模型在整个训练集上进行训练，然后在测试集上进行最终评估。
- 此方法适用于课程前期，用于解决模型训练中验证集划分不佳可能导致结果偏差的问题。

![[Pasted image 20250911161223048.png]]
![[Pasted image 20250911161223057.png]]
- 使用Model 1进行M4交叉验证，先在全部交易集上训练，再用测试集测试。
- 上周预测2月26日观看人数的结果很差，预测值与真实值相差2.58K。所有模型预测都失败，2月26日实际为峰值，而模型预测为低点。
- 模型失败的原因是数据分布不匹配(Mismatch)，这与过拟合(Overfitting)不同。过拟合可以通过收集更多数据解决，但数据分布不匹配则不能。
- 数据分布不匹配是指训练数据和测试数据分布不同，增加训练数据也无济于事。
- COVID-19作业中，使用2020年数据训练，2021年数据测试，结果很差，因为两年的数据分布不同。最终使用了不同的数据分割方法。

---

# 原始语音转文字

這個是over fitting嗎這個不是over fitting因為你又回到了deep learning裡面常用來限制模型的方法那這個之後還會再提到 但是我們也不要給太多的限制為什麼不能給模型太多的限制呢假設我們現在給模型更大的限制說我們假設我們的模型一定是linear的model一定是寫成y等於a加bf那你的model呢它能夠產生的function就一定是一條直線今天給三個點沒有任何一條直線可以同時通過這三個點但是你只能找到一條直線這條直線跟這些點比起來他們的距離是比較近的 但是你沒有辦法找到任何一條直線同時通過這三個點這個時候你的模型的限制就太大了你在測試資料上就不會得到好的結果但是這個是over fitting嗎這個不是over fitting因為你又回到了model bias的問題所以你現在這樣在這個情況下在這個投影片的case上面你結果不好並不是因為over fitting了而是因為你給你的模型太大的限制達到你有了model bias的問題所以你會發現說這邊 產生了一個有點矛盾這邊產生了一個矛盾的狀況今天你讓你的模型的複雜的程度這樣讓你的模型的彈性越來越大但什麼叫做複雜的程度什麼叫做這樣讓你的產生了一個bias的問題所以你會發現說這邊產生了一個有點矛盾這邊產生了一個矛盾的狀況今天你讓你的模型的複雜的程度這樣讓你的模型的彈性越來越大但什麼叫做 複雜的程度什麼叫做彈性在今天這堂課裡面我們其實都沒有給明確的定義只給你一個概念上的敘述那在下一下這個課程裡面你會真的認識到什麼叫做一個模型很複雜什麼叫做一個模型有彈性怎麼真的衡量一個模型的彈性複雜的程度有多大那今天我們先用直觀的來瞭解所謂比較複雜就是它可以包含的 function 比較多它的參數比較多這個就是一個比較複雜的 model那一個比較複雜的 比較複雜的模特兒如果你看他的training的loss你會發現說隨著模特兒越來越複雜training的loss可以越來越低但是testing的時候呢當模特兒越來越複雜的時候剛開始啊你的testing的loss會跟著下降但是當複雜的程度超過某一個程度以後testing的loss就會突然暴增那這就是因為說當你的模特兒越來越複雜的時候複雜到某一個程度overfeeding 你的狀況就會出現所以你在Training的Loss上面可以得到比較好的結果但在Testing的Loss上面你會得到比較大的Loss那我們當然期待說我們可以選一個中庸的模型不是太複雜的也不是太簡單的剛剛好可以在訓練資料上給我們最好的結果給我們最低的Loss給我們最低的Testing Loss怎麼選出這樣的Model呢 一個很直覺的你很有可能沒有人告訴你要怎麼做的話你可能很直覺就會這麼做的做法是說誒這個Cardinal不是立刻上傳就可以知道答案了嗎所以假設我們有三個模型他們的複雜的程度不太一樣我不知道選哪一個模型才會剛剛好在測試資料上得到最好的結果因為你選太複雜的就Over-fitted太簡單的有Model Bias的問題那怎麼選一個不偏不倚的不知道那怎麼辦這三個模型的結果都不一樣 然後上傳到Kargo上面你即時的知道了你的分數看看哪個分數最低那個模型顯然就是最好的模型但是並不建議你這麼做為什麼不建議你這麼做呢我們再舉一個極端的例子我們再把剛才那個極端的例子拿出來假設現在有一群model這一群model不知道為什麼都非常廢他們每一個model產生出來的都是 是一無是處的function我們有1到這個0有多少個我不知道隨便打一兆好了我們有1到1兆個model這一到一兆個model不知道為什麼認出來的function都是一無是處的function他們會做的事情就是訓練資料裡面有的資料就把它記下來訓練資料沒看過的就直接open隨機的結果那你現在有一兆個模型那你再把這一兆個模型的結果通通上傳到kaggle上面你就得到兆的分數 然後看這一兆的分數裡面哪一個結果最好你就覺得那個模型是最好那雖然說每一個模型他們在這個Testing Data上面Testing Data他都沒有看過啊所以他輸出的結果都是隨機的但雖然在Testing Data上面輸出的結果都是隨機的但是你不斷的隨機你總是會找到一個好的結果對不對所以也許編號56789的那一個模型啊他找出來的function正好在Testing Data上面就給你一個好的結果謝謝 你就會很高興覺得說這個model鞭炮56789是個好model這個好model得到一個好function雖然它其實是隨機的但你不知道但是好function這個好function在這個testing data上面給我們好的結果所以你就覺得說嗯這個結果不錯就這樣我就選這個model這個function當作我們最後上傳的結果當作我最後要用在Private Testing Set上的結果但是如果你這樣做往往就會得到非常高的結果因為這個model畢竟是隨機的 隨機的 它恰好在Public的Testing Data上面它Public的Testing Set上得到一個好結果但是它在Private的Testing Set上可能仍然是隨機的所以假設你今天在選Model的時候你都用Public的就我們這個Testing Set分成Public的Set跟Private的Set你在看分數的時候你只看得到Public的分數 Private的分數要Data以後才知道但假設你在挑模型的時候你完全看你在Public的Set上面的分數 也就是leaderboard上的分數來選擇你的模型的話你可能就會這個樣子你在Public的leaderboard上面排前10但是Dayline一結束你就心態就崩了這樣就掉到300名之外而且我們這休克人這麼多你搞不好會掉到1000名之外也說不定而且這件事情並不是傳說並沒有誇飾每年都會有這樣子的狀況發生因為今年我們會看Public就是說我們在算分數的時候你在Public上面的結果好還是會給你 一點分數,我們不是只看Private的分數而已是Public跟Private的分數看啦那過去有些學期是只看Private的分數的時候發生這種狀況你心態就會整個崩掉了你就會非常非常的鬱悶好,那這個時候有同學就會說那為什麼我們要把Testing的Set分成Public跟Private呢?為什麼我們不能就 學期是只看Private的分數的時候發生這種狀況你心態就會整個崩掉你就會非常非常的鬱悶好那那呃呃呃那這個時候有同學就會說那為什麼我們要把Testing的Set分成Public跟Private呢為什麼我們不能就通通都分Public就好呢為什麼要為難大家呢為什麼要讓大家疑神疑鬼不知道自己Private上的結果是什麼你仔細想想看假設所有的Data都是Public那我剛才說 就算是一個一無是處的model得到的一無是處的function他也有可能在public的data上面得到好的結果如果我們今天只有public的testing set沒有private的testing set那你就回去寫一個程式不斷random產生輸出就好然後不斷把random的輸出上傳到puddle然後看你什麼時候可以random出一個好的結果那這個作業就結束了那這個顯然沒有意義顯然不是我們要的而且因為如果今天 然後這邊有另外一個有趣的事情就是你知道因為如果今天Public的Testing Data是公開的你可以知道Public的Testing Data的結果那你就算是一個很廢的模型產生了很廢的Function也可能得到非常好的結果那這就印證了說為什麼在機器學習的領域在那些Benchmark的Codec上面往往機器可以得到一如尋常的好的結果往往都超越人類所以Benchmark 意思就是有一些 dataset 是公開的然後舉例來說這個 Libris Speech 是一個公開的用來訓練語音辨識的資料集那如果你想要測試自己的語音辨識的模型好不好的話那就訓練在 Libris Speech 上面那 Libris Speech 有 Testing Set所有人都共用一模一樣的 Testing Set那我們就可以比較不同模型的好壞但是問題是這些 Testing Set 的結果都是 Public 的所以就算是一個很廢的模型它只能產生很廢的 function你只要做得夠多你還是可以在 Public 的設計上得到結果得到好的結果那這就解釋為什麼說這些 Benchmark Components最終往往機器可以得到超乎人類的結果那這個最有名的就是 就是這個2016年的時候Microsoft跟那個IPM都不約而同的說欸他們的Machine在語音辨識上面得到超越人類的結果專業的聽打員做的這個語音辨識的錯誤率還要低那這個是怎麼來的那個其實就是做在Benchmark的Corpus上面啦那個其實是做在一個叫做Switch4的Benchmark的Corpus上面那你說那在Benchmark Corpus上面得到一個非常好的超越人類結果在現實生活中他真的有超越人類嗎我想你不會相信對不對就算你不是做語音辨識的研究人員你光是有用過你今天語音辨識系統無所不在嘛每個手機拿出來都會語音辨識你其實不會相信說 机器在语音辨识的能力已经超越人类所以这个就是在那些Benchmark Composite上Benchmark Composite的Testing Set就是Priving的Testing Set但是你真的训练出一个语音辨识系统上线给人用的时候那这个是Priving的Testing Set你有可能在Priving的Testing Set上面得到什么超越人类的结果但并不代表它在Priving的Testing Set上已经是好的你在那些Benchmark Composite上面今天机器都说超越人类的语音辨识正确率了并不代表在日常生活中它的语音辨识的正确率超越了人类所以知道说那些说在Benchmark Composite上得到什么超越人类的结果可能都比较像是骗骗麻瓜的商业的辞令不过我觉得说用Benchmark Composite做出结果来还算是已经很好的 是很有品的了我聽過更沒有品的是怎樣就是有一個不知道哪來的新創去接了一個政府的計劃然後說要做語音辨識然後就拿那個Data Set然後KPI就訂說我們這個要做到90%以上的正確率然後做完哇沒有得到90%怎麼做都做不到90%人家要來驗收怎麼辦呢他們就說跟驗收的人說你這個Testing Set不好你再等下這個Testing Set裡面雜訊很多我幫你把它清乾淨我把有雜訊的那個句子拿掉這樣然後KPI就達到了正確率90%以上就起飛了就過了那個KPI了而且還有更更沒品的就是有人會有怪怪的新創會拿出一個東西拿出他自己的AirDrop 我自己做了一個語音辨識系統你知道嗎?跟Google辨識出來的結果都一樣好喔為什麼呢?而且因為他偷偷的Google的API這樣子好所以有各種各樣奇奇怪怪的東西啊所以網路上的東西網路上奇奇怪怪的追蹤大家有時候看看就好好所以講了這麼多只是想要告訴大家說欸我們為什麼要切Public的Testing Set我們為什麼要切Private的Testing Set然後你即使不要花不要用你Public的Testing Set去調你的模型因為你可能會在Private的Testing Set上面得到很差的結果那不過因為今年呢 你在Public Set上面的好的結果也有算分數所以怎麼辦呢?為了避免你為了就你可能會說好那我放棄Public Set的結果我就只拿Public Set的結果然後不斷的產生隨機的結果去上傳到Cargo然後看看說能不能夠正好生產出一個好的結果為了避免浪費時間做這件事情所以有每次上傳的限制讓你不會說我拿很廢的模型只產生隨機的結果不斷的測試Public Testing的Score好那到底要怎麼做才選擇Model才是比較合理的呢?那建議的方法是這個樣子的那其實助教程式裡面也都幫大家做好了你亮 � lira training的資料分成兩半一部分叫做training set一部分是validation set剛才助教程式裡面已經看到說有90%的資料放在training set裡面有10%的資料會被拿來做validation set那你在training set上訓練出來的模型你在validation set上面去衡量他們的分數你根據validation set上面的分數去挑選結果再把這個結果上傳到parkour上面去看看你得到的parkour的分數那因為你在挑分數的時候 來挑你的model所以你的public testing set的分數就可以反映你的private testing set的分數就比較不會得到說在public上面結果很好但是在private上面結果很差這樣子的狀況當我知道說其實你看到public的結果以後你就會去想要調他就你看到你現在弄了一堆模型然後用validation set檢查一下找了一個模型放到public set以上以後發現結果不好你其實不太可能不根據這一個結果去調整你的模型但是假設這一個loop做太多次 根據你的Public Testing Set上的結果去調整你的Model太多次你就又有可能Fit在你的Public Testing Set上面然後在Private Testing Set上面得到差的結果不過還好反正我們有限制上傳的次數所以這個Loop呢你也沒有辦法走太多次可以避免你太多Fit在Public Testing Set上面的結果好 那我知道說今天因為Public Testing Set上面的結果是大家都可以看到然後很多人都會然後名字你又可以隨便亂取所以假設有一個人洗到第一名的話他就會非常的得意他就會把自己的名字改成一些什麼第一次試就第一名了 不是 我其實只是個旁聽但其實他不是旁聽的他感覺說我其實只是個旁聽的隨便做就第一名了那這個時候你就會覺得很緊張尤其他如果是更認識的隔壁小毛得到第一名到時候耀武揚威的時候你就會開始有點緊張你就會說等一下你不要得意我等一下就去把你刷下來這樣那這個時候你要不要理他呢你不要理他根據過去的經驗就在Puffin leaderboard上排前幾名的往往Puffin是很容易慘掉這樣子所以在Puffin的Testing上面得到太好的結果也不用高興的太早 其實最好的做法就是用validation loss最小的值間條就好了就是你不要去管你的public testing set的結果但我知道在實作上你不太可能這麼做因為public set的結果你又看到所以你會對它對你的邏輯的選擇可能還是有一些影響但是你要越少去看那個public testing set的結果越好這樣我回答到你的問題嗎 好,那其他問題我等一下再回答好,那這個是剛才忘了那個附屬那個同學的問題啦線上直播的同學,我附屬一下那個同學的問題他問題是說所以我們不能去看Public Testing Set的結果嗎?理想上是,就理想上你就用Validation Set挑就好然後上傳以後,怎樣就是怎樣有過那個雙Base以後就不要再去動它了那這樣子你比較不會了那這樣子可以避免你Overfit在Testing Set上面好,那但是這邊會有一個問題就是怎麼分Trending Set跟Validation Set呢?那如果在重要程式裡面 就是隨機分的但是你可能會說搞不好我這個分分的不好啊搞不好我分到很奇怪的validation theta會導致我的結果很差啊如果你有這個擔心的話那你可以用N4的Process Validation那N4的Process Validation是怎麼做的呢就你先把你的訓練資料切成 當你的嚴重症狀說你無法做Tespire說你可以做Tespire是一個加強治療N等份在這個例子裡面我們切成三等份切完以後你拿其中一份當做Validation Set另外兩份當Trending Set然後這件事情要重複三次也就是說你先第一份第二份當Trend第三份當Validation然後第一份第三份當Trend第二份當Validation第一份當Validation第二份第三份當Trend然後接下來你有三個模型你不知道哪一個是好的你就把這三個模型在這三個Setting下 三個Trending跟Validation的Data Set上面通通跑過一次然後把這三個模型在這三種狀況的結果都平均起來把每一個模型在這三種狀況的結果都平均起來再看看誰的結果最好再看看誰的結果最好假設現在Model 1的結果最好你用這三個Fold得出來的結果是這個Model 1最好然後你再把Model 1用在全部的Trending Set上然後訓練出來的模型再用在Testing Set上面這個是跟Fold的Process 那這個就是這門課前期的攻略他可以帶你打贏前期所有的部分那接下來也許你要問的一個問題是上週結束的時候不是講到預測2月26號也就是跑過一次然後把這三個模型在這三種狀況的結果都平均起來把每一個模型在這三種狀況的結果都平均起來再看看誰的結果最好再看看誰的結果最好假設現在model 1的結果最好你用這三個forge得出來的結果是 這個Model 1最好然後你再把Model 1用在全部的Trading Set上然後訓練出來的模型再用在Testing Set上面這個是M4的Cross-Valuation好 那這個就是這門課前期的攻略它可以帶你打贏前期所有的部門那接下來也許你要問的一個問題是上週結束的時候不是講到預測2月26號也就是上週的觀看人數嗎到底結果做的怎麼樣好 那這個就是我們要做的結果上週比較多人選了單程的內容 所以我們就把三成的Network拿來測試一下以下是測試的結果我們就沒有再調參數了大家決定用三成的就是下好離手了就直接用上去了好得到的結果是這個樣子的這個圖上這個橫軸就是從2021年的1月1號開始一直往下然後紅色的線是真實的數字藍色的線是預測的結果2月26號在這邊這個是今年2021年觀看人數最高的一天 77的預設怎樣呢哇非常的慘差距非常的大差距有2.58K這麼多感謝大家為了讓這個模型不準這個下午我花了很多力氣去點了這個video所以這一天是今年觀看人數最多的一天那你可能開始想說那別的模型怎麼樣呢其實我也跑了一層二層跟四層的看看啦所有的模型都會慘掉兩層跟三層的錯誤率都是2點多K其實四層跟一層比較好都是1.8K左右但是這四個模型不約而同的 第二個 2月26號應該是個低點但實際上 2月26號是一個峰值那模型其實會覺得他是個低點也不能怪他因為根據過去的資料禮拜五就是沒有人要學機器學習禮拜五晚上大家出去玩了對不對禮拜五觀看的人數是最少的但是2月26號出現了反常的狀況好那這個呢就不能怪模型了那我覺得出現這種狀況啊應該算是另外一種錯誤的形式這種錯誤的形式呢我們這邊叫做Mismatch那也有人會說Mismatch也算是一種Overfitting這樣也可以啦這都只是名詞定義的問題那我覺得要想 我想表達的事情是Mismatch它的原因跟Overfitting其實不一樣一般的Overfitting你可以用收集更多的資料來克服但是Mismatch意思是說你今天的訓練資料跟測試資料它們的分佈是不一樣的在訓練資料跟測試資料分佈是不一樣的時候你訓練資料再增加其實也沒有幫助那其實在多數的作業裡面我們不會遇到這種Mismatch的問題那我們都用把這個題目設計好了訓練資料跟測試資料它的分佈差很多舉例來說以剛才作業一的COVID-19為例的話假如 假設我們今天資料在分訓練資料跟測試資料的時候我們說2020年的資料是訓練資料2021年的資料是測試資料那mismatch的問題可能就很嚴重了我們其實有試過了試了一下如果今天用2020年當訓練資料2020年當測試資料你就怎麼做都是慘啊就做不起來你訓練什麼模型都會慘掉因為2020年的資料跟2021年的資料他們背後的分佈其實就是不一樣的所以拿2020年的資料來訓練那2021年的作業日的資料上你根本就預測不準所以後來資料是用了別的方式來分割訓練資料跟測試資料 所以我們多數的作業都不會有這種Mismatch的問題那除了作業11因為作業11就是針對Mismatch的問題來設計的作業11也是一個加分類的問題這是他的訓練資料看起來蠻正常的但他測試資料就是長這樣子所以你知道這個時候這個時候增加資料哪有什麼用呢增加資料你也沒有辦法讓你的模型做得更好所以這種問題要怎麼解決那留待作業11的時候再講那你可能會問說我怎麼知道現在到底是不是Mismatch呢那我覺得要知道是不是Mismatch 那就要看你對這個資料本身的理解你可能要對你的訓練資料跟測試資料的產生方式有一些理解才能判斷說他是不是遇到了Mismatch的狀況好 那這個就是我們作業的攻略那我在這邊停下來看看大家有沒有問題要問的