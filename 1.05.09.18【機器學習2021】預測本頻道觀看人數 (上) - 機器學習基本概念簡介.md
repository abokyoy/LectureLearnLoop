# 笔记总结

![[Pasted image 20250918172753642.png]]

- 机器学习的核心概念是让机器具备寻找函数的能力。
- 通过寻找合适的函数，机器可以执行各种任务，例如语音识别（输入：声音信号；输出：文字）、图像识别（输入：图片；输出：图片内容）、围棋游戏（输入：棋盘状态；输出：下一步落子位置）。
- 这些函数通常非常复杂，无法人工编写，因此需要机器学习算法自动寻找。
- 不同的任务需要寻找不同的函数，导致机器学习有多种类别。

![[Pasted image 20250918172753665.png]]

- 回归 (Regression):  预测数值型输出的机器学习任务，例如预测未来的PM2.5数值。输入可以是各种相关指标（如当天的PM2.5、温度、臭氧浓度等），输出是预测的数值。

- 分类 (Classification):  选择预定义类别作为输出的机器学习任务，例如垃圾邮件检测。输入是一封邮件，输出是从预定义的类别（例如“垃圾邮件”或“非垃圾邮件”）中选择一个。![[Pasted image 20250918172753692.png]]

- 分类问题不限于两个选项，也可以有多个选项。
- AlphaGo就是一个多选项分类问题的例子，其选项数量为棋盘上所有可落子位置的数量（19x19）。
- 教机器下围棋，本质上是让机器从19x19个选项中选择最佳落子位置。
- 这是一个寻找函数的问题：输入是棋盘上黑白子的位置，输出是从19x19个选项中选择一个最佳落子位置。![[Pasted image 20250918172753729.png]]


- 机器学习的教科书通常只介绍回归(Regression)和分类(Classification)两种任务，这过于简化。
-  除了回归和分类，机器学习还有更复杂的任务，如同“黑暗大陆”般鲜为人知。
-  这个“黑暗大陆”指的是结构学习 (Structure Learning)，即机器学习产生结构化对象，例如图像或文章。
-  结构学习可以被理解为让机器学会创造。
-  文中总结了三种机器学习任务：回归、分类和结构学习。![[Pasted image 20250918172753790.png]]


- 机器学习的目标是找到一个函数。
- 通过YouTube频道流量预测的例子说明机器如何寻找函数。
-  该例子旨在说明如何根据YouTube后台数据（例如每日点赞数、订阅数、观看次数）预测频道次日的总观看次数。
-  此例子的目的是为了说明机器学习寻找函数的过程，而非实际应用价值。![[Pasted image 20250918172753818.png]]


- 機器學習尋找函數的過程分為三個步驟。
- 第一步是寫出一個帶有未知參數的函數（模型），例如：y = b + w * x1，其中y是預測值（例如，2月26日YouTube頻道的觀看人數），x1是已知數據（例如，2月25日YouTube頻道的觀看人數），b和w是未知參數。
-  x1 稱為特徵(Feature)，b和w是未知參數，w稱為權重(Weight)，b稱為偏差(Bias)。
-  這個初始函數的建立基於對問題的領域知識（Domain Knowledge）。
- 模型中的已知數據來自於YouTube後台資訊。![[Pasted image 20250918172753845.png]]

![[Pasted image 20250918172753871.png]]

- 第二步是定义损失函数 (Loss function)。
- 损失函数的输入是模型参数 (b 和 w)，输出值代表参数设定好坏。
- 以预测YouTube频道每日观看次数为例，模型为 Y = 0.5K + 1 * X1 (其中 X1 为前一天观看次数，Y 为预测观看次数)。
- 损失函数计算预测值与真实值 (Label) 的差距。
-  举例：若预测1月2日观看次数为5.3K，实际为4.9K，则误差为 0.4K。
-  计算误差的方法有多种，例如 Mean Absolute Error (MAE) 和 Mean Square Error (MSE)。
-  MAE 计算预测值与真实值差的绝对值；MSE 计算差的平方。
-  本例中使用 MSE，将三年每天的误差加总取平均，得到最终的 Loss 值。
-  Loss 值越大，参数设定越差；Loss 值越小，参数设定越好。![[Pasted image 20250918172753894.png]]

- 使用真实频道后台数据，通过调整参数W和B，计算损失函数Loss。
- 等高线图显示Loss值：红色代表Loss大（W和B组合差），蓝色代表Loss小（W和B组合好）。
-  W=-0.25, B=-5时，Loss很大，表示频道观看人数减少。
- W=0.75, B=5时，预测更精准。
- W接近1, B接近0时，预测最精准，与预期相符（用前一天的观看次数预测后一天）。
- 等高线图（Loss Surface）展示了不同参数组合下的Loss值。


![[Pasted image 20250918172753948.png]]

- 机器学习的第三步是解决最佳化问题，找到最佳的W和B，使loss值最小。
- 使用梯度下降法 (Gradient Descent) 寻找最佳W和B。
- 梯度下降法简化后，假设只有一个参数W。
- 随机选择初始点W0。
- 计算W对loss的微分（或斜率）：
    - 若斜率为负，则W值变大，loss变小。
    - 若斜率为正，则W值变小，loss变小。
- 更新W的步长取决于斜率和学习率 (Learning Rate, Eta)。
- 学习率 (Eta) 是超参数 (Hyperparameter)，需要手动设定。  较大的Eta导致学习速度快，较小的Eta导致学习速度慢。



有人提问  为什么loss会是负数

- Loss 函数是用户自定义的，因此可以是负值。
- 之前提到的 Loss 函数例子 (绝对值)  并非真实情况，仅为一般性案例。
- 真实任务中的 error surface 形状不固定。
-  如果 Loss 定义为绝对值，则不可能为负值。
- 讲解将暂停，以解答听众和直播观众的问题。



- 梯度下降法中，超参数（Hyperparameter）需要自行设定，例如参数更新的次数。
- 参数更新的公式：W1 = W0 - Eta * 微分结果，其中 Eta 为学习率。
- 梯度下降法会停止于两种情况：达到预设的最大迭代次数或微分结果为0。
- 梯度下降法可能陷入局部最小值 (local minima)，而非全局最小值 (global minima)，这是教科书中常提到的问题。
-  局部最小值问题在实际深度学习中并非主要难题，其真实挑战将在后续讨论。

![[Pasted image 20250918172753971.png]]


- 使用梯度下降法，即使模型有两个参数 (W 和 B)，方法也与只有一个参数时相同。
- 初始化 W 和 B 为随机值 (W0 和 B0)。
- 计算 W 对损失函数 L 的偏导数和 B 对 L 的偏导数 (在 W=W0, B=B0 的位置)。
- 更新 W 和 B：W1 = W0 - learning rate * ∂L/∂W； B1 = B0 - learning rate * ∂L/∂B。
- PyTorch 等深度学习框架会自动计算偏导数，只需调用一行代码即可。
- 重复上述步骤，迭代更新 W 和 B，直到找到最佳值 W* 和 B*。



![[Pasted image 20250918172753988.png]]


- 通过反复更新W和B，寻找最佳的W*和B*。
- 通过计算W和B对损失函数L的微分，确定更新方向。
- 更新方向为负梯度乘以学习率η。
- 举例说明：最终找到最佳W为0.97，最佳B为0.0，损失函数L为0.48K。
-  在2017年到2020年的数据上，使用W=0.97, B=0.1，平均误差约为500人次。![[Pasted image 20250918172754047.png]]


- 机器学习流程包含三个步骤：定义包含未知数的函数、定义损失函数、通过优化算法找到使损失函数最小的一组参数W和B。
- 使用2017-2020年的数据进行训练，得到的损失值为0.48K，但这只是在已知答案的数据上进行的测试，缺乏实际意义。
- 真正重要的是预测未来数据。利用训练好的模型预测2021年1月1日至2月14日的观看人次，平均误差为0.58K (约580-600人)。
- 将预测结果与真实数据对比，发现预测结果近似于将真实数据右移一天，模型过于简单，仅利用前一天的数据进行预测。
- 真实数据显示出每周循环的周期性，周五周六观看人次较少。
- 当前模型忽略了数据的周期性特征，改进模型需要结合领域知识，例如考虑前七天的数据进行预测。![[Pasted image 20250918172754066.png]]



- 初始模型 (y = b + Ws1) 预测效果不佳。
- 观察真实数据后，发现存在周期性规律 (7天循环)，改进模型为 y = b + Σ(wj * xj) (j=1 to 7)，其中 xj 代表 j 天前的观看人次，wj 为对应的权重。
- 改进后的模型在训练数据上的 Loss 为 0.38K，在未见过的数据上的 Loss 为 0.49K (原始模型为 0.58K)。
- 模型权重分析：w1 (前一天) 为正且较大 (0.79)，w2, w4, w5 为负，其他为正。
- 扩展模型考虑 28 天 (一个月) 的数据，训练数据 Loss 为 0.33K，未见过数据 Loss 为 0.46K。
- 进一步扩展到 56 天，训练数据 Loss 微降至 0.32K，未见过数据 Loss 保持 0.46K，效果提升有限。
- 以上模型均为线性模型 (linear model)，后续将探讨改进线性模型的方法。

---

# 原始语音转文字

好 那我们就开始上课吧那第一堂课啊是要简单跟大家介绍一下Machine Learning 还有 Deep Learning 的基本概念那等一下呢会讲一个跟宝可梦完全没有关系的故事告诉你机器学习还有深度学习的基本概念好 那什么是机器学习呢那我想必大家在报章杂志上其实往往都已经听过机器学习的故事 機器學習這個詞彙那你可能也知道說機器學習就是跟今天很熱門的AI好像有那麼一點關聯那所謂的機器學習到底是什麼呢顧名思義好像是說機器它具備有學習的能力那些科普文章往往把機器學習這個東西吹得玄之又玄好像機器會學習 學習以後我們就有了人工智慧有了人工智慧以後機器接下來就要統治人類了那機器學習到底是什麼呢事實上機器學習概括來說可以用一句話來描述機器學習這件事什麼叫機器學習呢機器學習就是讓機器具備找一個函式的能力那機器具備找函式的能力以後它可以做什麼樣的事情 它確實可以做很多事舉例來說假設你今天想要叫機器做語音辨識機器聽一段聲音產生這段聲音對應的文字 那你需要的就是一個函式這個函式的輸入是聲音訊號輸出是這段聲音訊號的內容那你可以想像說這個可以把聲音訊號當作輸入文字當作輸出的函式其實是一個很簡單的方法 顯然非常非常的複雜它絕對不是你只可以用人手寫出來的方程式這個函式它非常非常的複雜人類絕對沒有能力把它寫出來所以我們期待憑藉著機器的力量把這個函式自動找出來這件事情就是機器學習那剛才舉的例子是語音辨識還有好多好多的任務我們都需要找一個很複雜的函式舉例來說 假設我們現在要做影像辨識那這個影像辨識我們需要什麼樣的函式呢這個函式的輸入是一張圖片它的輸出是什麼呢它是這個圖片裡面有什麼樣的內容或者是大家都知道的alpha go其實也可以看作是一個函式要讓機器下文期我們需要的就是一個函式這個函式的輸入是棋盤上黑子跟白子的位置輸出是什麼 输出是机器下一步应该落子的位置假设你可以找到一个韩式这个韩式的输入就是棋盘上黑子跟白子的位置输出就是下一步应该落子的位置那我们就可以让机器自动下围棋这件事就可以做一个阿法狗那随着我们要找的韩式不同机器学习有不同的类别那这边介绍几个专有名词 給大家認同的那隨著我們要找的函數不同啊就可以做一個 AlphaGo那隨著我們要找的函式不同啊機器學習有不同的類別那這邊介紹幾個專有名詞給大家認識一下第一個專有名詞叫做Regret給大家那隨著我們要找的函式不同就可以做一個AlphaGo那隨著我們要找的函式不同啊機器學習有不切顧應該落子的位置那我們就可以讓機器做自行學習 自動下落子 黑子跟白子假設你可以找到一個函式這個函式的輸入就是棋盤上黑子跟白子的位置輸出就是下一步應該落子的位置那我們就可以讓機器做自動下圍棋這件事就可以做一個AlphaGo那隨著我們要找的函式不同機器學習有不同的類別那這邊介紹幾個專有名詞給大家認識一下第一個專有名詞 這個名詞叫做 regressionregression 的意思是說假設我們今天要找的函式它的輸出是一個數值它的輸出是一個 scalar那這樣子的機器學習的任務我們稱之為 regression那這邊舉一個 regression 的例子假設我們今天要機器做的事情是預測未來某一個時間的 PM 2.5 的數值你要叫機器做的事情是找一個函式這個我們用 Apple 來表示這個函式的輸入 PM2.5 的數值他的輸入可能是種種跟預測 PM2.5有關的指數包括今天的 PM2.5 的數值今天的平均溫度今天平均的臭氧濃度等等這個函式可以拿這些數值當做輸入輸出明天中午的 PM2.5 的數值那這一個找這個函式的任務叫做Return Regression那還有別的任務嗎還有別的任務除了Regression以外另外一個大家耳熟能詳的任務呢叫做Classification那Classification這個任務要進行做的是選擇力我們人類先準備好一些選項這些選項呢又叫做類別又叫做Class我們現在要找的函式它的輸出啊就是從我們設定好的選項裡面選擇 選擇一個當作輸出這個問題這個任務就叫做classification舉例來說現在每個人都有Gmail account那Gmail account裡面呢有一個函式這個函式可以幫我們偵測一封郵件是不是垃圾郵件這個函式的輸入是一封電子郵件那它的輸出是什麼呢你要先準備好你要機器選的選項在偵測垃圾郵件這個問題裡面可能的選項會是 就是兩個是垃圾郵件或不是垃圾郵件Yes 或是 No那機器要從 Yes 跟 No 裡面選一個選項出來這個問題叫做 Classification那 Classification 不一定只有兩個選項也可以有多個選項舉例來說 AlphaGo 本身也是一個 Classification 的問題那只是這個 Classification 它的選項是比較多的如果要機器下文 你想做一個AlphaGo的話我們要給機器多少個選項呢你就想想看棋盤上有多少個位置那我們知道棋盤上有19x19個位置那叫機器下圍棋這個問題其實就是一個有19x19個選項的選擇題你要叫機器做的就是找一個函式這個函式的輸入是棋盤上A子跟白子的位置輸出就是從19x19個選項裡面選出一個正確的選項 從 19 乘 19 個可以落子的位置裡面選出下一步應該要落子的位置這個問題也是一個分類的問題那其實很多教科書啊在講機器學習的種種不同類型的任務的時候往往就講到這邊告訴你說機器學習兩大類任務一個叫做 Regression一個叫做 Regression 一個叫做 classification然後就結束了但是假設你對機器學習的認知只停留在機器學習就是兩大類任務regression 跟 classification那就好像你以為說這個世界只有五大洲一樣但你知道這個世界不是只有五大洲對不對這個世界不是外面是有一個黑暗大陸的這鬼滅之刃連載之前我們就已經出發前往黑暗大陸了鬼滅之刃連載以後我們居然都還沒有到我見 這個黑暗大陸距離不遠那在機器學習這個領域裡面所謂的黑暗大陸是什麼呢在regression跟transportation以外大家往往害怕碰觸的問題叫做structurality也就是機器今天不只是要做選擇題不只是輸出一個數字它要產生一個有結構的物件舉例來說機器畫一張圖寫一篇文章這種叫機器產生有結構物件 结构的东西的这个问题就叫做 Structure Learning那如果要讲的比较擬人化比较潮一点Structure Learning你可以用擬人化的讲法说我们就是要叫机器学会创造这件事情好那到目前为止我们就是讲了三个机器学习的任务RegressionClassification 跟 Structure Learning接下来我们要讲的是那我们说机器学习就是要找一个函式那机器怎么办 怎么找一个函式呢这边要用个例子跟大家说明说机器怎么找一个函式这边的例子是什么呢这边的例子在讲这个例子之前先跟大家说一下说这一门课有一个YouTube的频道然后我会把上课的录影放到YouTube的频道上面这个频道感谢过去修过这门课的同学不嫌弃其实也蛮多人 所以我算是一個三流的 YouTuber是沒有什麼太多流量但是也是有這邊說7萬多訂閱了那為什麼突然提到這個YouTuber的頻道呢因為我們等一下要舉的例子跟YouTuber是有關係的那你知道身為一個YouTuberYouTuber在意的東西是什麼呢YouTuber在意的就是這個頻道的流量對不對假設有一個YouTuber是靠著YouTuber維生的他會在意說頻道有沒有流量 流量 這樣他才會知道他可以獲利多少所以我在想說我們有沒有可能找一個函式這個函式他的輸入是YouTube後台的資訊輸出是這個頻道隔天的總點閱率總共有多少假設你自己有YouTube頻道的話你會知道說在YouTube後台你可以看到很多相關的資訊比如說每一天按讚的人數有多少每天訂閱的人數有多少 每天觀看的次數有多少我們能不能夠根據一個頻道過往所有的資訊去預測他明天有可能的觀看的次數是多少呢我們能不能夠找一個函式這個函式的輸入是YouTube上面YouTube後台所有的資訊輸出就是某一天隔天這個頻道會有的觀看的次數呢那你可能會問說為什麼要做這個 如果我有盈利的話我可以知道我未來可以賺到多少錢但我其實沒有開盈利所以我也不知道為什麼東西要做這個就是了完全沒有任何卵用我單純就是想舉一個例子而已好那接下來啊我們就要問怎麼找出這個函式呢 怎麼找這個函式 F 輸入是 YouTube 後排的資料輸出是這個頻道隔天的點閱的總人數呢那機器學習找這個函式的過程分成三個步驟那我們就用 YouTube 頻道點閱人數預測這件事情來跟大家說明這三個步驟是怎麼運作的第一個步驟是我們要寫出一個帶有未知參數的函式 簡單來說就是我們先猜測一下我們打算找的這個函是 f它的數學式到底長什麼樣子舉例來說我們這邊先做一個最初步的猜測這個 f 長什麼樣子呢這個數目跟 y 之間有什麼樣的關係呢我們寫成這個樣子y 等於 b 加 w 乘以 xy這邊的每一個數值是什麼呢這個 y 就假設是今天吧 這個Y因為今天還沒有過完所以我們還不知道今天總共的點閱次數是多少所以這件事情是我們未知的Y是我們準備要預測的東西我們準備要預測的是今天2月26號這個頻道總共觀看的人數那X1是什麼呢X1是這個頻道前一天總共觀看的人數Y跟X1都是主持都是我們這個Y呢是我們準備要預測的東西 而 X1 是我們已經知道的資訊那 B 跟 W 是什麼呢B 跟 W 是未知的參數它是準備要透過資料去找出來的我們還不知道 W 跟 B 應該是多少我們只是隱約的猜測說那這個猜測為什麼會有這個猜測呢這個猜測往往就來自於你對這個問題的紙上的瞭解也就是 Format Knowledge所以你常常會聽到有人說 做機器學習你就需要一些 Domain Knowledge這個 Domain Knowledge 通常是用在哪裡呢這個 Domain Knowledge 就是用在你寫這個帶有位置數的函數的時候所以我們怎麼知道說這個能夠預測未來點閱次數的函數 F它就一定是前一天的點閱次數乘上 W 再加上 E 呢我們其實不知道 這是一個猜測也許我們覺得說 今天的點閱次數總是會跟昨天的點閱次數有點關聯吧所以我們把昨天的點閱次數乘上一個數值但是總是不會一模一樣所以再加上一個 B 做修正當作是對於2月26號點閱次數的預測那這是一個猜測它不一定是對的我們等一下回頭會再來修正這個猜測好那現在總之我們就隨便猜說 Y 等於 B 加 W 乘以 X1和 B 跟 W 是未知的這個帶有未知的參數這個 Parameter 中文通常翻成參數這個帶有 Unknown Parameter 的這個 Function我們就叫做 Model所以常常聽到有人說模型 Model 這個東西Model 這個東西在機器學習裡面就是一個帶有未知的 Parameter 的 Function那這個 S1 是這個 Function 裡面我們已經知道的東西它是來自於 YouTube 後台資訊我們已經知道2月25號點閱的總人數是多少這個東西叫做 Feature而 W 跟 B 是我們不知道的他是 Unknown 的 Parameter那這邊我們也給 W 跟 B 呢給他一個名字這個跟 Feature 做相乘的未知的參數這個 W 我們叫他 Weight這個沒有跟 Feature 相乘的是直接加上去的這個我們叫他 Bias而這個只是一些名詞的代表 定義而已讓等一下我們講課的時候在稱呼模型裡面的每一個東西的時候會更為方便好那這個是第一個步驟好那第二個步驟是什麼呢第二個步驟 這個步驟呢 是我們要定義一個東西叫做 loss什麼是 loss 呢loss 啊 它也是一個 function那這個 function 它的輸入是我們 model 裡面的參數我剛才已經把我們的 model 寫出來了對不對我們的 model 叫做 y 等於 b 加 w 乘以 x1而 b 跟 w 是未知的 是我們準備要找出來的那所謂的 Loss 它是一個 function這個 function 的輸入是什麼這個 function 的輸入就是 b 跟 w所以 l 它是一個 function它的輸入是 parameter是 model 裡面的 parameter那這個 loss 這個 function 的輸出的值代表什麼呢這個 function 輸出的值代表說現在如果我們把這一組未知的參數設定某一個數值的時候這個數值好還是不好那這樣講 可能你觉得有点抽象所以我就举一个具体的例子假设现在我们给未知的参数的设定是B这个bias等于0.5k这个W呢直接等于1那这个Loss怎么计算呢如果我们B设0.5k这个W设1那我们拿来预测未来的这个点阅次数的函式啊就变成Y等于0.5k 0.5K加1倍的X1那這樣子的一個函式這個0.5K跟1他們所代表的這個函式他有多好呢這個東西就是Loss那在我們的問題裡面我們要怎麼計算這個Loss呢這個我們就要從訓練的資料來進行計算在這個問題裡面我們的訓練資料是什麼呢我們的訓練資料是這個頻道過去的點閱次數 從2017年到2020年的點閱次數每天的這個頻道點閱次數都知道嗎這邊是假的數字啦隨便亂編的好那我們知道2017年1月1號到2020年12月31號的點閱數字是多少接下來我們就可以計算MOS怎麼計算呢我們把2017年1月1號的點閱次數代入這一個盤勢裡面 我們已經說我們想要知道D設定為0.5KW設定為1的時候這個函數有多棒當B設定為0.5KW設定為1的時候我們拿來預測的這個函數是Y等於0.5K加1倍的X1那我們就把這個X1帶4.8K看看預測出來的結果是多少所以根據這個函數根據D設0.5KW設1的這個函數如果1月1號是4.8K的點閱次數的話 隔天應該是 4.8K 乘以 1 加 0.5K也就是 5.3K 的點閱次數隔天實際上的點閱次數1月2號的點閱次數我們知道嗎從後台的資訊裡面我們是知道的所以我們可以比對一下現在這個韓式預估的結果跟真正的結果它的差距有多大這個韓式預估的結果是 5.3K真正的結果是多少呢真正的結果是 4.9K是高估了這個頂到的可能性 點閱的人數就可以計算一下差距計算一下估測的值跟真實的值的差距這邊估測的值用Y來表示真實的值用Yhat來表示你可以計算Y跟Yhat之間的差距得到一個1萬代表估測的值跟真實的值之間的差距那計算差距其實有不只一種方式我們這邊把Y跟Yhat相減直接取絕對值算出來的值是0.4K好的我們今天就聊到這兒 不是只有1月1號跟1月2號的資料我們有2017年1月1號到2020年12月31號總共三年的資料那這個真實的值啊叫做 Label所以常常聽到有人說做機器學習就需要 LabelLabel 指的就是正確的數值這個東西叫做 Label好那我們不是只能夠看用1月1號來預測1月2號的值我們可以用1月2號的值來預測1月3號的值 如果我們現在的函數是Y等於0.5K加1倍的X1那1月2號根據1月2號的點閱次數預測的1月3號的點閱次數的值是多少呢是5.4K以X1大概4.9K進去乘以1倍加0.5K等於5.4K接下來計算這個5.4K跟真正的答案跟Label之間的差距Label是7.5K看來是一個低估了這個頻道在1月3號的時候的點閱次數大概可以算出1.5K 這個EQ是Y-Y和Y hat之間的差距算出來是2.0K那同樣的方法你就可以算過這三年來每一天的預設的誤差假設我們今天的function是Y等於0.5K加1倍的XY這三年來每一天的誤差通通都可以算出來每一天的誤差都可以給我們一個小E好那接下來我們就把每一天的誤差通通加起來加起來然後取一個平均 這個N代表我們的訓練資料的個數訓練資料的個數就是三年來的訓練資料所以就365乘以3每年365兼3年所以365乘以3那我們算出一個L我們算出一個大L這個大L是每一筆訓練資料的誤差這個E相加以後的結果這個大L就是我們的Loss這個大L越大代表說我們現在這一組參數越不好這個大L越小 代表我們現在這一組參數約好那這個1啊就是計算這個估測的值跟實際的值之間的差距其實有不同的計算方法在我們剛才的例子裡面我們是算Y跟Y hat 絕對值的差距這種計算差距的方法得到的這個大條啊 得到的 loss 啊叫做 mean absolute error所寫是 M A E那在作業1裡面啊我們是算Y跟Y hat 相減以後的平方而論文裡面 今天的E是用相減用平方算出來的這個叫 mean square error 叫 MSE那 MSE 好那我們算出一個跟真正的答案跟 label 之間的差距label 是7.5的時候的根本就可以今天的方法通通都算出來每一天的誤差都可以給我們一個小E好那接下來我們就把每一天的誤差通通加起來加起來然後取一個平均這個大N呢代表我們的訓練資料的次數的個數讓我們訓練資料的個數就是三連半 訓練資料嘛所以就365乘以3啦每年365天三年所以365乘以3好那我們算出一個L我們算出一個大L這個大L是每一筆訓練資料的誤差這個E啊相加以後的結果這個大L就是我們的Loss這個大L越大代表說我們現在這一組參數越不好這個大L越小代表我們現在這一組參數越好那這個E啊就是計算這個誤差的值跟實際的值 其實這詞之間的差距其實有不同的計算方法在我們剛才的例子裡面我們是算 y 跟 y hat 絕對值的差距這種計算差距的方法得到的這個大條啊得到的 loss 啊叫做 mean absolute error所寫是 n a e那在作業一裡面呢我們是算 y 跟 y hat 相減以後的平方如果你今天的一是用相減以後的平方算出來的這個叫 mean square error 叫 n s e那 n s e 跟 n a e 他們其實有非常微妙的差別啦那通常你要選擇用哪一種方法來衡量距離那是看你的需求看你對這個任務的理解在這邊呢我們就不往下細講反正我們就是選擇了 MSE作為我們計算這個誤差的方式把所有的誤差加起來就得到 loss那你要選擇 MSE也是可以的在作業裡面我們會用 MSE那有一些任務有如果 Y 跟 Y hat它都是 都是几率分布的话在这个时候你可能会选择Course Entropy这个我们都之后再说反正我们这边就是选择了MADE好的这个是机器学习的第二步那我刚才许的那些数字不是 是真正的例子但是在這門課裡面我們在講課的時候就是要舉真正的例子給你看所以你以下的數字是真實的例子是這個頻道真實的後台的數據所計算出來的結果好那我們可以調整不同的 W我們可以調整不同的 B窮取各種 W 窮取各種 B我組合起來以後我們可以為不同的 W 跟 B 的組合都去計算他的 Loss然後就可以畫出 以下這一個等高線圖在這個等高線圖上面越偏紅色系代表計算出來的Loss 越大就代表說這一組 W 跟 B 越差如果越偏藍色系就代表 Loss 越小就代表這一組 W 跟 B 越好那這一組 W 跟 B放到我們的 Function 裡面放到我們的 Model 裡面那我們的預測會越精準所以就可以所以就知道說假設 加六代-0.25 這個 B 帶負 5 版就代表說這個 W 帶負 0.25 B 帶負 5 版就代表說這個頻道每天看的人越來越少而且 Loss 是很大的只能真實的狀況不太好如果 W 帶 0.75 B 帶 5 版那這個正確率會這個估測會比較精準那估測最精準的地方看起來應該是在這裡如果你今天 W 帶一個很接近 1 的值B 帶一個小小的值比如說 100 多 那這個時候估測是最精準的那這跟大家預期可能是比較接近的就是你拿前一天的點閱的總次數去預測隔天的點閱的總次數那可能前一天跟隔天的點閱的總次數其實差不多的所以答覆是1然後P設一個小一點的數值也許你的估測就會蠻精準的那像這樣子的一個等高線圖啊就是你試著試了不同的參數然後計算它的 North 這個高線圖叫做 Arrow 的 Surface好,那這個是機器學習的第二步接下來我們進入機器學習的第三步那第三步要做的事情其實是解一個最佳方法 最佳化的問題那如果你知道最佳化的問題是什麼的話也沒有關係我們今天要做的事情就是找一個 W 跟 B把未知的參數找一個數值出來看帶哪一個數值進去可以讓我們的大 L讓我們的 loss 值最小那個就是我們要找 W 跟 B那這個可以讓 loss 最小的 W 跟 B我們就叫做 W 是大跟 B 是大代表說他們是最好的一組 W 跟 B可以讓 loss 的值最小好 那这个东西要怎么做呢在这门课里面我们唯一会用到的Optimization 的方法叫做 Gradient Descent那这 Gradient Descent 这个方法怎么做呢它是这样做的为了要简化起见我们先假设我们位置的参数只有一个就是 W我们先假设没有 B 那个位置的参数只有 W 这个位置的参数那当我们 W 在不同的数值的时候我们就会得到不同的 Loss那这条曲线 就是Aero Surface刚才在前一个例子里面我们看到的Aero Surface是二维的是二滴的那这边只有一个参数所以我们看到的这个Aero Surface是一滴的 那怎麼樣找一個W去讓這個Loss的值最小呢?首先你要隨機選取一個初始的點這個初始的點我們叫做W0這個初始的點往往真的就是隨機的就是隨便選一個真的都是隨機的在往後的課程裡面我們其實會看到也許有些方法可以給我們一個比較好的W0的值那我們先不講這件事 我們先當作就是隨機的隨便值個骰子隨機決定說 W0 的值應該是多少那假設我們隨機決定的結果是在這個地方那接下來啊你將計算說這個在 W 等於 W0 的時候W 這個參數對 loss 的微分是多少那我假設你知道微分是什麼你知道微分是什麼這對你來說不是個問題就計算 W 對 loss 的微分是多少如果你不知道微分是什麼的話 那沒有關係 反正我們做的事情就是計算在這一個點在W0這個位置的這個Aero Surface的前線斜率也就是這一條藍色的虛線它的斜率那如果這一條虛線的斜率是負的那代表什麼意思呢代表說左邊比較高 右邊比較低代表在這個位置附近左邊比較高 右邊比較低那如果左邊比較高 右邊比較低的話那我們要做什麼樣的事情呢如果 左邊比較高右邊比較低的話那我們就把 W 的值變大那我們就可以讓 loss 變小如果算出來的斜率是正的就代表說左邊比較低右邊比較高是這個樣子左邊比較低右邊比較高如果左邊比較低右邊比較高的話那就代表左代表我們把 W 變小把 W 往左邊移我們可以讓 loss 的值變小那這個時候你就應該把 W 的值變小那假設你連斜率是什麼的話是什麼都不知道的話也沒有關係 你就想像說有一個人站在這個地方然後他左右環視一下那這個算為分這件事就是左右環視他會知道說左邊比較高還是右邊比較高看哪邊比較低他就往比較低的地方跨出一步那這一步要跨多大呢這一步的步伐的大小取決於兩件事情第一件事情是這個地方的斜率有多大這個地方斜率大這個步伐就會跨大一點斜率小步伐就 就化小一點另外除了斜率以外就是除了這個微分這一項微分這一項我剛才說他就代表斜率除了微分這一項以外這個還有另外一個東西會影響步伐大小這個東西我們這邊用 Eta 來表示這個 Eta 叫做 Learning Rate叫做學習數據這個 Learning Rate 啊他是怎麼來的呢他是你自己設定的你自己決定這個 Eta 的大小如果 Eta 設大一點那你每次參數 update 就會量很大你的學習可能就比較快如果Eta是小一點那你參數的update就很慢每次都會只會改變一點點參數的數值那這種你在做機器學習需要自己設定的東西叫做Hyperparameter這個我們剛才講說機器學習的第一步就是訂一個有未知參數的function而這些參數這些未知的參數是機器自己找出來的但是有 請說有個問題好 請說為什麼Loss可以是負的定義上不是只能是正的嗎好 這其實是一個好的問題我複述一下這個問題有同學問說為什麼Loss可以是負的呢Loss這個函數是你自己定義的所以在剛才我們的定義裡面我們說Loss就是估測的值跟正確的值它的絕對值那如果根據剛才Loss的定義那麼它不可能是正的 不可能是負的但是 Loss 這個 function 是你自己決定的你可以說我今天要決定一個 Loss function就是絕對只在減100那你可能就負的所以我這邊這個 curve我這邊可能剛才忘了跟大家說明說這個 curve 並不是一個真實的 Loss它是我隨便亂舉的一個例子因為在我今天想要舉一個比較 general 的 case它並不是一個真實任務的 error surface所以這個 Loss 的這個 curve這個 error surface它可以是 任何形狀我們這邊沒有預測立場說他一定要是什麼形狀但是確實在真實在剛才這個如果 loss 的定義就跟我們剛才定的一樣是絕對值那他就不可能是負值但是 loss 這個 function 是你自己決定的所以他有可能是負的好既然有同學問問題我們就在這邊停一下看大家有沒有問題想問然後住校以後會幫我看那個YouTube 的直播來 有人在直播上問問題嗎?如果有的話,你就幫我念一下 你先看好以後再唸給我聽我們就先繼續講我們等下講到一個段落再來繼續回答大家的問題再問一下現場同學有沒有同學想要問問題呢好沒有的話就請容我繼續講好那剛才講到哪裡呢剛才講到還不夠 Hyperparameter 這個東西Hyperparameter 是你自己設的所以在機器學習的這整個過程中你需要自己設定的這個東西就叫做 Hyperparameter好 那我們說我們要把 W0 往右移一步那這個新的位置就叫做 W1那這一步的步伐是 Eta 乘上微分的結果那如果你要用數學式來表示它的話就是把 W0 減掉 Eta 乘上微分的結果 得到 W1好那接下來你就是反覆進行剛才的操作你就計算一下 W1 這個微分的結果然後呢 再決定現在要把 W1 移動多少然後再移動到 W2然後你再繼續反覆做同樣的操作不斷的把 W 移動位置最後你會停下來什麼時候會停下來呢?往往有兩種狀況第一種狀況是你失去耐心了一開始會設定說 我今天在調整我的參數的時候我在計算我的微分的時候我最多計算幾次你可能會設說我的上限就是設定100萬次所以我的參數更新100萬次以後我就不再更新了那至於要更新幾次這個也是一個Hyperparameter這個是你自己除非你的Data就是明天那你可能更新的次數就設少一點不願意再下重新更新的次數就設多一點那還有另外一種理想上的停下來的可能是今天當我們不斷調整 參數調整到一個地方它的微分的值就是這項算出來正好是0的時候如果這項正好算出來是00乘上這個Learning Rate App還是0所以你的參數就不會在移動的位置好 那假設我們是一個理想的狀況我們把W0更新到W1再更新到W2最終更新到WT這電腦有點卡更新到WT 卡住了也就是算出來微分的值是0了 那就不會在參數的位置就不會再更新了那講到這邊你可能會馬上發現說Gradient Descent這個方法哇!有一個巨大的問題這個巨大的問題在這個例子裡面非常容易被看出來就是我們沒有找到真正最好的景我們沒有找到那個可以讓Loss最小的那個W在這個例子裡面把W設定在這個地方你可以讓Loss最小 但是如果 gradient descent 是從這個地方當作隨機初始的位置的話你很有可能走到這裡你的訓練就停住了你就沒有辦法再移動 w 的位置那這個位置這個真的可以讓這個 loss 最小的地方叫做 global 的 minima而這個地方叫做 local 的 minima它的左右兩邊都比這個地方的這個 loss 還要高一點但是它不是整個 error free Surface上面的最低點這個東西叫做 Local Minima最常可能會聽到有人講到 Gradient Descent就會說Gradient Descent 不是個好方法這個方法會有 Local Minima 的問題你沒有辦法真的找到 Global Minima但這裡教科書常常這樣講農場文常常這樣講但這個其實只是幻覺而已事實上假設你有做過深度學習相關的事情假設你有自己訓練或自己做過 做 gradient descent 的經驗的話其實 local minima 是一個假議題我們在做 gradient descent 的時候我們真正面對的難題不是 local minima那到底是什麼這個我們之後會再講到在這邊你就先接受先相信多數人的講法說 gradient descent 有 local minima 的問題在這個圖上 在這個例子裡面顯然有 local minima 的問題但之後會再告訴你說gradient descent 是只用一個參數的例子而已那我們實際上我們剛才模型有兩個參數有 W 跟 B那有兩個參數的情況下怎麼用 gradient descent 呢其實跟剛才一個參數沒有什麼不同如果一個參數你沒有問題的話你可以很快的推廣到兩個參數好我們現在有兩個參數那我們給他兩個參數我們都給他隨機的出設值就是 W0 跟 B0然後接下來呢你要計算 W 對 loss 的尾分你要計算 W 對 loss 的尾分 計算 B 對 L 的微分計算是在 W 等於 W0 的位置B 等於 B0 的位置在 W 等於 W0 的位置B 等於 B0 的位置你要計算 W 對 L 的微分計算 B 對 L 的微分計算完以後就根據我們剛才一個參數的時候的做法去更新 W 跟 B把 W0 減掉 learning rate 乘上微分的結果得到 W1把 B0 減掉 learning rate 乘上微分的結果得到 B1謝謝 有同學可能會問說這個微分這個要怎麼算啊在正版不會算微分的話不用緊張怎麼不用緊張呢在Deep Learning的Framework裡面或在我們作業議會用的PyTorch裡面這個算微分啊都是程式自動幫你算的你就Call一行你就寫一行程式自動就把微分的值就算出來了你就算完全不知道自己在幹嘛你還是可以算微分的 分子算出來所以這邊如果你根本就不知道微分是什麼不用擔心這一步驟就是一行程式這個等一下之後在作為一的時候大家可以自己體驗看看那就是反覆同樣的步驟就不斷的更新W跟B然後期待最後你可以找到一個最好的WW star 跟最好的B,B star好那這邊呢就是舉一下例子跟大家看一下說如果在這個問題上他操作 是什么样子那假设你随便选一个初始的值在这个地方那你就先计算一下这个W对L的微分计算一下B对L的微分然后接下来你要更新W跟B更新的方向就是W对L的微分乘以Eta再乘以一个负号B对L的微分再乘以Eta乘以一个负号那你算出这个微分的值你就可以决定更新的方向你就可以决定W要怎么更新B要怎么更新那把W跟B更新的方向结合起来 這是一個向量就是這個紅色的箭頭我們就從這個位置移到這個位置然後再計算一次微分然後你再決定要走什麼樣的方向把這個微分的值乘上learning rate再乘上負號你就知道紅色的箭頭指向哪裡你就知道 Jumping out. 最好 B B上做起來是什麼樣子假設你隨便選一個初始的值在這個地方你就先計算一下這個 W 對 L 的微分計算一下 B 對 L 的微分然後接下來你要更新 W 跟 B更新的方向就是W 對 L 的微分要更新的微分在這個如果在好那這邊呢W 打你 BW 跟 B然後期待最後你可以找到一個最好的 W打你上最好 B B上好那這邊 這邊就是舉一下例子跟大家看一下說這個等一下之後在作業一的時候大家可以自己體驗看看那就是反覆同樣的步驟就不斷的更新W跟B然後期待最後你可以找到一個最好的WW是R,最好是B,B是R好,那這邊就是舉一下例子跟大家 那就是反覆同樣的步驟就不斷的更新W跟B然後期待最後你可以找到一個最好的WW star 跟最好的B,B star好,那這邊呢就是舉一下例子跟大家看一下說如果在這個問題上他操作起來是什麼樣子那假設你隨便選一個出色的值在這個地方那你就先計算一下這個W對L的微分計算一下B對L的微分然後接下來你要更新 更新W跟B更新的方向就是W對L的微分乘以Eta再乘以一個負號B對L的微分再乘以Eta乘以一個負號那你算出這個微分的值你就可以決定更新的方向你就可以決定W要怎麼更新B要怎麼更新那把W跟B更新的方向集合起來它就是一個向量就是這個紅色的箭頭我們就從這個位置移到這個位置然後再計算一次微分然後你再決定要走什麼樣的方向把這個微分的值乘上 Running rate 再乘上負號你就知道紅色的箭頭要指向哪裡你就知道怎麼移動 R 跟 B 的位置一直移動期待最後可以找出一組不錯的 R 跟 B好那實際上呢真的用 gradient descent 進行一番計算以後這個是真正的數據啦我們算出來的最好的 W 是 0.97最好的 B 是 0.0跟我們猜測嘛 很接近的因為 X1 的值可能跟 Y 很接近所以這個 W 就設一個接近 1 的值B 就設一個比較偏小的值那 Loss 多大呢Loss 算一下是 0.48 K也就是在 2017 到 2020 年的資料上如果使用這個函式 B 代 0.1 KR 代 0.97那平均的誤差是 0.48 K也就是它的誤差的預測的關係 觀看人數不差大概是500人次左右好那講到目前為止 我們就講了機器學習的三個步驟第一個步驟寫出一個函式這個函式裡面是有未知數的第二個步驟定義一個叫做Loss 的 Function第三個步驟Jekyll Optimization Problem找到一組 W 跟 B 讓 Loss 最小W 跟 B 的值剛才找出來的那這一組 W 跟 B 可以讓 Loss 小到 0.48 K但是這樣是一個讓人滿意或值得稱道的結果 也許不是為什麼因為這三個步驟合起來叫做訓練我們現在是在我們已經知道答案的資料上去計算囉2017到2020年的資料我們已經知道啦我們其實已經知道2017到2020年每天的觀看次數所以我們現在其實只是在自嗨而已我們就是假裝我們不知道隔天的觀看次數然後拿錄音機 來進行預測發現誤差是0.48K但是我們真正要在意的是已經知道的觀看次數嗎不是我們要真正在意的是我們不知道未來的觀看次數是多少好所以我們接下來要做的事情是什麼呢就是拿這個函式來真的預測一下未來的觀看次數那這邊我們只有2017年到2020年的值我們在這個2020年的值 最後一天跨年夜的時候張出了這個函式接下來從2021年開始每一天我們都拿這個函式去預測隔天的觀看人次我們就拿2020年的12月31號的觀看人次去預測2021年元旦的觀看人次用2021年元旦的觀看人次預測一下2021年這個元旦隔天1月2號的觀看人次用1月2號觀看人次去預測1月3號的觀看人次 每天都做這件事一直做到2月14號就做到情人節然後得到平均的誤差值是多少呢這個是真實的數據的結果在2021年沒有看過的資料上這個誤差值是我們這邊用L'來表示它是0.58所以在有看過的資料上在訓練資料上誤差值是比較小的在沒有看過的資料上在2021年的資料上看起來會比較小 誤差值是比較大的那我們每天的平均誤差有580人左右 600人左右 Yo! 能不能夠做得更好呢在做得更好之前我們先來分析一下結果這個圖怎麼看呢這個圖的橫軸代表的是時間所以0這個點啊最左邊的點代表的是2021年1月1號最右邊的點代表的是2021年2月14號然後這個縱軸啊就是觀看的人次啊這邊是用千人當作單位紅色的線是什麼呢 紅色的線是真實的真實的這個觀看人次藍色的線是機器用這一個函式預測出來 你的觀看人次那你會發現很明顯的這個藍色的線沒什麼神奇的地方他就是幾乎就是紅色的線往右平移一天而已他其實也沒做什麼特別厲害的預測就把紅色的線往右平移一天因為這很合理嘛因為我們覺得F1也就是前一天的觀看人次跟隔天觀看人次的要怎麼拿前一天觀看人次去預測隔天觀看人次呢前一天觀看人次乘以0.97加上0.1K加上100就是隔天的觀看人次 所以你會發現說機器幾乎就是拿前一天的觀看人次來預測隔天的觀看人次但是如果你仔細觀察這個圖你就會發現這個真實的資料啊有一個很神奇的現象它是有週期性的啊它有神奇的週期性啊你知道這個週期是什麼嗎你知道這個你知道它每個 They say if it's too good to be true, it must be. But at WGU, you can get your degree with the same accreditations as the large universities. See why over 95% of employers say they would hire another WGU grad. And learn more at wgu.edu. 女司機現在可以向女司機要求女性的喜好Uber 跟YouTube背後神奇的演算法有關係比如說YouTube都會推頻道的影片 也許YouTuber在推平民道的影片的時候他都選擇禮拜五禮拜六不推只推禮拜天到禮拜四可是為什麼推禮拜天到禮拜四呢這個我也不了解但是反正看出來的結果我們看真實的數據就是這個樣子每隔七天一個循環禮拜五禮拜六看的人就是特別少所以既然我們已經知道每隔七天都是一個循環那這一個式子這一個model顯然很爛因為他只能夠看前一天如果我知道每隔七天開一個循環 我們應該要看七天對不對我們如果我們一個模型他是參考前七天的資料把七天前的資料直接複製到拿來當作預測的結果也許預測的會更準也說不定所以我們就要修改一下我們的模型那通常你對模型的修改往往來自於你對這個問題的理解也就是 Domain Knowledge所以一開始我們對問題完全不理解的時候我們就胡亂寫個 Y 等於 B 加 W S1並沒有做得特別好接下來 我們觀察了真實的數據以後得到的結論我們的模型更準也說不定所以我們就要修改一下我們的模型通常你對模型的修改往往來自於你對這個問題的理解也就是 Domain Knowledge所以一開始我們對問題完全不理解的時候我們就胡亂寫個 Y 等於 B 加 W S1並沒有做得特別好接下來我們觀察了真實的數據以後得到的結論是每個期間有一個循環所以我們應該要把前期 7天的觀看人次都列入考慮所以我們寫了一個新的模型這個模型長什麼樣子呢這個模型就是 y 等於 b 加 xjxj 代表什麼這個下標 j 代表是幾天前然後這個 j 等於 1 到 7也就是從一天前兩天前一直考慮到7天前那7天前的資料通通乘上不同的 weight乘上不同的 wj 加起來再加上 IAM得到預測的結果如果這個是我們的model那我們得到的結果是怎麼樣呢我們在訓練資料上的Loss是0.38K那因為這邊只考慮一天嘛這邊考慮七天嘛所以在訓練資料上你會得到比較低的Loss這邊考慮比較多的資訊在訓練資料上應該要得到更好的更低的Loss這邊算出來是0.38K但是在他沒有看過的資料上面做不做得好呢在你沒有看到的資料上 有比较好是0.49K所以刚才只考虑一天是0.58K的误差考虑七天是0.49K的误差那这边每一个W跟B我们都会用规律计算算出它的最佳值它的最佳值长什么样子呢这边秀出来给你看它的最佳值长这个样子当然机器的逻辑我是有点没有办法了解我本来以为他会选七天前的数据十七天前的这个观看人数值直接复制过来我看来还没有将来 他的邏輯是前一天跟你要預測的隔天的數值的關係很大所以W1是0.79那不知道為什麼他還考慮前三天前三天是0.12然後前六天是0.3前七天是0.18不過他知道說如果是前兩天前四天前五天他的值呢會跟未來我們要預測的隔天的值是成反比的所以W2 W4跟W5他們最佳的值 讓 Loss 可以在訓練資料上是 0.38 K 的值是負的但是 W1 W3 W6 跟 W7 是正的我們考慮前七天的值那你可能會問說能不能夠考慮更多天呢可以這個輕易的改考慮更多天本來只考慮前七天考慮二十八天會怎麼樣呢二十八天就一個月嘛考慮前一個月每一天的觀看人次去預測隔天的觀看人次預測出來結果怎樣呢訓練資料上是 0.33K那在2021年的資料上在沒有看過的資料上是0.46K看起來又更好一點好28天那接下來考慮56天會怎麼樣呢在訓練資料上是稍微再好一點是0.32K在沒看過資料上還是0.46K看起來考慮更多天沒有辦法再更進步了看來考慮天數這件事也許已經到了一個極限好那這邊呢這些模型 他們都是把書的這個X這個X還記得他叫什麼嗎?他叫做feature把feature乘上一個weight再加上一個bias就得到預設的結果這樣的模型 有一個共同的名字叫做linear model那我們接下來會看怎麼把linear model做得更好linear 的 model 也許太過簡單了怎麼說它太過簡單呢怎麼說它太過簡單 you 有更好一點好 28天那接下來考慮56天會怎麼樣呢在訓練資料上才更進步好那這邊呢這些模型他們feature乘上一個weight再加上一個bias就得到預測的結果這樣的模型有一個共同的名字叫做linear model好那我們接下來會看怎麼把linear model做得更好linear 的model也許太過簡單了怎麼說他太過簡單呢怎麼說他太過簡單了