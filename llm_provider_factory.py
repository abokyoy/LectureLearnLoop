#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
LLMæä¾›å•†å·¥å‚ - ç»Ÿä¸€ç®¡ç†æ‰€æœ‰LLMè°ƒç”¨
"""

import logging
import requests
import json
import time
from typing import Optional, Dict, Any
from config import load_config
from llm_call_logger import start_llm_call, record_first_byte, end_llm_call


class LLMProvider:
    """LLMæä¾›å•†åŸºç±»"""
    
    def __init__(self, config: Dict[str, Any], logger: Optional[logging.Logger] = None):
        self.config = config
        self.logger = logger or logging.getLogger(__name__)
        
    def call(self, prompt: str, context: str = "") -> Optional[str]:
        """è°ƒç”¨LLM API"""
        raise NotImplementedError("å­ç±»å¿…é¡»å®ç°callæ–¹æ³•")
    
    def test_connection(self) -> tuple[bool, str]:
        """æµ‹è¯•è¿æ¥"""
        try:
            result = self.call("è¯·å›å¤'è¿æ¥æµ‹è¯•æˆåŠŸ'")
            if result:
                return True, f"âœ… è¿æ¥æµ‹è¯•æˆåŠŸ\nå›å¤å†…å®¹: {result[:100]}{'...' if len(result) > 100 else ''}"
            else:
                return False, "âŒ è¿æ¥æµ‹è¯•å¤±è´¥: æ— å“åº”"
        except Exception as e:
            return False, f"âŒ è¿æ¥æµ‹è¯•å¼‚å¸¸: {str(e)}"


class OllamaProvider(LLMProvider):
    """Ollamaæä¾›å•†"""
    
    def __init__(self, config: Dict[str, Any], logger: Optional[logging.Logger] = None):
        super().__init__(config, logger)
        self.api_url = config.get("ollama_api_url", "http://localhost:11434/api/generate")
        self.model = config.get("ollama_model", "deepseek-r1:1.5b")
        
    def call(self, prompt: str, context: str = "") -> Optional[str]:
        """è°ƒç”¨Ollama API"""
        # å¼€å§‹è®°å½•è°ƒç”¨æ—¥å¿—
        call_id = start_llm_call("Ollama", self.model, prompt, context, self.api_url)
        
        self.logger.info(f"è°ƒç”¨Ollama API: {self.api_url}, æ¨¡å‹: {self.model}")
        
        try:
            response = requests.post(self.api_url, json={
                "model": self.model,
                "prompt": prompt,
                "stream": False
            }, timeout=120)
            
            # è®°å½•é¦–å­—èŠ‚æ—¶é—´
            record_first_byte(call_id)
            
            response.raise_for_status()
            data = response.json()
            ai_response = data.get("response", "").strip()
            
            if ai_response:
                self.logger.info(f"Ollamaå›å¤æˆåŠŸï¼Œé•¿åº¦: {len(ai_response)}")
                # è®°å½•æˆåŠŸè°ƒç”¨
                end_llm_call(call_id, ai_response, True, "", response.status_code)
                return ai_response
            else:
                self.logger.warning("Ollamaè¿”å›ç©ºå›å¤")
                # è®°å½•å¤±è´¥è°ƒç”¨
                end_llm_call(call_id, "", False, "è¿”å›ç©ºå›å¤", response.status_code)
                return None
                
        except requests.exceptions.ConnectionError as e:
            error_msg = f"Ollamaè¿æ¥å¤±è´¥: è¯·æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦è¿è¡Œåœ¨ {self.api_url}"
            self.logger.error(f"Ollamaè¿æ¥å¤±è´¥: {e}")
            end_llm_call(call_id, "", False, error_msg, 0)
            raise Exception(error_msg)
        except requests.exceptions.Timeout as e:
            error_msg = "Ollamaè¯·æ±‚è¶…æ—¶: è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–å¢åŠ è¶…æ—¶æ—¶é—´"
            self.logger.error(f"Ollamaè¯·æ±‚è¶…æ—¶: {e}")
            end_llm_call(call_id, "", False, error_msg, 0)
            raise Exception(error_msg)
        except Exception as e:
            error_msg = f"Ollamaè°ƒç”¨å¤±è´¥: {str(e)}"
            self.logger.error(error_msg)
            status_code = getattr(e, 'response', {}).get('status_code', 0) if hasattr(e, 'response') else 0
            end_llm_call(call_id, "", False, error_msg, status_code)
            raise Exception(error_msg)


class GeminiProvider(LLMProvider):
    """Geminiæä¾›å•†"""
    
    def __init__(self, config: Dict[str, Any], logger: Optional[logging.Logger] = None):
        super().__init__(config, logger)
        self.api_key = config.get("gemini_api_key", "").strip()
        self.model = config.get("gemini_model", "gemini-1.5-flash-002").strip()
        
        if not self.api_key:
            raise ValueError("Gemini API Keyæœªé…ç½®ï¼Œè¯·åœ¨è®¾ç½®ä¸­é…ç½®API Key")
    
    def call(self, prompt: str, context: str = "") -> Optional[str]:
        """è°ƒç”¨Gemini API"""
        # å¼€å§‹è®°å½•è°ƒç”¨æ—¥å¿—
        api_url = f"https://generativelanguage.googleapis.com/v1beta/models/{self.model}:generateContent"
        call_id = start_llm_call("Gemini", self.model, prompt, context, api_url)
        
        self.logger.info(f"è°ƒç”¨Gemini APIï¼Œæ¨¡å‹: {self.model}")
        
        url = f"https://generativelanguage.googleapis.com/v1beta/models/{self.model}:generateContent?key={self.api_key}"
        headers = {"Content-Type": "application/json"}
        payload = {
            "contents": [
                {"parts": [{"text": prompt}]}
            ]
        }
        
        try:
            response = requests.post(url, headers=headers, json=payload, timeout=60)
            
            # è®°å½•é¦–å­—èŠ‚æ—¶é—´
            record_first_byte(call_id)
            
            if not response.ok:
                error_msg = f"Gemini APIè°ƒç”¨å¤±è´¥: HTTP {response.status_code}"
                if response.status_code == 401:
                    error_msg += " - API Keyæ— æ•ˆ"
                elif response.status_code == 403:
                    error_msg += " - APIé…é¢ä¸è¶³æˆ–æƒé™ä¸å¤Ÿ"
                self.logger.error(error_msg)
                raise Exception(error_msg)
            
            data = response.json()
            candidates = data.get("candidates", [])
            if candidates and "content" in candidates[0]:
                parts = candidates[0]["content"].get("parts", [])
                if parts and "text" in parts[0]:
                    result = parts[0]["text"].strip()
                    self.logger.info(f"Geminiå›å¤æˆåŠŸï¼Œé•¿åº¦: {len(result)}")
                    end_llm_call(call_id, result, True, "", response.status_code)
                    return result
            
            error_msg = "æ— æ³•è§£æGeminiå“åº”æ ¼å¼"
            self.logger.error("æ— æ³•è§£æGeminiå“åº”")
            end_llm_call(call_id, "", False, error_msg, response.status_code)
            raise Exception(error_msg)
            
        except requests.exceptions.ConnectionError as e:
            error_msg = "Geminiè¿æ¥å¤±è´¥: è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥"
            self.logger.error(f"Geminiè¿æ¥å¤±è´¥: {e}")
            end_llm_call(call_id, "", False, error_msg, 0)
            raise Exception(error_msg)
        except requests.exceptions.Timeout as e:
            error_msg = "Geminiè¯·æ±‚è¶…æ—¶: è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥"
            self.logger.error(f"Geminiè¯·æ±‚è¶…æ—¶: {e}")
            end_llm_call(call_id, "", False, error_msg, 0)
            raise Exception(error_msg)
        except Exception as e:
            if "API" in str(e):
                end_llm_call(call_id, "", False, str(e), 0)
                raise e
            error_msg = f"Geminiè¯·æ±‚å¼‚å¸¸: {str(e)}"
            self.logger.error(error_msg)
            end_llm_call(call_id, "", False, error_msg, 0)
            raise Exception(error_msg)


class DeepSeekProvider(LLMProvider):
    """DeepSeekæä¾›å•†"""
    
    def __init__(self, config: Dict[str, Any], logger: Optional[logging.Logger] = None):
        super().__init__(config, logger)
        self.api_key = config.get("deepseek_api_key", "").strip()
        self.model = config.get("deepseek_model", "deepseek-chat").strip()
        self.api_url = config.get("deepseek_api_url", "https://api.deepseek.com/v1/chat/completions").strip()
        
        if not self.api_key:
            raise ValueError("DeepSeek API Keyæœªé…ç½®ï¼Œè¯·åœ¨è®¾ç½®ä¸­é…ç½®API Key")
    
    def call(self, prompt: str, context: str = "") -> Optional[str]:
        """è°ƒç”¨DeepSeek API"""
        # å¼€å§‹è®°å½•è°ƒç”¨æ—¥å¿—
        call_id = start_llm_call("DeepSeek", self.model, prompt, context, self.api_url)
        
        self.logger.info(f"è°ƒç”¨DeepSeek API: {self.api_url}, æ¨¡å‹: {self.model}")
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }
        
        payload = {
            "model": self.model,
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "stream": False
        }
        
        try:
            response = requests.post(self.api_url, headers=headers, json=payload, timeout=60)
            
            if not response.ok:
                error_msg = f"DeepSeek APIè°ƒç”¨å¤±è´¥: HTTP {response.status_code}"
                if response.status_code == 401:
                    error_msg += " - API Keyæ— æ•ˆ"
                elif response.status_code == 403:
                    error_msg += " - APIé…é¢ä¸è¶³æˆ–æƒé™ä¸å¤Ÿ"
                self.logger.error(error_msg)
                raise Exception(error_msg)
            
            data = response.json()
            
            if "choices" in data and len(data["choices"]) > 0:
                content = data["choices"][0].get("message", {}).get("content", "").strip()
                if content:
                    self.logger.info(f"DeepSeekå›å¤æˆåŠŸï¼Œé•¿åº¦: {len(content)}")
                    end_llm_call(call_id, content, True, "", response.status_code)
                    return content
            
            error_msg = "æ— æ³•è§£æDeepSeekå“åº”æ ¼å¼"
            self.logger.error("æ— æ³•è§£æDeepSeekå“åº”")
            end_llm_call(call_id, "", False, error_msg, response.status_code)
            raise Exception(error_msg)
            
        except requests.exceptions.ConnectionError as e:
            error_msg = "DeepSeekè¿æ¥å¤±è´¥: è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥"
            self.logger.error(f"DeepSeekè¿æ¥å¤±è´¥: {e}")
            end_llm_call(call_id, "", False, error_msg, 0)
            raise Exception(error_msg)
        except requests.exceptions.Timeout as e:
            error_msg = "DeepSeekè¯·æ±‚è¶…æ—¶: è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥"
            self.logger.error(f"DeepSeekè¯·æ±‚è¶…æ—¶: {e}")
            end_llm_call(call_id, "", False, error_msg, 0)
            raise Exception(error_msg)
        except Exception as e:
            if "API" in str(e):
                end_llm_call(call_id, "", False, str(e), 0)
                raise e
            error_msg = f"DeepSeekè¯·æ±‚å¼‚å¸¸: {str(e)}"
            self.logger.error(error_msg)
            end_llm_call(call_id, "", False, error_msg, 0)
            raise Exception(error_msg)


class QwenProvider(LLMProvider):
    """é€šä¹‰åƒé—®æä¾›å•†"""
    
    def __init__(self, config: Dict[str, Any], logger: Optional[logging.Logger] = None):
        super().__init__(config, logger)
        self.api_key = config.get("qwen_api_key", "").strip()
        self.model = config.get("qwen_model", "qwen-flash").strip()
        self.api_url = config.get("qwen_api_url", "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation").strip()
        
        if not self.api_key:
            raise ValueError("é€šä¹‰åƒé—® API Keyæœªé…ç½®ï¼Œè¯·åœ¨è®¾ç½®ä¸­é…ç½®API Key")
    
    def call(self, prompt: str, context: str = "") -> Optional[str]:
        """è°ƒç”¨é€šä¹‰åƒé—® API"""
        # å¼€å§‹è®°å½•è°ƒç”¨æ—¥å¿—
        call_id = start_llm_call("Qwen", self.model, prompt, context, self.api_url)
        
        self.logger.info(f"è°ƒç”¨é€šä¹‰åƒé—® API: {self.api_url}, æ¨¡å‹: {self.model}")
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }
        
        payload = {
            "model": self.model,
            "input": {
                "messages": [
                    {"role": "user", "content": prompt}
                ]
            },
            "parameters": {
                "result_format": "message"
            }
        }
        
        try:
            response = requests.post(self.api_url, headers=headers, json=payload, timeout=60)
            
            # è®°å½•é¦–å­—èŠ‚æ—¶é—´
            record_first_byte(call_id)
            
            if not response.ok:
                error_msg = f"é€šä¹‰åƒé—® APIè°ƒç”¨å¤±è´¥: HTTP {response.status_code}"
                if response.status_code == 401:
                    error_msg += " - API Keyæ— æ•ˆ"
                elif response.status_code == 403:
                    error_msg += " - APIé…é¢ä¸è¶³æˆ–æƒé™ä¸å¤Ÿ"
                self.logger.error(error_msg)
                self.logger.error(f"å“åº”å†…å®¹: {response.text}")
                end_llm_call(call_id, "", False, error_msg, response.status_code)
                raise Exception(error_msg)
            
            data = response.json()
            
            if "output" in data and "choices" in data["output"] and len(data["output"]["choices"]) > 0:
                content = data["output"]["choices"][0].get("message", {}).get("content", "").strip()
                if content:
                    self.logger.info(f"é€šä¹‰åƒé—®å›å¤æˆåŠŸï¼Œé•¿åº¦: {len(content)}")
                    end_llm_call(call_id, content, True, "", response.status_code)
                    return content
            
            error_msg = "æ— æ³•è§£æé€šä¹‰åƒé—®å“åº”æ ¼å¼"
            self.logger.error("æ— æ³•è§£æé€šä¹‰åƒé—®å“åº”")
            self.logger.error(f"å“åº”æ•°æ®: {data}")
            end_llm_call(call_id, "", False, error_msg, response.status_code)
            raise Exception(error_msg)
            
        except requests.exceptions.ConnectionError as e:
            error_msg = "é€šä¹‰åƒé—®è¿æ¥å¤±è´¥: è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥"
            self.logger.error(f"é€šä¹‰åƒé—®è¿æ¥å¤±è´¥: {e}")
            end_llm_call(call_id, "", False, error_msg, 0)
            raise Exception(error_msg)
        except requests.exceptions.Timeout as e:
            error_msg = "é€šä¹‰åƒé—®è¯·æ±‚è¶…æ—¶: è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥"
            self.logger.error(f"é€šä¹‰åƒé—®è¯·æ±‚è¶…æ—¶: {e}")
            end_llm_call(call_id, "", False, error_msg, 0)
            raise Exception(error_msg)
        except Exception as e:
            if "API" in str(e):
                end_llm_call(call_id, "", False, str(e), 0)
                raise e
            error_msg = f"é€šä¹‰åƒé—®è¯·æ±‚å¼‚å¸¸: {str(e)}"
            self.logger.error(error_msg)
            end_llm_call(call_id, "", False, error_msg, 0)
            raise Exception(error_msg)


class LLMProviderFactory:
    """LLMæä¾›å•†å·¥å‚ç±»"""
    
    _instance = None
    _provider = None
    _config_hash = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def get_provider(self, force_reload: bool = False) -> LLMProvider:
        """è·å–å½“å‰é…ç½®çš„LLMæä¾›å•†å®ä¾‹"""
        # åŠ è½½æœ€æ–°é…ç½®
        config = load_config()
        config_hash = hash(json.dumps(config, sort_keys=True))
        
        # å¦‚æœé…ç½®æ²¡æœ‰å˜åŒ–ä¸”ä¸å¼ºåˆ¶é‡æ–°åŠ è½½ï¼Œè¿”å›ç¼“å­˜çš„æä¾›å•†
        if not force_reload and self._provider and self._config_hash == config_hash:
            return self._provider
        
        # è·å–é€‰ä¸­çš„æä¾›å•†
        selected_provider = config.get("llm_provider", "Ollama")
        
        self.logger.info(f"åˆ›å»ºLLMæä¾›å•†: {selected_provider}")
        self.logger.info(f"é…ç½®ä¿¡æ¯: {json.dumps({k: v for k, v in config.items() if 'api_key' not in k}, indent=2)}")
        
        try:
            # æ ¹æ®é…ç½®åˆ›å»ºå¯¹åº”çš„æä¾›å•†
            if selected_provider == "Ollama":
                provider = OllamaProvider(config, self.logger)
            elif selected_provider == "Gemini":
                provider = GeminiProvider(config, self.logger)
            elif selected_provider == "DeepSeek":
                provider = DeepSeekProvider(config, self.logger)
            elif selected_provider == "Qwen":
                provider = QwenProvider(config, self.logger)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„LLMæä¾›å•†: {selected_provider}")
            
            # ç¼“å­˜æä¾›å•†å’Œé…ç½®å“ˆå¸Œ
            self._provider = provider
            self._config_hash = config_hash
            
            self.logger.info(f"âœ… LLMæä¾›å•† {selected_provider} åˆ›å»ºæˆåŠŸ")
            return provider
            
        except Exception as e:
            error_msg = f"åˆ›å»ºLLMæä¾›å•†å¤±è´¥ - å½“å‰é…ç½®: {selected_provider}\né”™è¯¯ä¿¡æ¯: {str(e)}"
            self.logger.error(error_msg)
            raise Exception(error_msg)
    
    def call_llm(self, prompt: str, context: str = "", force_reload: bool = False) -> str:
        """ç»Ÿä¸€çš„LLMè°ƒç”¨æ¥å£"""
        try:
            provider = self.get_provider(force_reload)
            result = provider.call(prompt, context)
            
            if result:
                return result
            else:
                config = load_config()
                selected_provider = config.get("llm_provider", "æœªçŸ¥")
                raise Exception(f"LLMè°ƒç”¨å¤±è´¥ - å½“å‰é…ç½®çš„æä¾›å•†: {selected_provider}ï¼Œè¿”å›äº†ç©ºç»“æœ")
                
        except Exception as e:
            config = load_config()
            selected_provider = config.get("llm_provider", "æœªçŸ¥")
            
            # æ„å»ºè¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            error_msg = f"""LLMè°ƒç”¨å¤±è´¥è¯¦æƒ…:
ğŸ“‹ å½“å‰é…ç½®çš„æä¾›å•†: {selected_provider}
âŒ é”™è¯¯ä¿¡æ¯: {str(e)}
ğŸ’¡ å»ºè®®: è¯·æ£€æŸ¥è®¾ç½®é¡µé¢ä¸­çš„é…ç½®æ˜¯å¦æ­£ç¡®ï¼Œæˆ–å°è¯•åˆ‡æ¢åˆ°å…¶ä»–å¯ç”¨çš„æä¾›å•†"""
            
            self.logger.error(error_msg)
            raise Exception(error_msg)
    
    def test_current_provider(self) -> tuple[bool, str]:
        """æµ‹è¯•å½“å‰é…ç½®çš„æä¾›å•†"""
        try:
            provider = self.get_provider(force_reload=True)
            return provider.test_connection()
        except Exception as e:
            config = load_config()
            selected_provider = config.get("llm_provider", "æœªçŸ¥")
            error_msg = f"æµ‹è¯•å¤±è´¥ - å½“å‰é…ç½®: {selected_provider}\né”™è¯¯: {str(e)}"
            return False, error_msg


# å…¨å±€å·¥å‚å®ä¾‹
llm_factory = LLMProviderFactory()


def get_llm_provider() -> LLMProvider:
    """è·å–LLMæä¾›å•†å®ä¾‹çš„ä¾¿æ·å‡½æ•°"""
    return llm_factory.get_provider()


def call_llm(prompt: str, context: str = "", force_reload: bool = False) -> str:
    """è°ƒç”¨LLMçš„ä¾¿æ·å‡½æ•°"""
    return llm_factory.call_llm(prompt, context, force_reload)


def test_llm_connection() -> tuple[bool, str]:
    """æµ‹è¯•LLMè¿æ¥çš„ä¾¿æ·å‡½æ•°"""
    return llm_factory.test_current_provider()
